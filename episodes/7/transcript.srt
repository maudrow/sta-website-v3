1
00:00:03,270 --> 00:00:05,100
Audrow Nash: This is a
conversation with Jason

2
00:00:05,100 --> 00:00:09,180
Richards. Jason is the CEO of
Daxbot, which makes a

3
00:00:09,180 --> 00:00:13,380
charismatic robot for food
delivery. We talk about deck

4
00:00:13,380 --> 00:00:17,820
spots delivery robot, their
crowdfunding campaign, that spot

5
00:00:17,850 --> 00:00:22,710
revenue model, how Jason works
with lawmakers to have robots on

6
00:00:22,710 --> 00:00:26,430
the sidewalks. And Jason teases
their real time operating

7
00:00:26,430 --> 00:00:31,830
system. DaxOS. This is the Sense
Think Act podcast. And thank you

8
00:00:31,830 --> 00:00:35,760
to our founding sponsor, Open
Robotics. Now, here's my

9
00:00:35,760 --> 00:00:41,070
conversation with Jason
Richards. Would you introduce

10
00:00:41,070 --> 00:00:41,550
yourself?

11
00:00:42,930 --> 00:00:46,860
Jason Richards: Sure, you bet.
My name is Jason Richards. And I

12
00:00:46,860 --> 00:00:48,630
am the CEO of Daxbot.

13
00:00:49,110 --> 00:00:50,580
Audrow Nash: Would you tell me a
bit about Daxbot?

14
00:00:51,930 --> 00:00:56,040
Jason Richards: Sure, yeah. So
Daxbot is a robotics company

15
00:00:56,340 --> 00:01:01,110
that started out at a research
and development lab back 2015,

16
00:01:01,560 --> 00:01:06,120
the founder, started a couple
other companies, pretty

17
00:01:06,120 --> 00:01:09,480
successful guy, and decided that
he wanted to create a research

18
00:01:09,480 --> 00:01:12,750
and development lab to, you
know, what is the next wave of

19
00:01:12,750 --> 00:01:16,770
technology that we can get in
front of, you know, and so, in

20
00:01:16,770 --> 00:01:20,250
2015, one of the thoughts was,
okay, well, we need robots to be

21
00:01:20,250 --> 00:01:23,550
able to do things. And there's
plenty of plenty of robots in

22
00:01:23,550 --> 00:01:26,700
the, you know, in the workforce,
you know, more in factories and

23
00:01:26,700 --> 00:01:30,150
things like that. But how do we
bridge the gap into the next

24
00:01:30,150 --> 00:01:32,760
generation of robotics, where
you have robots that are on the

25
00:01:32,760 --> 00:01:36,630
streets, similar to, you know,
iRobot? Before the robots go bad

26
00:01:36,780 --> 00:01:39,930
kind of thing? It's like the
movie being able to say, yeah,

27
00:01:39,930 --> 00:01:40,170
like,

28
00:01:40,559 --> 00:01:42,629
Audrow Nash: not the company
backs the back. Yeah.

29
00:01:42,840 --> 00:01:45,510
Jason Richards: Not No, no, not
that company. No, yeah, the

30
00:01:45,510 --> 00:01:49,140
movie, right. So you can tell a
robot, Hey, pick up that, and go

31
00:01:49,140 --> 00:01:52,050
watch that window. And robots
are smart enough to go and do it

32
00:01:52,380 --> 00:01:56,370
in human space. So that was the
idea. And looking at what things

33
00:01:56,370 --> 00:01:59,490
could be most helpful out of the
chute, is the delivery seemed

34
00:01:59,490 --> 00:02:02,610
like a natural first thing that
a robot could do that would be

35
00:02:02,610 --> 00:02:05,610
pretty simple, you know, simple
tasks that would be helpful to

36
00:02:05,610 --> 00:02:10,380
humans. So so that's where the
idea came from. And so as of

37
00:02:11,010 --> 00:02:15,510
recently, we just got through
restructuring companies now

38
00:02:15,510 --> 00:02:20,100
we're dexpot, Inc. and now we're
able to go after and

39
00:02:20,100 --> 00:02:22,920
commercialize and, and have
robots now being deployed to

40
00:02:22,920 --> 00:02:26,520
different states around the
country where you before? Yeah,

41
00:02:26,520 --> 00:02:30,960
so before is Nova dynamics. So
Nova dynamics was the research

42
00:02:30,960 --> 00:02:34,950
and development incubator. And
they've got several projects,

43
00:02:35,460 --> 00:02:40,080
Dax as the robot was kind of the
big one. And so with that

44
00:02:40,530 --> 00:02:43,350
platform, we knew that at some
point, it has to move out of the

45
00:02:43,350 --> 00:02:47,340
r&d Lab and into its own entity
that can be commercialized and

46
00:02:47,580 --> 00:02:51,960
become something Gotcha. So. So
that was the next step. So as of

47
00:02:52,020 --> 00:02:54,240
last year, when you know, all
the research and development

48
00:02:54,240 --> 00:02:58,440
last year, when COVID hit, it
was a great opportunity to start

49
00:02:58,440 --> 00:03:02,100
getting robots out in the field,
in our local hometown, because

50
00:03:02,370 --> 00:03:04,680
restaurants, people aren't
coming to, you know, restaurants

51
00:03:04,680 --> 00:03:07,410
are closing their doors. But
people aren't wanting to see

52
00:03:07,410 --> 00:03:10,320
people. So it seemed like a
great space. And it was, you

53
00:03:10,320 --> 00:03:13,500
know, for that beta test, being
able to take robots that we have

54
00:03:13,500 --> 00:03:16,020
go to the restaurants, pick up
food, and then ticket to

55
00:03:16,020 --> 00:03:18,930
people's houses, you know, all
without seeing a human was

56
00:03:19,470 --> 00:03:24,600
really highlighted the need for
this type of technology. And it

57
00:03:24,660 --> 00:03:27,870
is well received. And it was a
great, great beta test for us.

58
00:03:27,900 --> 00:03:30,420
So January of this year
realizing okay, we've got a

59
00:03:30,420 --> 00:03:34,080
product that is ready to go
awesome. So commercialized and

60
00:03:34,140 --> 00:03:37,830
Audrow Nash: here we go. Hell
yeah. Tell me can you describe

61
00:03:37,830 --> 00:03:38,910
what Dex looks like?

62
00:03:40,790 --> 00:03:45,230
Jason Richards: Yeah, so Dax is
a robot? It's about right. Yeah,

63
00:03:45,260 --> 00:03:47,660
yeah, if you're, if you're
watching the video, you can see

64
00:03:47,660 --> 00:03:52,040
the one behind and it's about
three foot tall, and white and

65
00:03:52,040 --> 00:03:56,420
black, kind of a reverse tuxedo
type situation. And so you see,

66
00:03:56,420 --> 00:03:59,900
you see, Dax has got a fully
articulated neck so that he's

67
00:03:59,900 --> 00:04:05,810
able to look all around and look
up and down also has LED panel

68
00:04:05,810 --> 00:04:08,420
eyes. So the eyes are able to be
expressive, you know, whether it

69
00:04:08,420 --> 00:04:13,190
is being able to be hearts or
you know, I love you going to

70
00:04:13,190 --> 00:04:18,140
sleep blinking, so that it has
some sort of emotional response

71
00:04:18,200 --> 00:04:23,150
from another human. So that is
and then the, the cargo that's

72
00:04:23,150 --> 00:04:25,460
the thing that everybody asks,
well, if it's the delivery

73
00:04:25,460 --> 00:04:28,880
robot, where's the cargo bay,
and it's actually inside the

74
00:04:28,880 --> 00:04:32,330
stomach. So what you think of
the stomach of this little

75
00:04:32,420 --> 00:04:35,300
creature, for people that
haven't seen DAX before, I'd

76
00:04:35,300 --> 00:04:38,870
encourage you to go, you could
check out DAX spot.com. You can

77
00:04:38,870 --> 00:04:42,410
see pictures of him and that
kind of thing, but, but inside

78
00:04:42,410 --> 00:04:45,890
the drawer, which is where his
stomach is, is where the payload

79
00:04:45,890 --> 00:04:48,410
goes. And then inside that is,
you know, heating elements.

80
00:04:48,410 --> 00:04:50,990
We've got to design for a
refrigerator so that we can do

81
00:04:50,990 --> 00:04:53,690
that's cool pharmaceutical
delivery. Yeah, things that need

82
00:04:53,690 --> 00:04:56,120
it, which is pretty. That was a
pretty astounding thing from the

83
00:04:56,120 --> 00:05:00,530
engineers perspective to be able
to build a refrigerator Inside

84
00:05:00,530 --> 00:05:02,630
the robot, obviously, it takes
up a lot of space with a

85
00:05:02,630 --> 00:05:06,140
compressor and, and heat
exchange, especially in that

86
00:05:06,170 --> 00:05:10,550
type of area. But fascinating,
and we did a lot of cool testing

87
00:05:10,550 --> 00:05:14,030
with that one. But so that is is
Dax.

88
00:05:14,520 --> 00:05:18,780
Audrow Nash: And so, to me, Dax,
I was gonna say refrigerator,

89
00:05:18,810 --> 00:05:22,920
like a little like college dorm
freezer kind of thing. Right?

90
00:05:22,920 --> 00:05:24,720
It's kind of looks like one of
those. And then it has this

91
00:05:24,720 --> 00:05:28,860
articulated kind of like
hexagonal head that has the LED

92
00:05:28,860 --> 00:05:32,640
lights on it. And that head is
on a neck that seems to have a

93
00:05:32,640 --> 00:05:36,450
few degrees of freedom, which
I'm sure we'll talk more about

94
00:05:36,450 --> 00:05:41,730
that later. And we can't see it
in the video now because of how

95
00:05:41,730 --> 00:05:47,700
the cameras positioned. But DAX
is on tank treads, right? Yep,

96
00:05:47,880 --> 00:05:52,470
gotcha. Correct. And to me, Dax
looks a lot like Wally without

97
00:05:52,470 --> 00:05:57,660
arms. And like, you say, like,
the belly is where you store the

98
00:05:57,660 --> 00:06:01,680
payload. So whatever you're
delivering, like in Wally Wally

99
00:06:01,680 --> 00:06:05,010
was designed to have that trash
compactor there and its belly.

100
00:06:05,010 --> 00:06:08,190
So it seems kind of similar to
me. Right. And the tank treads.

101
00:06:08,400 --> 00:06:10,320
Yeah, these kinds of things.
Yeah, we,

102
00:06:10,660 --> 00:06:13,330
Jason Richards: we joke about
the fact that Dax kind of looks

103
00:06:13,330 --> 00:06:15,880
like Wally and Eve had a baby.
And that baby.

104
00:06:16,170 --> 00:06:18,270
Audrow Nash: That is right,
because of the coloring. Yeah.

105
00:06:18,300 --> 00:06:21,480
And the sleek design color in
the scheme. Yep. Yeah, yeah,

106
00:06:21,480 --> 00:06:22,080
exactly.

107
00:06:23,310 --> 00:06:26,850
Okay. Tell me a bit about the
neck. Or the head, I guess.

108
00:06:26,880 --> 00:06:28,650
Yeah. Yeah.

109
00:06:28,680 --> 00:06:31,890
Jason Richards: So the getting
the biggest piece that we wanted

110
00:06:31,890 --> 00:06:35,460
to solve was, you know, the
human robot interaction piece.

111
00:06:36,210 --> 00:06:38,820
There other than delivery
robots. There's a lot of people

112
00:06:38,820 --> 00:06:42,450
that will ask about that, you
know, why? Essentially another

113
00:06:42,450 --> 00:06:45,990
delivery robot. And it's it's
kind of funny because, you know,

114
00:06:46,170 --> 00:06:50,130
Dax was built people are
familiar with starship, Dax and

115
00:06:50,130 --> 00:06:52,380
starship started almost the
exact same time not familiar

116
00:06:52,380 --> 00:06:55,140
with starship as far as what a
starship? Oh, yeah. So start.

117
00:06:55,620 --> 00:06:59,070
Yeah, so starship is a delivery
robot on some college campuses.

118
00:06:59,670 --> 00:07:02,370
And if you look up delivery
robots, if you just Google it,

119
00:07:02,370 --> 00:07:06,750
you'll find you know, kiwi, bot,
and starship. And there's some

120
00:07:06,750 --> 00:07:09,750
others. And essentially the
same, the design is exactly the

121
00:07:09,750 --> 00:07:13,740
same. It's about it looks
essentially like a cooler with a

122
00:07:13,740 --> 00:07:17,970
couple of wheels on it. And so
it has a payload and it's, it's

123
00:07:17,970 --> 00:07:23,220
useful in some applications. But
with the the neck and the eyes,

124
00:07:23,220 --> 00:07:26,280
it was more of, you know, how do
you create something that is

125
00:07:26,280 --> 00:07:31,170
going to be accepted in human
pedestrian spaces. So so that's

126
00:07:31,170 --> 00:07:34,980
where the idea of, you know,
articulating neck head, not

127
00:07:34,980 --> 00:07:38,790
putting sensors all over the
place, and the founder, you

128
00:07:38,790 --> 00:07:41,970
know, dexpot really wanted to
make sure that it wasn't creepy.

129
00:07:42,090 --> 00:07:44,490
You know, there's, there's
plenty of really cool, like,

130
00:07:44,910 --> 00:07:48,840
mechanically amazing feats that
people are coming up with, you

131
00:07:48,840 --> 00:07:51,390
know, as far as like mechanics
of robots, you know, bipeds,

132
00:07:51,390 --> 00:07:55,260
those types of things. But if
if, for example, you just saw,

133
00:07:55,290 --> 00:07:58,650
you know, two legs, two robotic
legs running you down the

134
00:07:58,650 --> 00:08:00,780
street, you'd think that
Armageddon was happening, I

135
00:08:00,780 --> 00:08:04,740
mean, that that would be a
creepy thing. So wanting to make

136
00:08:04,740 --> 00:08:07,680
something that would be accepted
in human space. So there's a lot

137
00:08:07,680 --> 00:08:11,340
of work that went into that, of
how do we make a robot that

138
00:08:11,340 --> 00:08:15,750
people want to have around? So
So delivery robots, most of them

139
00:08:15,750 --> 00:08:18,240
have that design of a cooler on
wheels, and about two feet off

140
00:08:18,240 --> 00:08:22,260
the ground. And it becomes an
annoyance for a lot of people,

141
00:08:22,350 --> 00:08:25,800
because it's it's in the way,
similarly, people don't really,

142
00:08:25,860 --> 00:08:29,550
people don't even want bicycles
on sidewalks, right? And it's a

143
00:08:29,550 --> 00:08:33,870
human on that bicycle, but it's
annoying and pedestrian space.

144
00:08:34,050 --> 00:08:35,550
And so they make bicycles

145
00:08:35,550 --> 00:08:37,710
Audrow Nash: that cars have. And
so I don't know if it's quite

146
00:08:37,710 --> 00:08:40,290
the same thing. I get that
they're also annoying, but

147
00:08:40,290 --> 00:08:43,380
perhaps it's like, they go fast.
And there's the potential to

148
00:08:43,380 --> 00:08:46,740
like, get hit by a bike
bicyclist or something like this

149
00:08:46,770 --> 00:08:49,620
or them for to come out of
nowhere. Yeah. Whereas I don't

150
00:08:49,620 --> 00:08:53,520
know that small tub robot would
be quite the same thing. Or what

151
00:08:53,520 --> 00:08:53,940
do you think?

152
00:08:54,000 --> 00:08:57,570
Jason Richards: No. Yeah, the
biggest piece had to do with

153
00:08:57,840 --> 00:09:00,720
with visibility is not being you
know, not being able to see it.

154
00:09:00,720 --> 00:09:03,540
If it's it knee height, it's
different than if it's it, you

155
00:09:03,540 --> 00:09:06,240
know, waist height, or, you
know, shoulder catches.

156
00:09:06,270 --> 00:09:08,310
Audrow Nash: So it's like a
mobile piece of furniture to

157
00:09:08,310 --> 00:09:11,610
trip over if it's short, and
that could irritating. Okay.

158
00:09:12,470 --> 00:09:15,200
Jason Richards: Yeah. Yeah. And
same way. I mean, you know,

159
00:09:15,350 --> 00:09:18,380
people don't like skateboarders
on on sidewalk. Yeah. Again,

160
00:09:18,380 --> 00:09:21,920
moving fast, though. Remote
Control often, yeah, some of it

161
00:09:22,160 --> 00:09:25,910
could be moving fast. And it's
in a way it's becomes a tripping

162
00:09:25,910 --> 00:09:29,930
hazard, that kind of thing. And
so the interesting thing with

163
00:09:29,930 --> 00:09:34,010
Dax is that with all the testing
we've done on the HRIS side of

164
00:09:34,010 --> 00:09:37,910
it, is you know, especially kids
love Dax, it's about the same

165
00:09:37,910 --> 00:09:40,370
height and so they can give it a
hug, and it's looking them like

166
00:09:40,400 --> 00:09:44,690
almost either with very cute and
with and with the neck itself.

167
00:09:45,200 --> 00:09:47,600
And the reason that we didn't
want to creepy with sensors all

168
00:09:47,600 --> 00:09:51,170
over the place, people ask,
well, it has 360 degree cameras

169
00:09:51,200 --> 00:09:53,570
in it, for example. You know,
it's kind of an assumption a lot

170
00:09:53,570 --> 00:09:56,180
of engineers have, it's like,
No, there's not 360 degree

171
00:09:56,180 --> 00:09:58,430
cameras in it. And people always
want to know why it's like,

172
00:09:58,430 --> 00:10:02,060
well, you don't have 360 degree
cameras and you know, you can

173
00:10:02,060 --> 00:10:05,150
kind of hear that their stuff
behind you. And so you can, you

174
00:10:05,150 --> 00:10:07,490
know, you have some sense and
awareness of what's around you.

175
00:10:08,090 --> 00:10:10,970
But you're not a spider, right?
You don't have this spidey sense

176
00:10:10,970 --> 00:10:13,670
where you can you know that
something's behind you. So in

177
00:10:13,670 --> 00:10:15,890
the same way, that's where the
articulating that came from, you

178
00:10:15,890 --> 00:10:18,140
know, you could have something
that that knows everything

179
00:10:18,140 --> 00:10:21,110
around it in a 360 degree view,
or you have something that

180
00:10:21,110 --> 00:10:24,680
actually is more like, you know,
more like a dog or a human,

181
00:10:25,070 --> 00:10:28,460
where you actually have to turn
your neck around to see what is

182
00:10:28,460 --> 00:10:33,680
actually going on. But yeah, and
so even those those pieces, it

183
00:10:33,680 --> 00:10:39,080
gives it a feeling that it is
actually alive, instead of just

184
00:10:39,080 --> 00:10:42,380
being a machine, which why not?
It actually is machine.

185
00:10:42,410 --> 00:10:44,660
Audrow Nash: So I mean, I'm
thinking of this thing, where

186
00:10:44,660 --> 00:10:48,020
it's like airplanes don't flap
their wings kind of thing. So I

187
00:10:48,020 --> 00:10:51,020
mean, we can draw some
inspiration from biology. But

188
00:10:51,020 --> 00:10:54,680
where do we end Drawing
inspiration? From biology?

189
00:10:54,860 --> 00:10:58,490
Absolutely. And I wonder, I
would think it would be possible

190
00:10:58,490 --> 00:11:02,450
to use cleverly hidden sensors
and to have the articulating

191
00:11:02,450 --> 00:11:07,670
neck, but have the articulating
neck not necessarily tied to a

192
00:11:07,670 --> 00:11:13,940
vision system. Why Why limit it
to having the vision system on

193
00:11:13,940 --> 00:11:19,850
the articulating neck instead of
just having hidden sensors and

194
00:11:19,850 --> 00:11:20,540
this kind of thing?

195
00:11:22,040 --> 00:11:24,770
Jason Richards: Yeah, that's a
good question. I don't know,

196
00:11:24,800 --> 00:11:27,710
the, the founder when he went
through all of this, there was a

197
00:11:27,710 --> 00:11:30,950
lot of there was a lot of, you
know, reasoning and things like

198
00:11:30,950 --> 00:11:35,780
that. And again, making it more
a phrase that we use around here

199
00:11:35,780 --> 00:11:39,260
is anamorphic. You know, it's
not anamorphic. It's not an ad.

200
00:11:39,590 --> 00:11:43,880
Cannon morphic. I don't know who
doesn't like thrupp amorphic are

201
00:11:43,910 --> 00:11:47,840
more like a like a dog. Yeah.
Yeah. Yeah, it seems more like

202
00:11:47,840 --> 00:11:51,170
it's a pet than it is a machine.

203
00:11:51,500 --> 00:11:54,290
Audrow Nash: So anamorphic. So
it's like anthropomorphic

204
00:11:54,320 --> 00:11:58,370
animal. So when we assume but
with cane, but for things like

205
00:11:58,370 --> 00:12:01,310
dogs, or whatever it might be,
okay, so it's a creature and has

206
00:12:01,610 --> 00:12:03,950
some sort of intent when it
moves around the world or

207
00:12:03,950 --> 00:12:05,300
whatever it might be. Right.

208
00:12:06,020 --> 00:12:08,240
Jason Richards: Yeah, exactly.
And so one of the things, for

209
00:12:08,240 --> 00:12:11,630
example, is, I believe one of
the stories I heard is that if

210
00:12:11,840 --> 00:12:15,410
Dax you know, robot goes into a
situation and needs to go

211
00:12:15,410 --> 00:12:19,550
backwards. You know, we as we as
humans, don't walk backwards,

212
00:12:20,360 --> 00:12:23,720
right? We don't just like walk
backwards into into us, yes, we

213
00:12:23,720 --> 00:12:26,030
will turn and say, Oh, here's
where we're going to go. And

214
00:12:26,030 --> 00:12:28,730
then, you know, if we need to
have some field division in

215
00:12:28,730 --> 00:12:31,670
front of us, we'll be looking
back and forth. So in the same

216
00:12:31,670 --> 00:12:34,820
way, wanting to train, train the
robot, right train the neural

217
00:12:34,820 --> 00:12:39,200
net, so that it's not just doing
things automatically. So it

218
00:12:39,200 --> 00:12:42,500
actually is showing intent, that
oh, here's where I'm going. One

219
00:12:42,500 --> 00:12:45,380
of the things that we've heard,
we actually, we live close to a

220
00:12:45,380 --> 00:12:49,430
university, Oregon State
University. And one of in

221
00:12:49,430 --> 00:12:53,450
starship, one of those
competitors is up there. And I

222
00:12:53,450 --> 00:12:56,900
haven't heard, I haven't talked
to anybody around that really

223
00:12:56,900 --> 00:12:59,480
likes them on campus. And the
reason for that is because it

224
00:12:59,510 --> 00:13:04,250
has to do with intent. So the
device will drive up to a street

225
00:13:04,250 --> 00:13:08,450
corner, and then stop. And then
all the other traffic, the cars

226
00:13:08,510 --> 00:13:11,870
are like, What are you doing?
Yeah, what do I do? Yeah, is it

227
00:13:11,870 --> 00:13:14,240
gonna cross? Am I gonna cross
there's no clarity? No, there's

228
00:13:14,240 --> 00:13:17,300
no law about right of way. Yeah,
exactly. So there's sensors, no

229
00:13:17,300 --> 00:13:19,640
signaling, there's, there's no
idea, you know, are you going to

230
00:13:19,640 --> 00:13:22,100
go am I going to go and so it's,
you know, some of these devices

231
00:13:22,100 --> 00:13:24,590
have been hit one guy hit by a
train that's been hit by a

232
00:13:24,590 --> 00:13:28,280
truck, one's one into a train.
And so it's, you know, and it

233
00:13:28,280 --> 00:13:31,190
has has plenty of sensors
onboard. You know, that's, you

234
00:13:31,190 --> 00:13:33,380
know, with that one
specifically, it's, that's not

235
00:13:33,380 --> 00:13:36,650
the problem. But because people
don't know what it's gonna do,

236
00:13:36,680 --> 00:13:39,680
they don't have any, any idea
versus Dax will get to the

237
00:13:39,680 --> 00:13:43,400
street corner, and like a human,
it will look at you, and it will

238
00:13:43,400 --> 00:13:45,890
watch you and follow your
vehicle as it drives past. So

239
00:13:45,890 --> 00:13:48,710
you know, oh, this thing sits
somewhere. And it's aware of

240
00:13:48,710 --> 00:13:51,440
what's going on. And it cannot
its head, like, yes, you can go

241
00:13:51,440 --> 00:13:53,960
Go ahead, or, you know, shake
its head so that people know,

242
00:13:53,990 --> 00:13:58,250
Oh, you want me to go? Versus
you're gonna go right. And so

243
00:13:58,700 --> 00:14:01,160
there was a lot of there was a
lot of thought and

244
00:14:01,160 --> 00:14:05,360
intentionality into the neck.
Yeah, isn't just necessary for

245
00:14:05,360 --> 00:14:08,150
the neck sake, but it's more for
communication, it looks one of

246
00:14:08,150 --> 00:14:08,840
the design,

247
00:14:09,260 --> 00:14:12,890
Audrow Nash: it looks to me. So
thinking of like the founders

248
00:14:12,890 --> 00:14:15,890
perspective, and this kind of
thing, perhaps it had something

249
00:14:15,890 --> 00:14:21,530
to do with making it so that so
I mean, if you have sensors

250
00:14:21,530 --> 00:14:23,990
everywhere, and you can get a
really good rendering of the

251
00:14:23,990 --> 00:14:27,440
world and this kind of thing.
You can design behavior that

252
00:14:27,440 --> 00:14:32,450
uses that understanding of the
world. But in this case, you're

253
00:14:32,450 --> 00:14:35,330
kind of constraining the
behavior of the robot to be

254
00:14:35,330 --> 00:14:40,880
something that seems more
natural. In like, it has to go

255
00:14:40,880 --> 00:14:43,340
and look behind it. There's
something if it's going to go

256
00:14:43,340 --> 00:14:47,240
and back up or if it wants to,
like look both ways for the

257
00:14:47,240 --> 00:14:51,080
street, you have to do that with
the head. And it almost so it

258
00:14:51,080 --> 00:14:56,480
basically constrains the
developers to like, I don't know

259
00:14:56,480 --> 00:14:59,750
program more lifelike behavior
in it so that it operates in the

260
00:14:59,750 --> 00:15:03,500
world. I could see that being a
clever way of doing that kind of

261
00:15:03,500 --> 00:15:03,650
thing.

262
00:15:04,670 --> 00:15:08,090
Jason Richards: And for for
engineers, specifically,

263
00:15:08,120 --> 00:15:11,150
mechanical engineers, they would
much prefer to have the device

264
00:15:11,150 --> 00:15:13,910
to be a cooler, right? Just
right. Yeah, that's it, that's a

265
00:15:13,910 --> 00:15:16,910
much simpler thing than a fully
articulating neck was dual pitch

266
00:15:16,910 --> 00:15:17,510
motors. Right?

267
00:15:17,540 --> 00:15:20,090
Audrow Nash: Yeah. Or just a
sensor information would be

268
00:15:20,090 --> 00:15:22,730
probably more valuable.
Absolutely. The who cares if it

269
00:15:22,910 --> 00:15:25,550
looks like a cooler, but I get
your meaning that for like,

270
00:15:25,550 --> 00:15:29,030
sensors everywhere and full 3d
map of the whole environment

271
00:15:29,030 --> 00:15:30,380
that it's in and this kind of
thing?

272
00:15:30,830 --> 00:15:33,260
Jason Richards: Yeah. Yeah,
absolutely. And we, you know,

273
00:15:33,260 --> 00:15:36,650
obviously, we got, you know,
that technology to be able to do

274
00:15:36,650 --> 00:15:39,230
that, so that Dax is able to
learn in 3d space, and you know,

275
00:15:39,230 --> 00:15:44,000
instead but, again, more similar
to, you know, a human if I'm

276
00:15:44,000 --> 00:15:46,550
walking down the street, and
I've been down that street

277
00:15:46,550 --> 00:15:49,010
before, I have an idea of what
to expect, right? I know,

278
00:15:49,010 --> 00:15:51,560
there's a curb there. I know.
There's a telephone pole. I see.

279
00:15:51,590 --> 00:15:53,810
Oh, there's a box there. I've
never seen that box before.

280
00:15:54,080 --> 00:15:56,990
Okay. New information might not
be there tomorrow. Right? So

281
00:15:56,990 --> 00:16:00,920
it's not something that that
really comes in. But because

282
00:16:01,430 --> 00:16:04,730
everything is changing in such a
rapid space, again, on a factory

283
00:16:04,730 --> 00:16:05,150
floor,

284
00:16:05,180 --> 00:16:07,220
Audrow Nash: you mean in the
environment? Like walking

285
00:16:07,220 --> 00:16:07,760
around?

286
00:16:08,840 --> 00:16:10,310
Jason Richards: Right, yeah. And
environment or walking? Right.

287
00:16:10,310 --> 00:16:13,940
If you're walking downtown, you
know, in a metropolitan area has

288
00:16:13,940 --> 00:16:16,520
a lot of moving parts. It's
changing all the time. Yeah,

289
00:16:16,550 --> 00:16:20,330
exactly. So so being able to
have something that is more

290
00:16:20,330 --> 00:16:23,360
lifelike to be able to adapt in
that environment. And it seems

291
00:16:23,360 --> 00:16:25,670
more natural in that
environment. Actually, pretty

292
00:16:25,670 --> 00:16:26,000
interesting.

293
00:16:26,060 --> 00:16:29,870
Audrow Nash: So what sensors are
on this robot on the Dax robot?

294
00:16:31,400 --> 00:16:34,460
Jason Richards: Yeah, so despair
division is is one of the two it

295
00:16:34,460 --> 00:16:34,640
has

296
00:16:34,640 --> 00:16:37,790
Audrow Nash: a stereo that has
two cameras beneath the kind of

297
00:16:37,790 --> 00:16:41,150
chin. Yes, something right below
the display. Right. And

298
00:16:41,210 --> 00:16:42,950
Jason Richards: yeah, so yeah,
where you see the display of the

299
00:16:42,950 --> 00:16:44,780
eyes is where that

300
00:16:45,260 --> 00:16:47,900
Audrow Nash: just been, you
know, just beneath it. Yeah. Cuz

301
00:16:47,900 --> 00:16:51,530
I think I see that a little
circles from the cameras that it

302
00:16:51,530 --> 00:16:53,810
has. So you have the two
cameras. And from that, you can

303
00:16:53,810 --> 00:16:57,590
infer some 3d information about
what's in front of the robot.

304
00:16:57,620 --> 00:17:03,470
Okay. Yeah. Anything else? I
mean, I imagine there's encoders

305
00:17:03,470 --> 00:17:07,610
and things on the wheels, or the
track. Oh, yeah. What, uh, and

306
00:17:07,610 --> 00:17:11,000
then all the motors have sensors
and things. But what else for

307
00:17:11,000 --> 00:17:11,600
perception?

308
00:17:11,630 --> 00:17:14,300
Jason Richards: Is it just those
cameras? Maybe microphones?

309
00:17:14,330 --> 00:17:18,710
Yeah. Yeah. So it's got, yeah,
it's got stereo microphones.

310
00:17:19,340 --> 00:17:22,850
See, so you can hear what's
going on. Again, kind of that

311
00:17:22,850 --> 00:17:25,400
sense. If you're, if you're a
human, you can hear somebody

312
00:17:25,430 --> 00:17:27,770
coming up behind you, right? You
don't have eyes in the back of

313
00:17:27,770 --> 00:17:30,290
your head, but you can hear that
something is happening. And

314
00:17:30,290 --> 00:17:32,660
you're more aware of your
environment that way. So, so

315
00:17:32,660 --> 00:17:35,990
yeah, those those. It's fun.
When I when I talk to people

316
00:17:35,990 --> 00:17:39,980
about sensors, a lot of times
they're thinking very

317
00:17:39,980 --> 00:17:42,620
unilaterally, and they're
thinking about, like Lidar and

318
00:17:42,620 --> 00:17:43,100
Audrow Nash: things.

319
00:17:44,120 --> 00:17:46,850
Jason Richards: Yeah, exactly.
Yeah. Like LiDAR, ultrasonic. I

320
00:17:46,850 --> 00:17:48,440
Audrow Nash: mean, I'm honestly
surprised. You don't have like a

321
00:17:48,440 --> 00:17:51,170
plane or lighter or anything in
there. But if you can do

322
00:17:51,170 --> 00:17:52,460
everything with vision, that's
great.

323
00:17:53,900 --> 00:17:56,750
Jason Richards: Yeah, yeah,
it's, it's pretty fascinating,

324
00:17:56,780 --> 00:17:59,180
obviously, yeah, the tracks have
sensors, things like that, you

325
00:17:59,180 --> 00:18:02,810
know, when we're going into, you
know, autonomy, things like

326
00:18:02,810 --> 00:18:06,710
that. Being able to have tracks
to be able to know, even if you

327
00:18:06,710 --> 00:18:09,170
lose, you know, GPS signal and
that kind of thing, you're able

328
00:18:09,170 --> 00:18:12,950
to know, within, you know,
centimeter to where you're at in

329
00:18:13,190 --> 00:18:13,580
your thing.

330
00:18:13,580 --> 00:18:16,580
Audrow Nash: The tracks are very
accurate for encoding it. And

331
00:18:16,580 --> 00:18:20,330
that's probably because they
don't slip as much as wheels

332
00:18:20,330 --> 00:18:20,690
might.

333
00:18:20,930 --> 00:18:24,050
Jason Richards: Correct. Yep.
Yeah, exactly.

334
00:18:24,830 --> 00:18:29,120
Audrow Nash: So Gotcha. Okay, so
going back to the neck, you have

335
00:18:29,510 --> 00:18:32,870
these two cameras, and those are
that's the majority of the bed

336
00:18:32,870 --> 00:18:35,480
that is the big perception
sensor, the two cameras, and

337
00:18:35,480 --> 00:18:37,940
then you have stereo
microphones, or whatever it's

338
00:18:37,940 --> 00:18:40,970
called, so that you can tell
directionality of the sound,

339
00:18:41,030 --> 00:18:45,410
right? Yeah. Great. That's
that's pretty much it for

340
00:18:45,410 --> 00:18:50,300
perception. Other than like,
actually, I forget what it's

341
00:18:50,300 --> 00:18:54,140
called now, like dead reckoning
your position for from the

342
00:18:54,140 --> 00:18:55,190
tracks and this kind of thing.

343
00:18:55,400 --> 00:18:59,240
Jason Richards: Yeah. But that's
it. Yep. Yeah,

344
00:18:59,270 --> 00:19:01,670
Audrow Nash: that's it you've
like, are there like capacitive

345
00:19:01,670 --> 00:19:04,640
sensors? So it can tell if it's
being pat or hugged or anything

346
00:19:04,640 --> 00:19:10,100
like this? Or how do you detect
like, a person is your buyer

347
00:19:10,100 --> 00:19:13,430
interact? I guess the cameras
would do that. But do anything

348
00:19:13,430 --> 00:19:15,080
for a person touching it or
anything?

349
00:19:16,520 --> 00:19:19,340
Jason Richards: No, no, not.
We've, you know, talked about

350
00:19:19,340 --> 00:19:22,760
that a little bit. It's not, we
don't know, how helpful that

351
00:19:22,760 --> 00:19:25,310
would be at this point. You
know, everything, everything has

352
00:19:25,310 --> 00:19:27,560
to have, you know, some lead
time and stuff like

353
00:19:27,560 --> 00:19:28,910
Audrow Nash: that. What do you
mean meantime,

354
00:19:28,940 --> 00:19:31,610
Jason Richards: you know, just
which I like to do is that just

355
00:19:31,610 --> 00:19:34,580
takes time. Gotcha. Yeah, yeah.
Just development time. You know,

356
00:19:34,610 --> 00:19:37,460
and so what you know, weighing
priorities, which is the most

357
00:19:37,460 --> 00:19:41,960
important thing totally, but
yeah, with with you know, the

358
00:19:41,960 --> 00:19:45,170
vision for example, you know, if
if somebody sneaks up behind me

359
00:19:45,170 --> 00:19:48,770
and touches me, yeah, I would
know that and maybe the robot

360
00:19:48,860 --> 00:19:52,760
doesn't at this point might
being able to hear this. Yeah,

361
00:19:52,790 --> 00:19:54,800
exactly. Somebody is coming up
behind me. I can look back and

362
00:19:54,800 --> 00:19:57,830
be like, Oh, hey, what's going
on? And then you know, we can

363
00:19:57,980 --> 00:20:00,470
have a conversation so no, no
perceptions. As far as that's

364
00:20:00,500 --> 00:20:00,980
gotcha

365
00:20:01,700 --> 00:20:03,440
Audrow Nash: is just curious
with like interacting with

366
00:20:03,440 --> 00:20:05,810
little kids and stuff with like
the little kid with patted on

367
00:20:05,810 --> 00:20:08,540
the head and would know with
being pat on the head. Oh, yeah.

368
00:20:08,570 --> 00:20:08,900
But

369
00:20:08,990 --> 00:20:11,210
Jason Richards: people people
are hugging it all the time.

370
00:20:13,190 --> 00:20:16,190
Audrow Nash: Yeah. Okay, so the
neck. Tell me about the neck. So

371
00:20:16,220 --> 00:20:18,680
it how many degrees of freedom
does it have? What kind of

372
00:20:18,680 --> 00:20:21,440
things gonna do? Yeah.

373
00:20:21,470 --> 00:20:24,950
Jason Richards: So not wanting
again, not wanting to be super

374
00:20:24,950 --> 00:20:26,810
creepy. So we've limited the
range.

375
00:20:26,990 --> 00:20:30,080
Audrow Nash: So we can't go
around like, a hole or

376
00:20:30,080 --> 00:20:30,470
something.

377
00:20:31,970 --> 00:20:34,370
Jason Richards: Yeah, we don't
want this. It's Halloween. Yeah.

378
00:20:35,600 --> 00:20:39,080
Yeah, robot, right. Yeah,
exactly. So it's a limited a

379
00:20:39,080 --> 00:20:42,530
limited range view, just just
like a human would be. So it

380
00:20:42,530 --> 00:20:45,500
doesn't it will go back,
probably a little farther than a

381
00:20:45,500 --> 00:20:48,680
human would by itself. Turn my
neck, I can get about 90

382
00:20:48,680 --> 00:20:52,700
degrees. But it doesn't go much
further than that. So we have we

383
00:20:52,700 --> 00:20:58,610
also do a full 180 either. And
so it's about it's about 150

384
00:20:58,610 --> 00:21:02,210
degrees. So that's pretty harsh.
And, you know, basically looking

385
00:21:02,210 --> 00:21:04,760
over your shoulder, right, you
can look back and see what's

386
00:21:04,760 --> 00:21:09,230
going on. Yeah. And then And
then, you know, down basically

387
00:21:09,290 --> 00:21:12,020
similar to a human right, you
can sit down touch your head.

388
00:21:12,320 --> 00:21:15,830
And then the other problem is,
you know, looking, looking up,

389
00:21:15,860 --> 00:21:17,990
you've got the back of the the
head,

390
00:21:18,020 --> 00:21:20,330
Audrow Nash: right, the Yeah,
because the cameras are just

391
00:21:20,360 --> 00:21:23,330
underneath the chin, in a sense.
So it really does like to look

392
00:21:23,330 --> 00:21:29,060
up. Yeah, to clear the chin.
Right. Yep. Yeah, exactly. So,

393
00:21:29,570 --> 00:21:32,090
Jason Richards: yeah. So that's
it fully, fully articulating in

394
00:21:32,090 --> 00:21:32,840
all of the directions.

395
00:21:32,840 --> 00:21:34,730
Audrow Nash: That's two degrees
of freedom. Can it turn

396
00:21:34,730 --> 00:21:36,950
sideways? I think it probably
can. Right?

397
00:21:37,700 --> 00:21:40,670
Jason Richards: Yeah. Yeah.
Being able to kind of, yeah,

398
00:21:41,390 --> 00:21:46,130
tilt the head, like, one of our
Yeah, and that's one of the one

399
00:21:46,130 --> 00:21:48,380
of the most favorite ones that
we have, you know, especially

400
00:21:48,380 --> 00:21:51,380
when we're interacting with
kids, or they want to play with

401
00:21:51,380 --> 00:21:53,930
the robot. And so the robot
kinda like a dog, like, you

402
00:21:53,930 --> 00:21:57,410
know, caulk its head and look at
you funny. So that's one of the

403
00:21:57,440 --> 00:21:59,600
one of the things that we love
doing with people. Yeah. Like,

404
00:21:59,630 --> 00:22:02,660
and then, and then a couple
sounds that we put into the

405
00:22:02,660 --> 00:22:06,230
robot. So for them, they sound
in some of the videos, which is

406
00:22:06,230 --> 00:22:09,110
interesting. I mean, Pixar did a
wonderful job. So

407
00:22:10,160 --> 00:22:11,540
Audrow Nash: we'll leverage a
lot of their things.

408
00:22:12,200 --> 00:22:15,320
Jason Richards: Yeah,
absolutely. Yeah. But it's, it's

409
00:22:15,320 --> 00:22:17,870
pretty fun. One of the in its,
you know, with the sounds,

410
00:22:17,990 --> 00:22:23,810
people ask, you know, if it can
talk in, in some language, you

411
00:22:23,810 --> 00:22:28,370
know, English or whatever? And
the answer is, well, yeah, tech,

412
00:22:28,400 --> 00:22:31,910
technologically, that's pretty
simple to do. Yeah, but we've

413
00:22:31,910 --> 00:22:34,580
made a decision to not and
again, the reason for that is

414
00:22:35,360 --> 00:22:40,190
when you don't expect a dog to
talk back to you, you know, in

415
00:22:40,220 --> 00:22:43,580
in a language, but it will bark
and you can you can understand

416
00:22:43,580 --> 00:22:47,750
with with nonverbal
communication, what is going on.

417
00:22:47,810 --> 00:22:51,950
And so in the same way, as we,
you know, work with different

418
00:22:51,950 --> 00:22:55,130
robots in urban spaces, and they
would talk to you in English, it

419
00:22:55,130 --> 00:22:57,920
doesn't matter how nice the
robot is trying to say something

420
00:22:57,920 --> 00:23:01,730
in English. It comes across
route, you know, you don't

421
00:23:01,730 --> 00:23:04,490
expect a device to be telling
you to get out of the way,

422
00:23:04,700 --> 00:23:08,720
right. It's like, well, that's,
that's kind of, versus a robot

423
00:23:08,720 --> 00:23:12,830
that is just making sounds, you
know, more RTD to ask, then,

424
00:23:13,010 --> 00:23:15,920
than anything else, right? So
that you, you understand, like,

425
00:23:15,950 --> 00:23:19,280
Oh, this is asking me to move. I
was actually just having lunch

426
00:23:19,280 --> 00:23:22,400
today. It's at our local Mexican
restaurant, and we have a robot

427
00:23:22,400 --> 00:23:25,700
down there that's doing
deliveries all the time. And the

428
00:23:25,700 --> 00:23:29,240
robot had had come inside. And
then there was a cup in the way

429
00:23:29,240 --> 00:23:31,880
where it's supposed to park. And
so it's looking down there

430
00:23:31,880 --> 00:23:35,180
looking up looking around, you
know, its head like, hey, come

431
00:23:35,180 --> 00:23:38,930
over here and putting sounds and
then just somebody that was at

432
00:23:38,930 --> 00:23:41,120
the restaurant looked at it was
like, oh, it's trying to tell me

433
00:23:41,120 --> 00:23:43,880
something went over, take the
cup, got it out of the way. And

434
00:23:43,880 --> 00:23:47,030
then the robot was able to say
thank you turn around and back

435
00:23:47,030 --> 00:23:50,750
into its spot. So one of your
robots you're saying? Yeah,

436
00:23:51,320 --> 00:23:58,160
yeah, one of one of the ones
like this. Yeah. So So with

437
00:23:58,190 --> 00:24:01,640
Yeah, so it a nonverbal
communication, and that human

438
00:24:01,640 --> 00:24:04,520
robot interaction pieces, again,
a lot of stuff that we've we've

439
00:24:04,520 --> 00:24:08,930
studied. So it's where we put a
lot of time and effort into

440
00:24:08,990 --> 00:24:12,350
Audrow Nash: Yeah, it seems like
a thing of setting expectations.

441
00:24:12,620 --> 00:24:15,320
Because I think if something
talks, we expect that it can

442
00:24:15,320 --> 00:24:19,670
really talk. Right? And then
it's frustrating when it

443
00:24:19,670 --> 00:24:23,120
immediately doesn't recognize
you, like you could say any

444
00:24:23,120 --> 00:24:26,270
arbitrary complex thing, and it
would have no idea unless

445
00:24:26,270 --> 00:24:28,190
there's someone behind the
scenes like this.

446
00:24:29,240 --> 00:24:31,220
Jason Richards: Right? Yeah,
like a web chat. Yeah. You know,

447
00:24:31,250 --> 00:24:34,820
mean, typing, and you're like,
No, this is definitely this

448
00:24:34,820 --> 00:24:35,420
isn't a human

449
00:24:35,450 --> 00:24:38,060
Audrow Nash: talking. Yeah, the
robot would be similar to that.

450
00:24:38,690 --> 00:24:42,230
Otherwise, yeah, I've seen that
quite a bit from like different

451
00:24:42,230 --> 00:24:45,080
human robot interaction research
where they want to set the

452
00:24:45,080 --> 00:24:50,810
expectation low. So that they
can not over promise.

453
00:24:52,310 --> 00:24:53,810
Jason Richards: Right. And
obviously, you know, other

454
00:24:53,810 --> 00:24:56,600
companies have done a good job
of overcoming that barrier by

455
00:24:56,600 --> 00:24:59,660
having an actual human you know,
via like a computer screen on

456
00:24:59,660 --> 00:25:02,690
the other side. And then you're
talking face to face with a

457
00:25:02,690 --> 00:25:06,230
human right? The robot just
happens to be, you know, the

458
00:25:06,230 --> 00:25:09,950
carrier. It's the media of the
iPad, for sure. Yeah, exactly.

459
00:25:10,220 --> 00:25:19,520
Audrow Nash: So do you. So how
much of the robots control? Is a

460
00:25:19,520 --> 00:25:23,060
human in the loop? Like tele
operating the robot? And how

461
00:25:23,060 --> 00:25:26,090
much is autonomous? At this
point?

462
00:25:26,690 --> 00:25:30,380
Jason Richards: Yeah, at this
point, the, the autonomous piece

463
00:25:30,410 --> 00:25:34,130
is almost strictly for
navigation. So now that part's

464
00:25:34,130 --> 00:25:36,530
out of the way, one of the
things obviously, the the

465
00:25:36,560 --> 00:25:40,280
software guys, they're really
excited about the HDRI

466
00:25:40,280 --> 00:25:43,790
components automatically know
facial recognition, somebody

467
00:25:43,790 --> 00:25:47,420
happy, sad, how do I some
emotion or act? Yeah, yeah,

468
00:25:47,450 --> 00:25:50,450
putting them putting emotion.
But you know, while it's on the

469
00:25:50,450 --> 00:25:53,870
job, if you will, you know,
especially with disparity

470
00:25:53,870 --> 00:25:56,300
vision, and making sure that,
you know, collision detection

471
00:25:56,300 --> 00:25:58,670
and avoidance and things like
that is all working, it's going

472
00:25:58,670 --> 00:26:01,670
down a path to making sure it's
not going to run into something,

473
00:26:01,820 --> 00:26:04,310
but at the same time, it comes
into an obstacle, you know, it

474
00:26:04,310 --> 00:26:07,790
hits an obstacle, not hits an
obstacle, it sees an obstacle

475
00:26:07,820 --> 00:26:10,820
stops, and then can look around.
And it's kind of like, you know,

476
00:26:10,850 --> 00:26:13,610
looks like it's looking at
flowers or trees are cars that

477
00:26:13,610 --> 00:26:17,030
are passing by. So that, so that
is more natural than, you know,

478
00:26:17,030 --> 00:26:19,670
a robot that just drives you
know, gets to a point and then

479
00:26:19,670 --> 00:26:22,610
just kind of sits there for
Yeah, so. So all of the the

480
00:26:22,610 --> 00:26:27,410
human interaction pieces have
been with tele operating, you

481
00:26:27,410 --> 00:26:31,280
know, having somebody on the
other end, being able to emote,

482
00:26:31,280 --> 00:26:36,110
and it's been really fun to,
again, experiment in a pretty

483
00:26:36,110 --> 00:26:39,110
tight loop. So you can see what
do people react to? And what do

484
00:26:39,110 --> 00:26:43,430
they want? Like, what do they
want to see from this robot on a

485
00:26:43,760 --> 00:26:47,090
emotional or interaction basis?
And then once they have that,

486
00:26:47,300 --> 00:26:49,370
you know, now that we've had
that we have some really good

487
00:26:49,370 --> 00:26:53,480
data now, just programming
right? This program, like, okay,

488
00:26:53,630 --> 00:26:57,350
when this than this, yeah. And
then the robot can know what to

489
00:26:57,350 --> 00:26:58,730
do in certain situations.

490
00:26:59,510 --> 00:27:03,410
Audrow Nash: So when so the
robot will autonomously go from

491
00:27:03,410 --> 00:27:08,840
point A to point B. It does it
uses GPS or how to do that.

492
00:27:10,130 --> 00:27:13,370
Jason Richards: Yeah, GPS and,
and Dead Reckoning and dead

493
00:27:13,370 --> 00:27:18,020
reckoning. So, right? Yeah. So
within the tracks itself, it, it

494
00:27:18,020 --> 00:27:21,860
knows where it is less slippage,
like you'd say, you know, better

495
00:27:22,100 --> 00:27:22,610
better than

496
00:27:22,640 --> 00:27:26,990
Audrow Nash: does it use. Is it
doing? It's not generating? I

497
00:27:26,990 --> 00:27:32,210
think, in us talking a little
bit before the actual recording

498
00:27:32,210 --> 00:27:38,270
started. It doesn't do mapping,
right, you have to go map an

499
00:27:38,270 --> 00:27:41,450
area before the robot goes into
that area. Is it correct?

500
00:27:42,920 --> 00:27:46,340
Jason Richards: Yeah, for the
autonomous piece. Yeah. So when

501
00:27:46,340 --> 00:27:49,520
we go into a new area, yeah, we
go into a new area, we have, you

502
00:27:49,520 --> 00:27:52,520
know, an operator that's doing
that piece, you know, going

503
00:27:52,520 --> 00:27:55,790
through finding out where the
the obstacles are stuff like

504
00:27:55,790 --> 00:27:59,090
that. And so here's the road
crossings. Here's the dangerous

505
00:27:59,090 --> 00:28:01,670
battle. Yeah, don't go here. You
know, kind of like kinda like

506
00:28:01,670 --> 00:28:05,300
the Waze app, you know. So once
we have all that, all that set,

507
00:28:05,330 --> 00:28:07,850
then we can say, Yeah, this is a
this is a free and clear

508
00:28:07,850 --> 00:28:11,810
sidewalk, you can go, you know,
full speed. This is a danger

509
00:28:11,810 --> 00:28:15,350
zone slowdown. This is a road
crossing. We don't, we aren't,

510
00:28:15,380 --> 00:28:17,120
you know, there's a lot more
programming, we aren't letting

511
00:28:17,120 --> 00:28:18,560
the robot cross streets fine. So
yeah,

512
00:28:18,560 --> 00:28:21,470
Audrow Nash: that seems that
seems terrifying. For your

513
00:28:21,470 --> 00:28:26,210
robot. Yeah. Yes. Yeah. So what
is it when someone is going to

514
00:28:26,210 --> 00:28:29,720
create one of these maps for the
robot? What does it involve? Do

515
00:28:29,720 --> 00:28:35,270
they just like a handheld stereo
camera to do it like? Or do you

516
00:28:35,270 --> 00:28:38,690
just bring a lidar through it?
And it just maps everything? Or

517
00:28:38,720 --> 00:28:41,210
what like, how do you how do you
create this initial map?

518
00:28:42,320 --> 00:28:44,240
Jason Richards: Question, we
actually, we actually just do it

519
00:28:44,240 --> 00:28:48,680
with a robot to drive the robot
relocating. Mm hmm. Which does

520
00:28:48,950 --> 00:28:51,110
Audrow Nash: generate a big 3d
mouse that way, and then you

521
00:28:51,110 --> 00:28:51,740
annotate it.

522
00:28:54,050 --> 00:28:57,440
Jason Richards: Know where it's
a little more complex than that.

523
00:28:57,740 --> 00:29:00,200
I won't, I won't try to butcher
it. Because I'm not one of the

524
00:29:00,200 --> 00:29:03,200
engineers on the project. But
yeah, but being able to go

525
00:29:03,200 --> 00:29:07,220
through. And then with that,
being able to process all the

526
00:29:07,220 --> 00:29:09,710
data that we need, so we can
see, here's, here's what's going

527
00:29:09,710 --> 00:29:12,440
on. And it also does a lot of
good stuff. When we're doing

528
00:29:12,770 --> 00:29:16,460
bigger deployments. One of the
things that people really want

529
00:29:16,490 --> 00:29:20,840
from our customer base is Dax is
really cool. It's a really cool

530
00:29:20,840 --> 00:29:24,560
thing, right? It just happens to
be cute, and it delivers food.

531
00:29:24,590 --> 00:29:27,980
So you know, what it does is
kind of an accessory. So when we

532
00:29:27,980 --> 00:29:31,280
go to a new area, it brings a
lot of goodwill to the, you

533
00:29:31,280 --> 00:29:34,160
know, to the people that are
getting in as well. You know,

534
00:29:34,160 --> 00:29:37,220
we're driving a robot on every
path that we can think of, you

535
00:29:37,220 --> 00:29:39,830
know, for delivery area, and
people are stopping us all the

536
00:29:39,830 --> 00:29:42,590
time asking questions, this
robots like oh, we're partnering

537
00:29:42,590 --> 00:29:45,680
with this person and and so and
then also gives us some good

538
00:29:46,190 --> 00:29:49,160
information as well for the
other connectivity, you know,

539
00:29:49,160 --> 00:29:52,790
LTE and stuff like that. So when
operators have to jump in, we

540
00:29:52,790 --> 00:29:55,340
got to make sure that we have
what carrier do we need to use

541
00:29:55,640 --> 00:29:58,910
for the different agencies? What
bands right Yeah, cuz

542
00:29:58,910 --> 00:30:02,510
everywhere. Everyone's A bit
different. And so obviously,

543
00:30:02,510 --> 00:30:06,140
what will work in here in Oregon
is not necessarily the best

544
00:30:06,140 --> 00:30:10,040
carrier down in Coronado,
California, which is may not be

545
00:30:10,040 --> 00:30:11,720
the best carrier in Atlanta,
Georgia.

546
00:30:11,840 --> 00:30:17,240
Audrow Nash: Yeah. Gotcha. Okay,
yeah. So you have a map of the

547
00:30:17,240 --> 00:30:19,940
environment that you get from
driving the robot through it.

548
00:30:20,480 --> 00:30:24,080
And then you have the robot
navigate autonomously, from

549
00:30:24,080 --> 00:30:29,120
point A to point B using kind of
like, somewhat labeled map in

550
00:30:29,120 --> 00:30:31,550
some way where someone says,
this is a crosswalk, this is a

551
00:30:31,550 --> 00:30:35,030
sidewalk, these kinds of things,
then you get to your

552
00:30:35,030 --> 00:30:40,880
destination, and the robot is
taken over by someone who's

553
00:30:40,880 --> 00:30:45,680
controlling it with a
teleoperation setup to actually

554
00:30:45,680 --> 00:30:48,410
do the delivery to it. Correct?

555
00:30:49,700 --> 00:30:52,430
Jason Richards: Correct. Yeah.
So that at this point, knowing

556
00:30:52,430 --> 00:30:55,280
that, you know, that's the
interaction. It's not just the

557
00:30:55,280 --> 00:30:57,800
delivery service, right. It's,
it's the experience of having a

558
00:30:57,800 --> 00:31:01,250
robot that is being nice to you.
Yeah, so that's exactly that's

559
00:31:01,250 --> 00:31:04,490
where a human takes over and is
able to be interactive. Yeah.

560
00:31:05,810 --> 00:31:09,980
Audrow Nash: And so for that
part, what does it look like,

561
00:31:09,980 --> 00:31:12,890
for from the humans perspective?
Are they sitting on some sort

562
00:31:12,890 --> 00:31:16,430
of, like, video call kind of
thing that sees what the robot

563
00:31:16,430 --> 00:31:19,100
sees? And they have a bunch of
buttons in front of them? Where

564
00:31:19,100 --> 00:31:22,820
they can say nod head like this,
make this noise? Like this kind

565
00:31:22,820 --> 00:31:25,280
of thing for picking the
behaviors? Or how does it work?

566
00:31:25,460 --> 00:31:28,910
Jason Richards: Yeah, a little
more simple than that. A couple

567
00:31:28,910 --> 00:31:31,550
joysticks, you know, that have,
you know, a lot of lot of pre

568
00:31:31,550 --> 00:31:35,030
programs into the joysticks, you
know, kind of your over your

569
00:31:35,060 --> 00:31:37,400
overview map, you can kind of
see where the robots were at,

570
00:31:37,430 --> 00:31:40,520
you know, you can monitor
several robots at a time. You

571
00:31:40,520 --> 00:31:42,890
know, making sure they're
staying on path. And then if one

572
00:31:42,890 --> 00:31:46,370
needs help, you know, the
operator is able to go look into

573
00:31:46,370 --> 00:31:49,550
it and see what they need to do
navigate around or end of

574
00:31:49,550 --> 00:31:54,830
destination, right. Being able
to, to contact the person. Let

575
00:31:54,830 --> 00:31:57,110
them know that they're their
personal copy that's funny for

576
00:31:57,110 --> 00:32:00,380
the delivery, like Uber Eats or
DoorDash or something. Yeah,

577
00:32:00,380 --> 00:32:02,330
when you have to call them and
be like, where

578
00:32:02,330 --> 00:32:06,980
Audrow Nash: are you at? Yeah.
Yep. That's funny. So, okay.

579
00:32:08,960 --> 00:32:13,310
Let's see. So then, so they're
using a simple interface to

580
00:32:13,310 --> 00:32:18,830
control the robot while they're
doing that interaction. You were

581
00:32:18,830 --> 00:32:21,350
saying that eventually, maybe
you could automate this kind of

582
00:32:21,350 --> 00:32:25,010
thing? Or you or at least part
of the interaction now that

583
00:32:25,010 --> 00:32:29,810
you're generating data? How and
then do you have the customer

584
00:32:29,810 --> 00:32:35,000
like rate how enjoyable it was?
Or different metrics of it. So

585
00:32:35,000 --> 00:32:38,210
you can see how the interaction
was and what parts of it they

586
00:32:38,210 --> 00:32:40,400
liked and disliked and whatever?

587
00:32:40,850 --> 00:32:45,020
Jason Richards: Yeah, good
question. So we, so there's not

588
00:32:45,020 --> 00:32:48,290
like a Dax app? For example,
people aren't ordering food

589
00:32:48,320 --> 00:32:49,010
through

590
00:32:49,040 --> 00:32:50,780
Audrow Nash: us to get not Yeah,
actually.

591
00:32:51,980 --> 00:32:56,240
Jason Richards: Yeah. Well,
it's, it's, it's an interesting

592
00:32:56,240 --> 00:32:58,910
paradigm. We work with the
restaurant and so we'd rather do

593
00:32:59,120 --> 00:33:03,230
integration with people so that
it's their brand. We just happen

594
00:33:03,230 --> 00:33:06,110
to be the Korea robot that's
delivered. Yep. Yeah, kinda

595
00:33:06,110 --> 00:33:09,050
like, you know, I lived in
Seattle and Boeing, obviously

596
00:33:09,050 --> 00:33:12,740
the a great company. But, you
know, a Boeing airplane could

597
00:33:12,740 --> 00:33:15,530
have Alaska Airlines on the
side, or it could have delta or

598
00:33:15,530 --> 00:33:19,490
United or whomever, right. So.
So when I go to get on my plane,

599
00:33:19,490 --> 00:33:22,040
I'm not saying hey, I'm on a but
am I on a Boeing? I'm no, I'm,

600
00:33:22,160 --> 00:33:24,560
I'm with delta this time, or
Alaska or whomever? Right? So

601
00:33:24,590 --> 00:33:30,920
yeah, it's kind of the same, you
know, kind of feel. But, but

602
00:33:30,920 --> 00:33:33,320
with that, yeah, so especially
here in our local town, it's a

603
00:33:33,320 --> 00:33:35,930
small town. And so being able
to, you know, we've got their

604
00:33:35,930 --> 00:33:37,790
phone number, and then we can
ask, you know, direct

605
00:33:37,970 --> 00:33:42,350
conversations, and see what
people like and don't like we we

606
00:33:42,350 --> 00:33:46,160
did a test at a retirement
community as well. I mean, the

607
00:33:46,160 --> 00:33:49,190
assumptions that we had were
kids, we know kids like robots,

608
00:33:49,370 --> 00:33:52,850
which I think back to when I was
a little kid, and I've got a

609
00:33:52,880 --> 00:33:55,160
I've got a eight year old and a
two year old, and to them

610
00:33:55,160 --> 00:33:57,110
growing up. Oh, yeah. Robots
around the street. Like it's

611
00:33:57,110 --> 00:34:01,670
just kind of a normal thing,
right? Yeah. But so I'd imagine

612
00:34:01,670 --> 00:34:04,910
kids probably are good with it.
And they do they love Dax,

613
00:34:05,300 --> 00:34:08,660
people, our age are, are a lot
more amenable to Dax as well,

614
00:34:08,660 --> 00:34:12,170
because it's cool technology. So
we didn't know how it was going

615
00:34:12,170 --> 00:34:17,060
to be received with people in
their 90s and, shockingly, Dax

616
00:34:17,060 --> 00:34:21,770
were all for overseas. Yeah. Oh,
yeah. Again, because it's not

617
00:34:21,800 --> 00:34:26,060
it's not a machine to them. It's
it's an entity. Right? So when

618
00:34:26,090 --> 00:34:29,510
you watch Star Wars, for
example, are 2d to into machine.

619
00:34:30,410 --> 00:34:34,190
He is but you don't perceive
him? Yeah, you see, you perceive

620
00:34:34,190 --> 00:34:37,880
him as an as an entity, you
know, somebody that you are

621
00:34:37,880 --> 00:34:40,820
having conversation with, not
just, you know, directing to do

622
00:34:40,820 --> 00:34:44,780
a thing so so in the same way
when people see Dax, you know,

623
00:34:44,780 --> 00:34:47,270
and it's funny, I do the same
thing. I know what's behind you

624
00:34:47,270 --> 00:34:50,540
know, I know you know, all the
inner workings of the robot, but

625
00:34:50,540 --> 00:34:52,760
I'll see him on the street while
I'm, you know, going to get a

626
00:34:52,760 --> 00:34:57,080
taco. And I'm like, Hey, Dad,
I'll say hi. And it's so funny.

627
00:34:57,170 --> 00:35:00,260
And obviously I know you know,
it, doesn't it You know, I'm

628
00:35:00,260 --> 00:35:04,340
saying but it's, it's definitely
an experience until somebody

629
00:35:04,370 --> 00:35:07,040
experiences or sees the
difference between, you know,

630
00:35:07,040 --> 00:35:10,370
just you know, a cart that can
deliver something to you versus

631
00:35:10,430 --> 00:35:14,810
having an interaction with a
beam. That is a machine. It's

632
00:35:15,710 --> 00:35:16,490
starkly different.

633
00:35:17,810 --> 00:35:20,330
Audrow Nash: Now. So you guys
are deploying in your, in your

634
00:35:20,330 --> 00:35:24,410
town. So you're in Portland,
right? I haven't been to No,

635
00:35:24,410 --> 00:35:25,970
Jason Richards: not poor. Oh,
where are you? Yeah, not

636
00:35:25,970 --> 00:35:29,930
Portland. We're. So Oregon State
University Corvallis, which is

637
00:35:29,930 --> 00:35:32,870
about an hour and a half south.
And we actually are in a small

638
00:35:32,870 --> 00:35:37,700
suburb of that in a little town
called fellowmen. But yeah, so

639
00:35:37,730 --> 00:35:39,800
not Portland. Not Not a huge
Metro.

640
00:35:41,420 --> 00:35:44,060
Audrow Nash: But yeah, small
town. How many people to the

641
00:35:44,090 --> 00:35:46,850
town? Oh, about 5000. Wow. So

642
00:35:46,850 --> 00:35:49,550
Jason Richards: small. It's
crazy. Small town. Yeah. Small

643
00:35:49,550 --> 00:35:52,040
town. I moved down here from
Seattle. It's a big change. It

644
00:35:52,040 --> 00:35:55,040
was it was good. Yeah. Yeah. I
grew up in it. I grew up in

645
00:35:55,040 --> 00:35:58,520
Montana personally. So it wasn't
changed to go back. Yeah. Back

646
00:35:58,520 --> 00:36:01,880
to a small town. Right. Yeah,
we've got robots now that are in

647
00:36:02,300 --> 00:36:06,740
Coronado, doing food delivery.
Down there. That's been fun. We

648
00:36:06,740 --> 00:36:10,730
got some robots that are
shipping out to Georgia. Cool.

649
00:36:10,730 --> 00:36:13,790
We next week Expo Hell yeah. So
we're getting packaged up and

650
00:36:14,120 --> 00:36:16,370
shipped to Georgia. And then
different deployments. I think

651
00:36:16,370 --> 00:36:18,260
Long Beach. We've got some
robots in Long Beach right now.

652
00:36:18,290 --> 00:36:22,880
Yeah. Deliver stuff. And then.
Yeah, and more people are

653
00:36:22,880 --> 00:36:26,090
contacting us all the time. Oh,
yeah. Yeah,

654
00:36:26,120 --> 00:36:29,450
Audrow Nash: how many? How many
robots are deployed? Right now?

655
00:36:29,450 --> 00:36:30,440
Would you estimate?

656
00:36:32,990 --> 00:36:35,060
Jason Richards: What really, I
could probably do the math in my

657
00:36:35,060 --> 00:36:38,630
head. It's not a lot. So we've
got the deployment in Coronado,

658
00:36:39,680 --> 00:36:40,520
Long Beach,

659
00:36:40,550 --> 00:36:43,310
Audrow Nash: we definitely use
it on the order. Georgia, or is

660
00:36:43,310 --> 00:36:44,180
it on the order of like,

661
00:36:44,210 --> 00:36:47,000
Jason Richards: yeah, that's
about it's about, we've got, you

662
00:36:47,000 --> 00:36:49,130
know, got more robots in that
that are coming off the

663
00:36:49,130 --> 00:36:52,040
production line, but we've got
actually in the field working.

664
00:36:52,430 --> 00:36:53,270
We've got about

665
00:36:54,260 --> 00:36:58,520
Audrow Nash: about 10 Yeah, 910.
Gotcha. They're doing stuff.

666
00:36:58,970 --> 00:37:04,370
Okay. Interesting. How large are
you guys as a company? Like how

667
00:37:04,370 --> 00:37:05,450
many people are involved?

668
00:37:06,500 --> 00:37:09,320
Jason Richards: Yeah, so we've
got there's a 18 of us, Hey,

669
00:37:09,320 --> 00:37:12,320
we've got the manufacturing. So
we got the manufacturing side.

670
00:37:12,890 --> 00:37:15,980
That is, you know, getting
robots, you know, off the line,

671
00:37:16,040 --> 00:37:18,620
then obviously, the operation
side and servicing, you know,

672
00:37:18,920 --> 00:37:23,090
like, getting data on that, you
know, how long do you have to

673
00:37:23,150 --> 00:37:26,210
run a robot before the tread
needs to get replaced? Yeah. How

674
00:37:26,210 --> 00:37:27,470
many miles on their tires, you

675
00:37:27,470 --> 00:37:30,650
Audrow Nash: know, imagine where
you were out fairly fast, too,

676
00:37:30,770 --> 00:37:31,670
which is interesting,

677
00:37:31,700 --> 00:37:34,580
Jason Richards: surprisingly,
not really, actually have lasted

678
00:37:34,580 --> 00:37:37,550
a long time, we finally we
finally got one that we're like,

679
00:37:37,550 --> 00:37:40,130
Hey, this looks like it's
certain to how many were out. We

680
00:37:40,130 --> 00:37:44,150
should swap this out. I don't
actually this actually just

681
00:37:44,150 --> 00:37:47,120
happened. I think it was either
earlier this week or late last

682
00:37:47,120 --> 00:37:49,400
week. And so I asked her
Operations Manager, like figure

683
00:37:49,400 --> 00:37:52,160
out how many hours we put on
this. Yeah, we can put that in

684
00:37:52,160 --> 00:37:56,960
our service manual. But yeah.
And then obviously, they the

685
00:37:56,960 --> 00:37:59,240
engineering team, you know,
mechanical and electrical and

686
00:37:59,240 --> 00:38:02,900
software and firmware, and yeah,
so Yeah.

687
00:38:03,620 --> 00:38:08,990
Audrow Nash: Gotcha. Cool. And
how are you guys funded? For

688
00:38:08,990 --> 00:38:11,300
this kind of thing? Have you
been accepting investment? Or

689
00:38:11,300 --> 00:38:14,510
like, what kind of where are you
at in terms of funding?

690
00:38:15,680 --> 00:38:20,450
Jason Richards: It's good
question. So um, so in 2002,

691
00:38:21,560 --> 00:38:23,840
brothers, I'll tell you the
story of the founder. So two

692
00:38:23,840 --> 00:38:29,420
brothers, they started an
internet an ISP. dead broke. I

693
00:38:29,420 --> 00:38:31,760
mean, they, they were volunteer
firefighters lived at the fire

694
00:38:31,760 --> 00:38:34,850
station and paid themselves $50
A week, a month kind of thing,

695
00:38:34,880 --> 00:38:38,030
you know, crazy. barely getting
by got it got it up and running.

696
00:38:38,330 --> 00:38:42,110
And then in 2011, or so kind of
limping along there. Since they

697
00:38:42,110 --> 00:38:45,860
were firefighters. One of the
brothers had this idea because

698
00:38:45,890 --> 00:38:48,980
they went to a fire call, they
got to the fire station. And the

699
00:38:48,980 --> 00:38:50,660
fire trucks were coming back
from the call, by the time they

700
00:38:50,660 --> 00:38:53,300
got there. He's like, this is
stupid. Like, we have the

701
00:38:53,330 --> 00:38:56,810
iPhone. I'm gonna build, I'm
gonna build an app. And so he

702
00:38:56,810 --> 00:39:01,250
built this app called active
911. And last I heard it's about

703
00:39:01,250 --> 00:39:03,920
40% of firefighters in North
America use this app called

704
00:39:03,920 --> 00:39:07,940
active. Wow. So it's done very,
very well. And by his that

705
00:39:07,970 --> 00:39:11,210
Joseph and Joseph goal was
always to have this r&d lab that

706
00:39:11,210 --> 00:39:14,960
I talked about. So in 2015, they
started Novo dynamics. And then

707
00:39:14,960 --> 00:39:17,720
Dax was kind of the first big
project that they said, Okay,

708
00:39:17,720 --> 00:39:20,840
five years from now, I think
robots doing delivery is going

709
00:39:20,840 --> 00:39:23,600
to be a thing. So let's start
working towards that end, and

710
00:39:23,600 --> 00:39:26,060
more importantly, working
towards the future that people

711
00:39:26,060 --> 00:39:28,850
want to see, you know, when, you
know, again, when people are

712
00:39:28,850 --> 00:39:32,090
thinking the future of electric
cars, electric cars have been

713
00:39:32,090 --> 00:39:35,030
around, you know, since the 80s.
Yeah, you know, long people have

714
00:39:35,030 --> 00:39:38,720
tried to do electric cars. But
it wasn't until Tesla before

715
00:39:38,720 --> 00:39:40,820
now, it's something that people
want, right? People are like,

716
00:39:40,850 --> 00:39:44,270
Oh, that's the vision of the
future that we want to see not

717
00:39:44,300 --> 00:39:48,110
these weird box things that are
glorified golf carts. Yeah,

718
00:39:48,140 --> 00:39:51,980
right. So in the same way, you
know, with the robot is solving

719
00:39:51,980 --> 00:39:55,040
that, you know, what kind of
thing do we want in pedestrian

720
00:39:55,040 --> 00:39:59,420
spaces? That isn't going to be
creepy, that people are going to

721
00:39:59,450 --> 00:40:02,030
want to attack themselves to
they can do helpful things for

722
00:40:02,030 --> 00:40:05,930
people. So that was that was the
idea. So the r&d portion is all

723
00:40:05,930 --> 00:40:08,930
been self funded, you know,
because these other two

724
00:40:08,930 --> 00:40:12,020
companies that have done well,
it's been self funded up until

725
00:40:12,050 --> 00:40:15,860
about a month ago. And so about
a month ago, I was actually, you

726
00:40:15,860 --> 00:40:17,600
know, January when we said, we
need to, we need to

727
00:40:17,600 --> 00:40:19,790
commercialize this thing, what
do we need to do we need to get

728
00:40:19,790 --> 00:40:23,030
investment. Because, you know,
building you know, 20 robots is

729
00:40:23,030 --> 00:40:26,750
one thing, building a fleet of
them. Hundreds is a totally

730
00:40:26,750 --> 00:40:30,320
different ballgame. So. So it's
actually just about a month ago

731
00:40:30,350 --> 00:40:34,490
that we opened up a crowd,
crowdfunding campaign through

732
00:40:34,520 --> 00:40:38,660
start engine, which is the
intermediary, so so it's kicked

733
00:40:38,660 --> 00:40:42,350
off. And this first time I've,
you know, I've been involved in

734
00:40:42,620 --> 00:40:46,700
lots of companies started a
couple myself, but I've never

735
00:40:46,760 --> 00:40:49,700
gone after investors. So it's a
whole new experience for me. You

736
00:40:49,700 --> 00:40:54,680
mean crowdfunding specifically?
Or? Or even venture capital? You

737
00:40:54,680 --> 00:40:57,290
know, filing with the SEC? Yeah.
Never. I've always been

738
00:40:57,290 --> 00:41:00,140
bootstrapped. Yep. Right. We've
always bootstrapped company. So

739
00:41:00,140 --> 00:41:02,600
this is the first time going
after investors. And it's really

740
00:41:02,600 --> 00:41:06,260
interesting, because the the
majority of people that don't

741
00:41:06,260 --> 00:41:08,210
know us, you know, they don't
know who we are. And they don't

742
00:41:08,210 --> 00:41:11,660
understand that, that the two
brothers like, people that know

743
00:41:11,660 --> 00:41:14,180
the two brothers, a lot of
people that know them have

744
00:41:14,180 --> 00:41:18,530
invested quite a bit because
they paid another venture. Yeah.

745
00:41:18,710 --> 00:41:21,980
Yeah. And yeah, and they, they
seen the robot, obviously, our

746
00:41:21,980 --> 00:41:24,710
hometown, couldn't ask for a
better reception, you know, from

747
00:41:24,710 --> 00:41:26,780
the investing standpoint,
because they're like, No, this

748
00:41:26,780 --> 00:41:28,850
is our robot. This is our
hometown, and, you know,

749
00:41:29,630 --> 00:41:32,630
friendly neighborhood robot that
we're investing in. But people

750
00:41:32,630 --> 00:41:37,250
that don't know us, that have
invested. You know, we're saying

751
00:41:37,280 --> 00:41:39,980
over $1,000, you know, $1,000 or
more, yeah, I've been reaching

752
00:41:39,980 --> 00:41:41,870
out to those people. And
predominantly, they're

753
00:41:41,870 --> 00:41:44,660
engineers, you know, it's people
that, that see what we're doing.

754
00:41:44,690 --> 00:41:48,260
And they say, Oh, this is cool.
Like, this is a cool version or

755
00:41:48,260 --> 00:41:50,720
vision of the future that you
guys have. And we want to be a

756
00:41:50,720 --> 00:41:56,990
part of that. So yeah, so that's
where learning, you know, how do

757
00:41:56,990 --> 00:41:59,810
we how do we get that message
out to more people? So not just

758
00:41:59,810 --> 00:42:02,840
engineers, but everyone that's
it seems to be? It seems to be

759
00:42:02,840 --> 00:42:06,290
interesting that that's the
community that is really liking

760
00:42:06,290 --> 00:42:06,800
what we're doing.

761
00:42:06,920 --> 00:42:08,750
Audrow Nash: So yeah, well,
they're interested. I don't

762
00:42:08,750 --> 00:42:11,420
know, as an engineer, I'm
interested in robots. And I

763
00:42:11,420 --> 00:42:13,460
imagine it's true for a lot of
other engineers.

764
00:42:14,240 --> 00:42:15,710
Jason Richards: It's true
anyway. And it's not just you

765
00:42:15,710 --> 00:42:18,560
know, robotics into you know,
every discipline you can imagine

766
00:42:18,560 --> 00:42:22,280
civil engineers are like, wow,
this is. So it's kind of cool.

767
00:42:22,790 --> 00:42:25,910
Audrow Nash: So, how was the
crowdfunding? What are your

768
00:42:25,910 --> 00:42:29,990
thoughts on crowdfunding after?
So is it still alive? Or yeah,

769
00:42:30,050 --> 00:42:32,690
it's still alive. So at the time
of this interview, it is how

770
00:42:32,690 --> 00:42:35,870
long will it stay live until?
Because it may be I said, it'll

771
00:42:35,870 --> 00:42:38,060
be like three weeks before this
is published. So

772
00:42:38,240 --> 00:42:42,350
Jason Richards: yeah, so the
the, the raise is going through

773
00:42:42,380 --> 00:42:46,340
either you hit the the first
goal that, you know, we can hit

774
00:42:46,340 --> 00:42:50,720
with what we filed with the SEC.
So we're looking for a million

775
00:42:50,720 --> 00:42:53,180
dollars by the end of the year
is kind of the the goal and the

776
00:42:53,180 --> 00:42:55,970
target that we have, we're about
a third of the way there, you

777
00:42:55,970 --> 00:43:00,500
know, after been going for about
a month, so it will either will

778
00:43:00,500 --> 00:43:05,240
either hit that that mark, and
the fund will shut off, or I

779
00:43:05,240 --> 00:43:08,720
believe it goes until I think
it's a six month campaign. So it

780
00:43:08,720 --> 00:43:11,090
would go until until April of
next year.

781
00:43:11,090 --> 00:43:13,160
Audrow Nash: Oh, hell yeah.
Okay, good. So this episode will

782
00:43:13,160 --> 00:43:17,990
air while it is still going if
it goes to the six month time.

783
00:43:18,200 --> 00:43:22,730
Awesome. Yeah. What have you
thought of crowdfunding? It's an

784
00:43:22,730 --> 00:43:25,970
interesting model. Does each
person get some equity? Or

785
00:43:25,970 --> 00:43:29,000
they're just supporting you? Or
they like get a tax robot? Or

786
00:43:29,210 --> 00:43:29,990
how does it work?

787
00:43:31,490 --> 00:43:34,550
Jason Richards: Yeah, so there,
yeah, there are different ways

788
00:43:34,580 --> 00:43:38,300
of crowdfunding? Yes. You know,
we're a nonprofit or something,

789
00:43:38,300 --> 00:43:40,400
it's not equity, what we're
doing is actually is actually

790
00:43:40,460 --> 00:43:43,940
equity. So you're actually
getting stuck in the company,

791
00:43:44,000 --> 00:43:47,810
which is, is kind of cool. So
I've heard horror stories of

792
00:43:47,810 --> 00:43:48,260
that,

793
00:43:48,800 --> 00:43:51,050
Audrow Nash: where you have
funding crowdfunding, where you

794
00:43:51,050 --> 00:43:54,410
give them equity, because like,
you'll have to go and get

795
00:43:54,410 --> 00:43:57,320
permission from everyone for all
the decisions and eventually,

796
00:43:57,320 --> 00:44:01,700
just like veure bureaucracy
weighs down the company no

797
00:44:01,700 --> 00:44:05,870
longer gets anywhere. That I
guess you have clear about their

798
00:44:05,870 --> 00:44:08,120
terms kind of thing, if you
guess.

799
00:44:08,450 --> 00:44:10,880
Jason Richards: Yeah, and
everybody so yeah, start engine

800
00:44:10,880 --> 00:44:14,090
is the platform that we're
using. And part of it is that

801
00:44:14,090 --> 00:44:17,930
yes, you do get common, you
know, common stock, you know,

802
00:44:17,930 --> 00:44:21,530
which is you have shares in the
company. But you also are

803
00:44:21,590 --> 00:44:24,200
essentially signing over your
vote to the CEO, which is me.

804
00:44:24,560 --> 00:44:27,710
So, so when anybody says yes,
you know, I'm going to put

805
00:44:27,740 --> 00:44:30,530
$1,000 into Dax and get, you
know, however many shares

806
00:44:30,920 --> 00:44:34,820
they're also saying that that,
okay, I know that you run

807
00:44:34,820 --> 00:44:37,190
minority shareholder, yep.
Right. So I am your

808
00:44:37,190 --> 00:44:39,680
representative. So I am the vote
for the shareholder. So

809
00:44:39,680 --> 00:44:43,100
obviously, if people want to
talk to me, obviously and let

810
00:44:43,100 --> 00:44:45,080
their voice be heard, you know,
kind of like a representative

811
00:44:45,080 --> 00:44:47,780
government, you know, cool and
Okay, I like that a good way

812
00:44:47,780 --> 00:44:50,660
better talk to me. Yeah. And
then same thing where it's

813
00:44:50,690 --> 00:44:53,330
exactly cuz otherwise, you know,
your shareholders meetings.

814
00:44:54,320 --> 00:44:57,260
Everybody's there by proxy, and
I'm the proxy and I'm the boat

815
00:44:57,290 --> 00:45:00,890
so, so it doesn't really bog us
down administrate If that makes

816
00:45:00,890 --> 00:45:06,200
sense, yes, the cap table is, is
big. And usually people that

817
00:45:06,200 --> 00:45:08,990
don't understand technology very
well and the liberalization,

818
00:45:10,020 --> 00:45:12,330
Audrow Nash: which is where the
monopolization goes, right? Like

819
00:45:12,330 --> 00:45:14,280
or who owns what? That's
typically correct,

820
00:45:14,280 --> 00:45:16,410
Jason Richards: right, right.
Yeah. Who owns what? Yeah. So if

821
00:45:16,410 --> 00:45:20,250
you buy, you know, 50 shares in
index bought, your name goes on

822
00:45:20,250 --> 00:45:24,120
the list of like, okay, you are
an investor. So Oh, yeah.

823
00:45:24,450 --> 00:45:27,390
Audrow Nash: Okay. That's crazy.
Yeah, that's, I will at least we

824
00:45:27,390 --> 00:45:30,030
have like, you can automate
everything. So you don't really

825
00:45:30,030 --> 00:45:31,920
have to do too much. Probably.

826
00:45:32,160 --> 00:45:33,810
Jason Richards: No, that's,
that's all gonna say, yeah,

827
00:45:33,810 --> 00:45:37,320
there's not and and people that
don't know technology very well,

828
00:45:37,350 --> 00:45:40,050
they have in their mind that
you're looking, you know, that

829
00:45:40,050 --> 00:45:42,630
many stamps, you know,
envelopes, as you're sending out

830
00:45:42,630 --> 00:45:45,930
like, requests, you know, to
shareholders and things like

831
00:45:45,930 --> 00:45:49,680
that, like, No, you put it in a
CSV file, and boom, everybody

832
00:45:49,680 --> 00:45:50,280
gets a mailer.

833
00:45:50,549 --> 00:45:54,329
Audrow Nash: So how, how much?
How much? So you said, your goal

834
00:45:54,329 --> 00:45:57,659
is a million, you're a third of
the way there, it's been a

835
00:45:57,659 --> 00:46:02,789
month, you have five more months
or so? How many, so in that

836
00:46:02,819 --> 00:46:07,079
third of a million dollars,
which you've collected? How many

837
00:46:07,079 --> 00:46:08,969
people are involved in that?

838
00:46:10,170 --> 00:46:13,200
Jason Richards: A little A
little over 200. So,

839
00:46:14,010 --> 00:46:16,170
Audrow Nash: okay, so it's
mostly like at least investing,

840
00:46:17,070 --> 00:46:21,060
at least investing like 1500 per
person, on average kind of

841
00:46:21,060 --> 00:46:21,240
thing.

842
00:46:21,900 --> 00:46:24,360
Jason Richards: That's, that's
the average. And we had to, we

843
00:46:24,360 --> 00:46:27,600
had to have a minimum that was
above 100. And so what we wanted

844
00:46:27,600 --> 00:46:31,440
to do, and back back to the
large cap table, right, the

845
00:46:31,440 --> 00:46:34,470
number of people that would be
on this, this list of investors,

846
00:46:34,890 --> 00:46:38,040
people were saying, you know,
don't make it too big, but at

847
00:46:38,040 --> 00:46:42,000
the same time, we have a lot of
fans. We call it the DAX fam.

848
00:46:42,810 --> 00:46:45,300
You know, and we wanted, we
wanted people to be a part. And

849
00:46:45,300 --> 00:46:48,300
so we said, Okay, we need to
have a minimum, that minimum

850
00:46:48,420 --> 00:46:53,490
entries 170 bucks, so that
people can be, you know, an

851
00:46:53,490 --> 00:46:54,630
owner of DAX but why

852
00:46:54,630 --> 00:46:55,410
Audrow Nash: is that arbitrary?

853
00:46:55,440 --> 00:46:55,920
Jason Richards: Weaver?

854
00:46:56,040 --> 00:46:57,600
Audrow Nash: Why $170?

855
00:46:58,590 --> 00:47:01,080
Jason Richards: Yeah, so we I
like round numbers personally.

856
00:47:01,170 --> 00:47:06,150
And so our, it's 169 50. And so
our with our current valuation

857
00:47:06,150 --> 00:47:08,670
and what the stock price is, and
ends up being 50 shares, we have

858
00:47:08,670 --> 00:47:11,550
to we had to have a number over
$100. Okay, and so we said,

859
00:47:11,550 --> 00:47:16,620
Okay, well, then 50 shares, so
50 shares is 169 5339 shares.

860
00:47:16,650 --> 00:47:20,070
Oh, I see. Okay, so that's how
it works out simple. Yeah. So I

861
00:47:20,070 --> 00:47:23,250
just made it that was it was
arbitrary, but it was a, okay, I

862
00:47:23,250 --> 00:47:26,670
have 50 shares of dexpot. Right,
that became kind of the lowest

863
00:47:26,730 --> 00:47:29,670
investment that you could do,
and then up to to answer your

864
00:47:29,670 --> 00:47:34,020
question. So up to $50,000 is,
is, you know, 50,000. and above,

865
00:47:34,020 --> 00:47:37,050
if somebody wants to invest at
that level, we said, yeah, we'll

866
00:47:37,050 --> 00:47:39,090
fly back to your location, and
we'll have them deliver

867
00:47:39,090 --> 00:47:42,150
something to you. Cool. So we
have nobody's, nobody's taking

868
00:47:42,150 --> 00:47:45,120
us up on the 50,000 yet, but
we've had had a few investments

869
00:47:45,120 --> 00:47:49,380
that invested down, you know,
several $1,000. And the average

870
00:47:49,380 --> 00:47:52,920
is about 1500. But I think
there's a lot of investors, the

871
00:47:52,920 --> 00:47:56,070
majority of the investors, if
you will, are about $1,000,

872
00:47:56,100 --> 00:47:59,850
guys, so it's been a good
reception. And because it's I

873
00:47:59,850 --> 00:48:03,090
don't know if I know this
correctly, but this is my

874
00:48:03,090 --> 00:48:06,420
impression, and you can correct
me, because it's relatively

875
00:48:06,420 --> 00:48:09,690
small amounts of money that
people are investing. They don't

876
00:48:09,690 --> 00:48:13,800
have to meet the bar, the people
investing don't have to meet the

877
00:48:13,800 --> 00:48:14,910
bar for

878
00:48:15,930 --> 00:48:18,120
Audrow Nash: being like an angel
investor or something, which I

879
00:48:18,120 --> 00:48:22,080
think is like a million in
assets or assets or something,

880
00:48:22,080 --> 00:48:26,340
and over some amount per year.
So anyone can invest in this

881
00:48:26,340 --> 00:48:26,970
kind of thing.

882
00:48:27,960 --> 00:48:30,990
Jason Richards: Yep, exactly.
And, and that was a law. And I

883
00:48:30,990 --> 00:48:33,540
might get it wrong. But it was,
you know, fairly recently, I

884
00:48:33,540 --> 00:48:36,810
think, when 10 years or maybe 15
years when the law came out.

885
00:48:37,140 --> 00:48:39,360
Before that. You're right.
Everything was an accredited

886
00:48:39,360 --> 00:48:42,750
investor, which, like you said,
massive amounts of either high

887
00:48:42,750 --> 00:48:46,440
income, high disposable income
or huge net worth that is in

888
00:48:46,440 --> 00:48:50,850
your house. Yeah. And so it's
very, very rare air to be an

889
00:48:50,850 --> 00:48:54,270
angel. Yeah, exactly. And so, so
with that, they said, okay,

890
00:48:54,330 --> 00:48:57,060
there are some rules around it.
So for example, if you go to

891
00:48:57,060 --> 00:49:00,090
start engine, and you say I want
to be an investor, they will ask

892
00:49:00,090 --> 00:49:02,640
you some questions about your
income and things like to

893
00:49:02,640 --> 00:49:05,190
confirm because there are
limits. Okay. Yeah. And to make

894
00:49:05,190 --> 00:49:08,520
sure that, that if you don't
have a lot of disposable income,

895
00:49:08,550 --> 00:49:11,310
you're not doing it going to
hurt yourself. Right. That's

896
00:49:11,310 --> 00:49:14,730
good. Exactly. So yeah, and
there are certain limits and

897
00:49:14,730 --> 00:49:17,250
things again, depending on
income and assets and things

898
00:49:17,250 --> 00:49:19,950
like that, so it doesn't hurt
anybody, but at the same time

899
00:49:20,100 --> 00:49:23,640
people can invest in in cool
startups like this totally. And

900
00:49:23,640 --> 00:49:24,060
it's kind of

901
00:49:25,170 --> 00:49:27,690
Audrow Nash: early to, yeah,
yeah,

902
00:49:27,750 --> 00:49:29,970
Jason Richards: absolutely. So
they can say oh, you know,

903
00:49:30,120 --> 00:49:32,820
people a lot of people like I
don't want to put my people that

904
00:49:32,850 --> 00:49:35,130
are okay putting money into
crypto are usually okay putting

905
00:49:35,130 --> 00:49:38,940
money into, into into startups,
right? They you know, it's not

906
00:49:38,940 --> 00:49:41,310
people they just want bonds,
right? Yeah, I don't want bonds

907
00:49:41,310 --> 00:49:45,210
and put it in that row massively
and I want to be something cool.

908
00:49:45,300 --> 00:49:49,590
So exactly. I get it more risk.
Yeah. Yeah. Yeah. So and again,

909
00:49:49,590 --> 00:49:54,180
it's not you know, for a lot of
people you know, $1,000 is not

910
00:49:54,180 --> 00:49:58,560
like a it's Yeah, doesn't
exactly doesn't break the bank

911
00:49:58,560 --> 00:50:00,960
but if it if it goes big like
that, pool. And again, it's

912
00:50:00,990 --> 00:50:04,230
again more a part of a, you
know, I've met people that

913
00:50:04,230 --> 00:50:06,420
invested in Microsoft when it
was nothing, you know, kind of

914
00:50:06,420 --> 00:50:09,330
thing. And it's like, yeah, wow,
you're probably enjoying that.

915
00:50:10,559 --> 00:50:14,159
Audrow Nash: So, so one thing
that I would be curious about if

916
00:50:14,159 --> 00:50:19,319
I was to invest in this is about
future rounds of funding,

917
00:50:19,889 --> 00:50:23,579
because it might be possible
that they get diluted through

918
00:50:23,609 --> 00:50:27,389
additional funding. How does
that look for like, yeah, just

919
00:50:27,389 --> 00:50:29,189
talk about that, because I don't
know too much about it.

920
00:50:29,279 --> 00:50:32,639
Jason Richards: Absolutely.
Yeah. I didn't either, you know,

921
00:50:32,669 --> 00:50:36,959
till a few months ago, when I
really had to learn a lot. So

922
00:50:36,959 --> 00:50:40,469
the way that it works is that
the goal when you go to each

923
00:50:40,469 --> 00:50:44,369
investment round is to the value
of the company wants to go up,

924
00:50:44,429 --> 00:50:47,279
right? That's, that's the goal.
So if the value of the company

925
00:50:47,279 --> 00:50:51,629
goes up, so does the amount of
dollars you know, easy math, if

926
00:50:51,629 --> 00:50:54,239
you invested $5, and then the
company, the value of the

927
00:50:54,239 --> 00:51:00,749
company doubles, then your $5
turns into $10. Right? Just, you

928
00:51:00,749 --> 00:51:04,349
know, hypothetically, yeah.
Okay, value goes up. But then

929
00:51:04,349 --> 00:51:06,839
right, then you say, Okay, we're
gonna go after other investors.

930
00:51:06,869 --> 00:51:09,659
So we're gonna add more shares
to this. Yeah, well, exactly.

931
00:51:09,659 --> 00:51:11,669
It's just gonna dilute your
shares, right. So your your

932
00:51:11,669 --> 00:51:14,939
share was $5. Now, it's $10.
Because we doubled it, but now

933
00:51:14,939 --> 00:51:17,369
we've put more shares in the
mix. So now my shares are worth

934
00:51:17,429 --> 00:51:21,989
eight. But it allows it allows
more fuel for more growth, if

935
00:51:21,989 --> 00:51:24,749
that makes sense. Yeah. And the
hope is that it grows again. So

936
00:51:24,749 --> 00:51:28,259
then that eight turns into 16.
And the company doubled again,

937
00:51:28,289 --> 00:51:31,829
right? So that's, that's
essentially how that works. And

938
00:51:31,859 --> 00:51:34,379
that's a very simple, very, very
soon. Yeah, but

939
00:51:34,380 --> 00:51:37,410
Audrow Nash: the dynamic, is
there. Yeah. And you just hope

940
00:51:37,410 --> 00:51:41,370
that it's not diluted to such an
extent that you make quite a bit

941
00:51:41,370 --> 00:51:45,210
at the end. If a company does
have some sort of liquidation

942
00:51:45,210 --> 00:51:45,600
event?

943
00:51:46,560 --> 00:51:48,630
Jason Richards: Yep. Yep. And
that's the goal. And I've, you

944
00:51:48,630 --> 00:51:52,260
know, talking to venture
capitalists and angels, and just

945
00:51:52,260 --> 00:51:55,110
kind of getting a sense for what
what are you looking for, you

946
00:51:55,110 --> 00:51:57,990
know, what is what does an angel
investor or a venture capitalist

947
00:51:57,990 --> 00:51:58,650
looking for?

948
00:51:58,680 --> 00:51:59,220
Audrow Nash: What do they think?

949
00:51:59,640 --> 00:52:03,630
Jason Richards: Most? Yeah. Most
of

950
00:52:03,630 --> 00:52:04,050
Audrow Nash: them

951
00:52:05,160 --> 00:52:06,750
Jason Richards: have no
experience with at least the

952
00:52:06,750 --> 00:52:09,600
ones that I know. I've talked to
most of them have no experience

953
00:52:09,600 --> 00:52:12,420
with it, because they haven't
had to write they they're that

954
00:52:12,420 --> 00:52:16,590
rare air of angel investors
accredited investors. So are

955
00:52:16,590 --> 00:52:19,260
they don't need to have an
investment firm or something.

956
00:52:19,410 --> 00:52:21,660
Exactly. This kind of Yep.
throwing around the money. A lot

957
00:52:21,660 --> 00:52:25,530
of people. Yeah. And since
again, because it's a fairly new

958
00:52:25,530 --> 00:52:28,110
concept in the scope of, you
know, the history of the world,

959
00:52:28,110 --> 00:52:32,550
it's a fairly new thing that you
can get into these crowdfunding

960
00:52:32,550 --> 00:52:35,520
rounds. They don't have a whole
lot of experience with it, but a

961
00:52:35,520 --> 00:52:38,640
lot of them, it seems that the
tides are turning a little bit.

962
00:52:38,670 --> 00:52:42,810
I'll give an example. So there's
a, an angel group are called the

963
00:52:42,810 --> 00:52:46,830
Oregon rain. And it's more for
small, small, small startups.

964
00:52:46,860 --> 00:52:50,280
You know, like, you know, I need
$30,000 to get this, this idea

965
00:52:50,280 --> 00:52:53,880
off the ground. Yeah. And so
because they're angels, and

966
00:52:53,880 --> 00:52:55,710
trying to figure out how they're
going to do it, they actually

967
00:52:55,710 --> 00:52:58,410
partnered with another
intermediary, I can't remember,

968
00:52:58,410 --> 00:53:00,750
but basically who it was, but
essentially, this other

969
00:53:00,750 --> 00:53:05,040
intermediary, like start engine
was going to give them a white

970
00:53:05,040 --> 00:53:07,770
label version of what does it
mean, programmable? That

971
00:53:08,160 --> 00:53:11,670
version? I don't know. Yeah. So
yeah, so a white label version

972
00:53:11,670 --> 00:53:17,250
would be. So for example, if I
have a software, let's call it

973
00:53:17,670 --> 00:53:21,660
Microsoft Office or Microsoft
Word, right? So a total a white

974
00:53:21,660 --> 00:53:22,770
liberal version amendment.

975
00:53:22,800 --> 00:53:23,220
Audrow Nash: Yeah.

976
00:53:24,840 --> 00:53:28,290
Jason Richards: Okay, so I can't
just I can't just say, hey, it's

977
00:53:28,320 --> 00:53:31,650
Microsoft Word. Yeah. But if I
had a white label version, if

978
00:53:31,650 --> 00:53:34,680
you came to me and said, I'm
going to give you you know, $300

979
00:53:34,680 --> 00:53:37,110
billion. So I have a white
labeled version. It's not

980
00:53:37,140 --> 00:53:41,370
Microsoft Word anymore. It's now
dexpot word, right? And so I the

981
00:53:41,370 --> 00:53:45,210
label, or you buy, essentially,
okay, you buy it, you buy the

982
00:53:45,210 --> 00:53:48,060
rights of the software, but they
put your name on it. And so

983
00:53:48,060 --> 00:53:51,330
that's so a white middle version
of startengine would be, you

984
00:53:51,330 --> 00:53:56,010
know, Oregon rain.com/investors.
But the actual software was not

985
00:53:56,010 --> 00:53:59,370
written by Oh, yes. Investors,
okay, by another company. Yeah,

986
00:53:59,370 --> 00:54:03,120
that makes sense. So, so, so I,
it looks like it's becoming

987
00:54:03,120 --> 00:54:06,960
more. There's more awareness
around crowd fund so that in

988
00:54:06,960 --> 00:54:08,190
these angel groups seems like,

989
00:54:08,670 --> 00:54:11,280
Audrow Nash: okay, that's what
you were. So I was a bit lost

990
00:54:11,280 --> 00:54:14,250
for a sec. You were saying
basically, that's an example of

991
00:54:14,310 --> 00:54:17,670
crowdfunding going into
something that is a bit larger.

992
00:54:17,670 --> 00:54:20,100
Yes. With investors or
something?

993
00:54:20,460 --> 00:54:23,850
Jason Richards: Right. Yeah. The
angels. They are they know of

994
00:54:23,850 --> 00:54:26,730
it, but they haven't
participated. But it seems to be

995
00:54:28,200 --> 00:54:30,270
just the research that I've been
doing, it seems to be that

996
00:54:30,300 --> 00:54:33,390
they're more open and receptive
to it. Interesting, more,

997
00:54:33,390 --> 00:54:36,210
because you have companies that
have gone through crowdfunding

998
00:54:36,210 --> 00:54:38,760
rounds, and they've they've done
they've done it successfully,

999
00:54:38,760 --> 00:54:42,660
meaning that they, they got the
money, and then they executed on

1000
00:54:42,900 --> 00:54:46,920
the goal correctly. They started
getting revenues. And now they

1001
00:54:46,920 --> 00:54:50,670
have revenue. So now they could
go into like an angel investment

1002
00:54:50,670 --> 00:54:55,290
group or venture capitalists and
say, here's our revenues. And

1003
00:54:55,290 --> 00:54:58,500
here's the funding that we got
before. Now, would you you know,

1004
00:54:58,500 --> 00:55:01,410
invest in this and there Now
seeing because they're seeing

1005
00:55:01,410 --> 00:55:04,710
this happening more often,
they're more privy to the fact

1006
00:55:04,710 --> 00:55:08,070
that, oh, we might want to look
into investing in crowdfunding

1007
00:55:08,070 --> 00:55:08,940
options ourselves.

1008
00:55:09,810 --> 00:55:12,600
Audrow Nash: Why would so why
would a company go into

1009
00:55:12,600 --> 00:55:18,600
crowdfunding as opposed to
seeking investment from, I don't

1010
00:55:18,600 --> 00:55:22,830
know, like a, like a pre seed or
some sort of company or some

1011
00:55:22,830 --> 00:55:25,710
sort of angel investor or
something like this, because I

1012
00:55:25,710 --> 00:55:30,240
understand that one of the
benefits of having investors

1013
00:55:30,240 --> 00:55:35,370
come together for your company
is that they can help you make

1014
00:55:35,670 --> 00:55:38,040
while they can connect you to
the right people, or they can

1015
00:55:38,040 --> 00:55:42,750
mentor you. So you avoid
potholes. Why would you choose

1016
00:55:43,560 --> 00:55:48,930
crowdfunding over a specific
like one larger investor? Kind

1017
00:55:48,930 --> 00:55:49,140
of thing?

1018
00:55:49,140 --> 00:55:53,340
Jason Richards: Yeah. Yeah, to
two main reasons. So January,

1019
00:55:53,340 --> 00:55:56,820
then back January of this year,
when we were looking at going

1020
00:55:56,820 --> 00:56:01,920
after investment, reached out to
some venture capitalists and

1021
00:56:01,920 --> 00:56:06,840
friends of friends that we have,
but no people. And essentially

1022
00:56:06,840 --> 00:56:10,620
all of them said, Okay, well,
what are your revenues? Zero.

1023
00:56:10,770 --> 00:56:16,020
Okay, so if your revenues are
zero, that's a pretty big

1024
00:56:16,020 --> 00:56:19,170
gamble. Right? You're for them.
That's the perception market

1025
00:56:19,170 --> 00:56:19,470
event

1026
00:56:19,530 --> 00:56:21,540
Audrow Nash: or these kinds of
things. So neurons,

1027
00:56:21,660 --> 00:56:24,120
Jason Richards: okay. Yeah.
Yeah. So I did at this point,

1028
00:56:24,120 --> 00:56:26,040
they would look at that and say,
Oh, it looks like it's still

1029
00:56:26,070 --> 00:56:28,140
like a research phase. It's
like, Yeah, but we want to go

1030
00:56:28,140 --> 00:56:30,900
from a research phase into a
commercialized phase. So in

1031
00:56:30,900 --> 00:56:34,620
order to make that jump, it's
going to require money. And so I

1032
00:56:34,920 --> 00:56:39,840
was talking to one, one venture
capitalist who he been on the

1033
00:56:39,840 --> 00:56:42,270
other side of the table to write
he had he had startups gone

1034
00:56:42,270 --> 00:56:47,340
after venture capital and had
some successful exits. And, and

1035
00:56:47,370 --> 00:56:49,680
he said, he said, what you're
going after right now is he

1036
00:56:49,680 --> 00:56:52,440
called it dumb money, for that
reason, right? Because when you

1037
00:56:52,440 --> 00:56:54,900
go after venture capitalists,
you're not only you're getting

1038
00:56:54,900 --> 00:56:57,450
their Rolodex, right, the old
Rolodex is where people had

1039
00:56:57,450 --> 00:57:00,450
contacts and things like that.
So you're going after their

1040
00:57:00,450 --> 00:57:04,860
Rolodex as much as you are on
the cash. And so that's kind of

1041
00:57:04,860 --> 00:57:06,480
the next step that I see.

1042
00:57:06,540 --> 00:57:10,080
Audrow Nash: And also just
advising to not just rollback,

1043
00:57:10,080 --> 00:57:13,260
but like they can really no,
because they could have could

1044
00:57:13,260 --> 00:57:16,050
have seen people go through
similar stages. And they can be

1045
00:57:16,050 --> 00:57:18,180
like, Ah, this Yep, like, at
this point, you should be

1046
00:57:18,180 --> 00:57:20,430
working on this. And at this
point, you do this, and this is

1047
00:57:20,430 --> 00:57:24,000
how we get to the next stage,
our Series A or whatever it

1048
00:57:24,000 --> 00:57:24,480
might be.

1049
00:57:25,560 --> 00:57:30,030
Jason Richards: Yep. So yeah. So
with with the, so as we talked

1050
00:57:30,030 --> 00:57:31,740
to different people,
essentially, it was, you know,

1051
00:57:31,740 --> 00:57:34,560
that was kind of the feedback we
got from several, the, you know,

1052
00:57:34,560 --> 00:57:37,470
get some revenue didn't come
back to us kind of thing. So we

1053
00:57:37,470 --> 00:57:39,300
looked at that and said, Okay,
well, then crowdfunding seems

1054
00:57:39,300 --> 00:57:43,200
like, a good option for a few
reasons. One was because we

1055
00:57:43,260 --> 00:57:46,590
everywhere DAX goes a celebrity.
You know, everybody, kind of

1056
00:57:46,590 --> 00:57:50,010
everybody, there's, about half
of the people will kind of walk

1057
00:57:50,010 --> 00:57:52,980
by like, oh, yeah, I see robots
trying to talk to me every day,

1058
00:57:53,010 --> 00:57:55,530
you know, like, and I might say,
I'm trying to

1059
00:57:55,740 --> 00:58:00,180
Audrow Nash: go, where is this?
I don't know. I feel like I

1060
00:58:00,180 --> 00:58:01,980
should be in the area with all
the robots. And you were

1061
00:58:01,980 --> 00:58:05,100
mentioning that it's not legal
for robots to drive on the

1062
00:58:05,100 --> 00:58:08,640
streets here. But I'm in I'm
supposed to be in the tech area.

1063
00:58:08,670 --> 00:58:14,190
And I so seldom see robots
anywhere doing anything. Work

1064
00:58:14,190 --> 00:58:16,410
From Home stuff, but yes. Okay.

1065
00:58:16,500 --> 00:58:18,210
Jason Richards: Right. And then
and then the other half of

1066
00:58:18,210 --> 00:58:20,730
people are, you know, getting
out of their cart. We haven't

1067
00:58:20,730 --> 00:58:22,950
caused any, you know, auto
accidents from people like

1068
00:58:23,220 --> 00:58:26,190
rubbernecking, but it is fun
being you know, walking. It's

1069
00:58:26,190 --> 00:58:29,610
fun walking behind dact. Yep.
And seeing people on the road

1070
00:58:29,610 --> 00:58:31,740
because people will look and
even if they're having a bad

1071
00:58:31,740 --> 00:58:34,410
day, it's funny watching the
people in the car because they

1072
00:58:34,410 --> 00:58:37,290
will, they will turn their head
look at the robot. And you know,

1073
00:58:37,290 --> 00:58:40,260
the look at DAX and then
everybody smiles. Where has this

1074
00:58:40,260 --> 00:58:42,780
look of like astonishment? Yeah,
what is that? Did I just see

1075
00:58:42,780 --> 00:58:45,930
that? But everybody smiles like,
well, that is that's cool. You

1076
00:58:45,930 --> 00:58:48,960
know, like, huh, I just saw
something special today. So

1077
00:58:48,960 --> 00:58:53,160
anywhere, the DAX goes. He's a
celebrity. Like I talked about,

1078
00:58:53,160 --> 00:58:55,950
you know, the hometown film, if
we wanted to be able to say,

1079
00:58:55,950 --> 00:58:58,830
hey, you can invest to for
crowdfunding. So yeah, like,

1080
00:58:58,830 --> 00:59:01,440
give you that opportunity. The
other piece of crowdfunding is

1081
00:59:01,440 --> 00:59:05,910
it's a lot of, it's a lot of
marketing. Right? Film is, is a,

1082
00:59:06,210 --> 00:59:10,350
it's a great little small town.
But it's a small town. Down here

1083
00:59:10,350 --> 00:59:14,070
for me. Yeah, versus Seattle.
And you got 4 million, you know,

1084
00:59:14,100 --> 00:59:18,030
not as big as you know, LA where
you're coming from but, but need

1085
00:59:18,030 --> 00:59:21,030
more eyeballs on DAX, right,
more the more eyeballs, the C

1086
00:59:21,030 --> 00:59:24,180
DAX, the more people that have
invested interest literally,

1087
00:59:24,210 --> 00:59:27,300
right? If somebody is in Texas,
and they put $1,000 into DAX,

1088
00:59:27,510 --> 00:59:29,730
they're probably also more privy
to be like, hey, I want DAX down

1089
00:59:29,730 --> 00:59:32,610
here, too, you know, or what
connections do you have any

1090
00:59:32,640 --> 00:59:35,580
larger sense? That makes sense?
So those were the two big

1091
00:59:35,580 --> 00:59:38,700
reasons that we said yeah, let's
let's go after crowdfunding and

1092
00:59:38,880 --> 00:59:39,600
make it work.

1093
00:59:39,660 --> 00:59:46,800
Audrow Nash: So what do you
think? So I coming from, so I

1094
00:59:46,950 --> 00:59:51,360
was in grad school doing human
robot interaction for a little

1095
00:59:51,360 --> 00:59:56,430
bit. Oh, yeah. And one of the
things that was very concerning

1096
00:59:56,430 --> 01:00:00,360
for the community was kind of
the this novelty effect. So like

1097
01:00:00,360 --> 01:00:03,960
someone can be, like really
excited about the robot, and

1098
01:00:03,960 --> 01:00:06,210
they'll like, do all the things
the robot wants and behave

1099
01:00:06,210 --> 01:00:11,430
really well around it. But then
over time, so as weeks go by,

1100
01:00:11,430 --> 01:00:17,340
and months go by any, like,
maybe the robot becomes less

1101
01:00:17,340 --> 01:00:21,030
influential on those people,
because they now are like, I

1102
01:00:21,030 --> 01:00:23,760
know what it does. I'm kind of
sick of it. This kind of thing.

1103
01:00:23,970 --> 01:00:27,450
Yeah. Any any thoughts on this?
Because the robot is currently a

1104
01:00:27,450 --> 01:00:33,000
celebrity. But I wonder, could
it be a novelty effect? Or is

1105
01:00:33,000 --> 01:00:34,890
it? How do you think of these
things?

1106
01:00:35,550 --> 01:00:40,320
Jason Richards: Yeah. So if if
all the DAX did was be novel? I

1107
01:00:40,320 --> 01:00:43,920
would agree with that. Right. So
differing things and so right.

1108
01:00:43,920 --> 01:00:46,800
So do you have to have a useful
function as well? Right. So if

1109
01:00:46,800 --> 01:00:50,970
he's, if it is, same with you
know, like, I would say that,

1110
01:00:51,480 --> 01:00:55,110
that if it's doing useful things
for people, whether it's, you

1111
01:00:55,110 --> 01:00:59,400
know, delivering you food or,
you know, patrolling grounds,

1112
01:00:59,400 --> 01:01:01,470
like a security robot or
something like that, right, if

1113
01:01:01,470 --> 01:01:04,290
it's doing something useful,
it's not only novel and cool.

1114
01:01:04,740 --> 01:01:06,810
You wouldn't get sick, because
it's actually helping. It's

1115
01:01:06,810 --> 01:01:10,650
justifying it feels way. Yeah,
exactly. Especially in the

1116
01:01:10,650 --> 01:01:13,080
delivery space, because
delivery. You know, if you look

1117
01:01:13,080 --> 01:01:18,240
back to the 1970s, when they
started tracking it, called the

1118
01:01:18,240 --> 01:01:22,680
labor force participation rate.
And so the US government

1119
01:01:22,680 --> 01:01:25,800
tracking, it was like, US
government. Yeah. Department of

1120
01:01:25,800 --> 01:01:28,470
Labor. Okay. Yeah. The
Department of Labor is looking

1121
01:01:28,470 --> 01:01:31,770
at how many people are working
compared to the population, and

1122
01:01:31,770 --> 01:01:34,770
that that number has been in
steady decline since the 70s.

1123
01:01:35,700 --> 01:01:39,000
COVID exasperated that. Yeah.
But you know, where was less

1124
01:01:39,000 --> 01:01:41,700
people working in at the same
time? If you look at just food

1125
01:01:41,700 --> 01:01:46,530
delivery service, in general,
that's been on an increased, you

1126
01:01:46,530 --> 01:01:48,930
know, exponential growth path
for the last several years.

1127
01:01:48,930 --> 01:01:53,070
COVID exasperated that as well.
So the restaurants and the

1128
01:01:53,070 --> 01:01:56,070
people that we talked to in the
delivery space, it's almost an

1129
01:01:56,070 --> 01:01:59,100
expectation. Now, you know,
where, what if you own a

1130
01:01:59,100 --> 01:02:02,880
restaurant delivery, you really
want a restaurant? Yeah,

1131
01:02:03,030 --> 01:02:07,020
exactly, exactly. You might not,
but Uber Eats or GrubHub or

1132
01:02:07,050 --> 01:02:09,480
DoorDash, or somebody is going
to deliver this food to me,

1133
01:02:09,570 --> 01:02:13,860
right? It's kind of the
expectation. Yep. And so that's,

1134
01:02:15,030 --> 01:02:18,150
maybe I don't want to say an
unrealistic expectation. But

1135
01:02:18,150 --> 01:02:22,050
when you don't have enough
people to fill those roles, it's

1136
01:02:22,050 --> 01:02:26,430
just not going to happen. So for
example, in Coronado, one of our

1137
01:02:26,430 --> 01:02:28,290
guys walked into one of the
restaurants in the back and they

1138
01:02:28,290 --> 01:02:30,540
said, do you see all those
orders, and there's like 20

1139
01:02:30,600 --> 01:02:32,880
delivery orders sitting there,
like they, they've been sitting

1140
01:02:32,880 --> 01:02:36,180
for hours, and there's nobody
going to come pick them up, like

1141
01:02:36,510 --> 01:02:38,790
very doubtful that somebody is
actually going to get on

1142
01:02:38,820 --> 01:02:41,370
UberEATS or GrubHub, or
wherever, and go and pick those

1143
01:02:41,370 --> 01:02:43,800
orders up and take them to the
people that want them. You know,

1144
01:02:43,920 --> 01:02:47,400
last delivery experience I had,
when I was on a trip to Montana,

1145
01:02:47,730 --> 01:02:51,930
is we had delivery set for 1130.
For lunch for there's only five

1146
01:02:51,930 --> 01:02:54,930
of us in the room. So okay,
lunch is gonna be here at 1130.

1147
01:02:54,930 --> 01:02:57,780
We we ordered it ahead of time,
make sure we got into delivery

1148
01:02:57,780 --> 01:03:00,870
schedule, and it didn't show up
till 2pm. And that's just

1149
01:03:00,870 --> 01:03:03,570
because of you know, there's not
enough people to fill those

1150
01:03:03,570 --> 01:03:04,740
types of roles. Is that

1151
01:03:04,740 --> 01:03:07,080
Audrow Nash: a thing that's more
particular to small towns, do

1152
01:03:07,080 --> 01:03:10,140
you think like basically, is the
whole like, say, middle of

1153
01:03:10,140 --> 01:03:15,420
America that would have smaller
towns? or basically any, because

1154
01:03:15,420 --> 01:03:21,330
I'm thinking of like, I'm in San
Francisco now. And generally, it

1155
01:03:21,330 --> 01:03:29,250
seems like the food delivery
services are quite quick. But

1156
01:03:29,250 --> 01:03:33,000
maybe this is so it hasn't been
in my experience, as you've

1157
01:03:33,000 --> 01:03:35,460
described, where there's like,
20 orders for this kind of

1158
01:03:35,460 --> 01:03:39,480
thing. Is this particular 20
locations, I guess, or any types

1159
01:03:39,480 --> 01:03:41,310
of sounds or types of towns?

1160
01:03:42,570 --> 01:03:44,610
Jason Richards: Yeah, that's a
good question. So Coronado,

1161
01:03:44,610 --> 01:03:50,370
California isn't isn't exactly
like, you know. Yeah, yeah. It's

1162
01:03:50,370 --> 01:03:53,250
right outside San Diego. But
it's right outside San Diego,

1163
01:03:53,280 --> 01:03:56,370
right, you got to go over the
bridge. So it's also very

1164
01:03:56,370 --> 01:04:00,150
wealthy zip code. And so if you
are living in Coronado, you're

1165
01:04:00,150 --> 01:04:04,620
probably not the person doing
the live delivery. So people

1166
01:04:04,620 --> 01:04:08,220
have to come in, and then do the
delivery within that, that area.

1167
01:04:08,520 --> 01:04:12,150
So, so that's kind of unique
thing. The other thing is that a

1168
01:04:12,150 --> 01:04:15,480
lot of people, you know, door
Dashers and people that do

1169
01:04:15,480 --> 01:04:18,090
GrubHub and UberEATS, and stuff
like that. A lot of them don't

1170
01:04:18,090 --> 01:04:21,030
want to do the short, the short
stuff. Yes. Similar to like,

1171
01:04:21,060 --> 01:04:24,090
your Uber driver wants to drive
you 30 miles away to an airport,

1172
01:04:24,090 --> 01:04:26,700
they don't want to drive you
three blocks, right? Because

1173
01:04:27,240 --> 01:04:27,660
they set

1174
01:04:27,930 --> 01:04:30,600
Audrow Nash: up for but in
general, some of them are trying

1175
01:04:30,600 --> 01:04:32,520
to generate trips and these
kinds of things.

1176
01:04:33,150 --> 01:04:36,210
Jason Richards: Right, yeah. But
in general, you know, you're

1177
01:04:36,210 --> 01:04:39,480
you're making more, if you will,
you know, the longer you have.

1178
01:04:39,870 --> 01:04:43,110
Yeah, you're doing the long
trips. So so same way you know,

1179
01:04:43,110 --> 01:04:46,560
for robot you know, this you
know, the DAX is not DAX is not

1180
01:04:46,590 --> 01:04:49,260
optimized to go 30 miles you
know, he's he's got a pretty

1181
01:04:49,260 --> 01:04:53,370
good like, one, one and a half
mile range. And those are that's

1182
01:04:53,370 --> 01:04:55,800
a good niche to fill.

1183
01:04:56,160 --> 01:05:02,880
Audrow Nash: Mm hmm. Okay, could
you Imagine pairing with

1184
01:05:02,910 --> 01:05:05,460
vehicles and things where this
are like kind of like the

1185
01:05:05,490 --> 01:05:10,320
package delivery idea where it'd
be an autonomous car that drives

1186
01:05:10,320 --> 01:05:13,380
the robot and the robot does the
last little bit of the delivery.

1187
01:05:13,920 --> 01:05:16,050
Could you imagine that being
used in that kind of thing?

1188
01:05:17,010 --> 01:05:20,910
Jason Richards: I could. It
would be it would definitely be

1189
01:05:20,940 --> 01:05:23,730
interesting. Obviously, there's
a lot of people that are working

1190
01:05:23,730 --> 01:05:25,590
towards those types of
solutions. Yeah.

1191
01:05:25,620 --> 01:05:29,010
Audrow Nash: Even in the Oregon
area to thinking of agility

1192
01:05:29,070 --> 01:05:31,380
robotics, I believe. Yeah,
they're trying to do the

1193
01:05:31,380 --> 01:05:31,800
legendary

1194
01:05:31,800 --> 01:05:33,690
Jason Richards: robotics thing.
They're just up the road from

1195
01:05:33,690 --> 01:05:37,170
us. So Oh, cool. Yeah. Yeah.
Hello,

1196
01:05:37,410 --> 01:05:40,110
Audrow Nash: litres of package
delivery, or whatever? Is that?

1197
01:05:40,410 --> 01:05:40,740
Yeah.

1198
01:05:41,370 --> 01:05:43,980
Jason Richards: Yeah. And it's a
very interesting it, you know,

1199
01:05:44,460 --> 01:05:46,620
that, you know, last mile
delivery is always the most

1200
01:05:46,620 --> 01:05:50,370
expensive, you know, you look at
logistics in general. That's,

1201
01:05:50,400 --> 01:05:56,010
that's the hard stuff to figure
out. Yeah. So very potentially,

1202
01:05:56,460 --> 01:05:58,770
there's a lot of, obviously, a
lot more that has to go into the

1203
01:05:58,770 --> 01:06:01,740
logistics of that loading and
unloading a robot moving parts.

1204
01:06:02,190 --> 01:06:04,470
Audrow Nash: You need, like a
little lowering platform or

1205
01:06:04,470 --> 01:06:05,970
something to get the robot

1206
01:06:05,970 --> 01:06:08,610
Jason Richards: down there.
Yeah. And we've got a band that

1207
01:06:08,610 --> 01:06:11,610
we, you know, haul robots in.
And we have that, but it from

1208
01:06:11,730 --> 01:06:14,940
from a large scale, you know,
like if you're a big Logistics

1209
01:06:14,940 --> 01:06:18,360
Center, like Amazon, or UPS or
something like that, right. It's

1210
01:06:18,840 --> 01:06:22,830
a lot of moving parts there. And
so sure, eventually, I could see

1211
01:06:22,830 --> 01:06:25,560
something like that, like an
Amazon locker, you know, that

1212
01:06:25,590 --> 01:06:27,990
that drives to a town, but then
every locker opens up and

1213
01:06:27,990 --> 01:06:30,090
becomes a little robot and
drives to your doorstep. You

1214
01:06:30,090 --> 01:06:33,990
know, it'd be super cool. Yeah.
You know, there's, there's

1215
01:06:33,990 --> 01:06:37,890
probably something there. But
there's a lot of not just not

1216
01:06:37,890 --> 01:06:41,190
just, you know, technological
hurdles to overcome, but a lot

1217
01:06:41,190 --> 01:06:43,530
of like legislative hurdles to
overcome as well.

1218
01:06:43,920 --> 01:06:46,020
Audrow Nash: Which I do want to
talk about for sure. Yeah,

1219
01:06:46,020 --> 01:06:49,260
legislative things are very
interesting. But just before we

1220
01:06:49,260 --> 01:06:54,720
do talk about that, when so
you're not generating revenue

1221
01:06:54,780 --> 01:06:56,460
yet? I believe, right.

1222
01:06:57,630 --> 01:06:59,700
Jason Richards: We are we are
now you're just doing the

1223
01:07:00,150 --> 01:07:01,740
stages. Yep. Gotcha.

1224
01:07:01,770 --> 01:07:04,650
Audrow Nash: I'm just wondering,
What will your business model

1225
01:07:04,740 --> 01:07:09,390
be? Or what is it now? Yeah, for
generating revenue? And maybe

1226
01:07:09,390 --> 01:07:10,830
you'll flip it around a bit. But

1227
01:07:12,000 --> 01:07:14,340
Jason Richards: yeah, um, so
the, the thing that we have

1228
01:07:14,340 --> 01:07:20,880
right now is kind of twofold.
One is that we realize kind of a

1229
01:07:20,910 --> 01:07:23,670
back to the DAX as a celebrity
thing. We can talk about it now,

1230
01:07:23,670 --> 01:07:26,910
there's been a few events that
the DAX has been in. One was for

1231
01:07:26,910 --> 01:07:32,190
a TV show called the coma FD. So
there was a bunch of DAX robots

1232
01:07:32,190 --> 01:07:35,280
that they turned into
firefighters and and get a bunch

1233
01:07:35,280 --> 01:07:39,990
of promotion for a season or get
one with or under. Yeah,

1234
01:07:40,230 --> 01:07:42,960
exactly. Right. He was an avid
he's like, my two worlds

1235
01:07:42,960 --> 01:07:47,550
collided, this is amazing. And
then, and then one was for a

1236
01:07:50,490 --> 01:07:53,670
Jack, Jack Daniels. It was some
some beverage company. And we

1237
01:07:53,670 --> 01:07:57,510
did a delivery promotional event
at a golf course in Napa Valley.

1238
01:07:58,350 --> 01:08:01,440
Things like so. Yeah. So
different marketing agencies are

1239
01:08:01,440 --> 01:08:04,140
reaching out to us because it's
a unique, it's unique enough,

1240
01:08:04,170 --> 01:08:06,510
right. It's novelty enough at
this point, that it's a great

1241
01:08:06,510 --> 01:08:10,320
marketing engine for them. So so
being able to do appearances and

1242
01:08:10,320 --> 01:08:12,210
stuff has been fun. And
obviously, it gets our brand out

1243
01:08:12,210 --> 01:08:14,460
there as well. So that's fun.
That's that's been generating

1244
01:08:14,460 --> 01:08:19,380
some revenue. The the flash
entertainment robot, yeah,

1245
01:08:19,410 --> 01:08:22,620
right, exactly. But those are,
those are gig events, if you

1246
01:08:22,620 --> 01:08:25,800
will, right. It's not recurring
revenue. So but the the actual

1247
01:08:25,800 --> 01:08:28,650
business model is revolving
around that you have different

1248
01:08:28,650 --> 01:08:31,890
deployments in different states
that are, you know, renting

1249
01:08:31,890 --> 01:08:37,110
robots for, you know, 1000s of
dollars a month for robot, and

1250
01:08:37,110 --> 01:08:40,380
they're gonna do deliveries a
service for you, which also

1251
01:08:40,380 --> 01:08:43,080
keeps it so that we can take
care of service and those types

1252
01:08:43,080 --> 01:08:46,050
of things. And then those that's
the monthly recurring model that

1253
01:08:46,260 --> 01:08:47,640
we want to spin up. So Gotcha.

1254
01:08:48,420 --> 01:08:51,900
Audrow Nash: So it's been where
they buy the hardware, or where

1255
01:08:51,900 --> 01:08:53,610
they rent the hardware. They

1256
01:08:53,610 --> 01:08:55,380
Jason Richards: rent the
hardware, correct? Yeah, yeah.

1257
01:08:55,410 --> 01:08:58,680
They're renting the service of
delivery. And we happen instead

1258
01:08:58,680 --> 01:09:01,530
of us being humans there. We
have robots that do it for

1259
01:09:01,530 --> 01:09:05,970
Audrow Nash: us. Gotcha. So I
just recently, at the time of

1260
01:09:05,970 --> 01:09:08,430
recording this interviewed
cobalt robotics, and they're

1261
01:09:08,430 --> 01:09:12,990
doing something very similar for
security guard robots, where

1262
01:09:12,990 --> 01:09:15,870
they're having the robots
basically be the legs and then

1263
01:09:15,870 --> 01:09:22,440
they have someone controlling it
at a very high level. And so to

1264
01:09:22,440 --> 01:09:25,440
me, this is kind of similar to
what you guys are doing, where

1265
01:09:25,440 --> 01:09:28,770
it's you will have the robot be
autonomous on the navigation and

1266
01:09:28,770 --> 01:09:33,510
then you'll have a person
control the robot for the

1267
01:09:33,510 --> 01:09:37,650
interaction for the delivery. Is
that Is that what you imagine?

1268
01:09:38,520 --> 01:09:38,910
Yeah,

1269
01:09:38,940 --> 01:09:42,390
Jason Richards: that's yeah, I
imagine that that more and more

1270
01:09:42,390 --> 01:09:46,050
pieces of that puzzle will get
solved, you know, autonomously.

1271
01:09:46,560 --> 01:09:49,500
Yeah, again, like the facial
recognition pieces. Are you

1272
01:09:49,500 --> 01:09:51,900
happy sad how do we make your
day if you're a kid, what do we

1273
01:09:51,900 --> 01:09:54,960
do you know, you know, those
those types of things. But then

1274
01:09:54,960 --> 01:09:57,090
say what the delivery piece you
know, being able to get to an

1275
01:09:57,090 --> 01:09:59,910
end destination, and then
somebody comes out you realize

1276
01:09:59,910 --> 01:10:01,980
that There's a person now in
front of you, you can interact

1277
01:10:01,980 --> 01:10:05,670
with that person. And then open
your drawer if you know, you can

1278
01:10:05,670 --> 01:10:07,860
see the code is correct, you
know, those those types of

1279
01:10:07,860 --> 01:10:11,850
things can all be young
automatable. And that's the next

1280
01:10:11,850 --> 01:10:12,360
pieces.

1281
01:10:12,720 --> 01:10:15,300
Audrow Nash: Gotcha. So the goal
is full autonomy as much as

1282
01:10:15,300 --> 01:10:20,970
possible. I'm just wondering,
yeah, if you would have a, like,

1283
01:10:21,480 --> 01:10:26,040
I don't know. So in cobalt, they
have, like, half of the people

1284
01:10:26,040 --> 01:10:28,830
there are just watching over the
security guards. They're the

1285
01:10:28,830 --> 01:10:32,790
people that take over autonomy.
And I would wonder if at least

1286
01:10:32,790 --> 01:10:36,480
before all this stuff is
automated, would you have some

1287
01:10:36,480 --> 01:10:39,720
sort of similar model where
it's, you have a bunch of people

1288
01:10:39,720 --> 01:10:44,220
working to do the autonomy, or
do to supplement the autonomy.

1289
01:10:44,250 --> 01:10:45,780
So when it can get there?

1290
01:10:45,810 --> 01:10:49,350
Jason Richards: Yeah. In you're
exactly right. It's more of the

1291
01:10:49,410 --> 01:10:51,960
and that's what we tell people,
it's semi autonomy, because when

1292
01:10:51,960 --> 01:10:55,410
people think autonomy, they
think, full on this robot is our

1293
01:10:55,410 --> 01:10:58,860
duty to and it does exactly what
I want, whenever I want. But no,

1294
01:10:58,890 --> 01:11:03,060
the semi autonomy piece is being
able to increase the number of

1295
01:11:03,090 --> 01:11:06,330
you know, the ratio of operators
to actual robots that they

1296
01:11:06,330 --> 01:11:08,940
oversee. Right. And that's the
goal is to have greater and

1297
01:11:08,940 --> 01:11:12,360
greater autonomy, which requires
fewer and fewer operators, that

1298
01:11:12,360 --> 01:11:14,700
aren't necessarily interacting
with every single person that

1299
01:11:14,700 --> 01:11:17,880
they see. But making sure that
robots are, are generally

1300
01:11:17,880 --> 01:11:20,790
staying in line with what they
need to be doing. Yeah, you

1301
01:11:20,790 --> 01:11:23,250
know, or if they see something
that's completely, you know, a

1302
01:11:23,250 --> 01:11:26,220
tree falls in the middle of
their path. And now they have no

1303
01:11:26,220 --> 01:11:29,070
way around, because we're not
letting it go outside the bounds

1304
01:11:29,100 --> 01:11:31,770
of you know, it's on a sidewalk,
it's like, we can't go into the

1305
01:11:31,770 --> 01:11:34,800
road. You know, so not very nice
to take over and make a human

1306
01:11:34,800 --> 01:11:38,160
decision of okay, we're gonna
have to find another path,

1307
01:11:38,160 --> 01:11:40,980
right, turn the robot around,
but that kind of thing. So. So

1308
01:11:40,980 --> 01:11:44,610
there will always be an
overseer, you know, somebody

1309
01:11:44,610 --> 01:11:48,570
that's commanding, you know, a
crew of 20 or so robots. So

1310
01:11:48,840 --> 01:11:52,350
Audrow Nash: cool. And how did
you decide to do the

1311
01:11:52,500 --> 01:11:57,930
subscription model as opposed to
like, they buy the robot, and

1312
01:11:58,740 --> 01:12:01,680
they're on their own, or they
buy the robot and you like,

1313
01:12:01,740 --> 01:12:04,710
manage it somehow? Or they buy
it? And then they are a

1314
01:12:04,710 --> 01:12:08,190
subscription thing? Or like all
these permutations? How did you

1315
01:12:08,190 --> 01:12:08,970
pick the one?

1316
01:12:09,720 --> 01:12:12,570
Jason Richards: So the current
one, investors really like MRR

1317
01:12:12,600 --> 01:12:15,690
monthly recurring revenue, they
like seeing that this is going

1318
01:12:15,690 --> 01:12:19,170
to come in whether you, you
know, yeah, essentially all the

1319
01:12:19,170 --> 01:12:22,890
time. Right, versus fairly
predictable. Exactly. And that

1320
01:12:22,890 --> 01:12:26,490
that really rings true for a lot
of investors. So that's one. The

1321
01:12:26,490 --> 01:12:29,160
second is, is that when people
have asked us to buy the robot

1322
01:12:29,190 --> 01:12:32,670
outright, we've kind of played
with the idea. But at the same

1323
01:12:32,670 --> 01:12:35,490
time, we said, Okay, well, we're
gonna have to come up with some

1324
01:12:35,730 --> 01:12:39,780
sort of service contract on top
of this. Because, you know, you

1325
01:12:39,780 --> 01:12:43,620
could take a, it's not
ubiquitous enough, where you

1326
01:12:43,620 --> 01:12:45,930
could take a laptop to a
computer repair shop, and they

1327
01:12:45,930 --> 01:12:48,840
could probably figure out what
to do or diagnose it. You take a

1328
01:12:48,840 --> 01:12:52,740
robot that nobody's seen before,
to like, you know, yeah, a

1329
01:12:52,740 --> 01:12:54,990
computer repair shop, they're
not gonna know what to do. And

1330
01:12:54,990 --> 01:12:57,750
so at the same time, we said, it
wouldn't, it doesn't make a lot

1331
01:12:57,750 --> 01:12:59,700
of sense, because we did come up
with a model and said, Yeah,

1332
01:12:59,700 --> 01:13:02,640
well, you can buy the robot for,
I don't remember what the number

1333
01:13:02,640 --> 01:13:05,550
was, like 30 or $40,000. And
then it's still going to be a,

1334
01:13:05,580 --> 01:13:09,150
you know, a fiver $1,000 monthly
service contract to have that

1335
01:13:09,150 --> 01:13:11,970
robot. So we said, you know, if
we're, we have to go that model

1336
01:13:11,970 --> 01:13:14,520
anyway. And explaining that to
people, they understand that

1337
01:13:14,520 --> 01:13:17,310
they're like, oh, yeah, because
if a track falls off, or you

1338
01:13:17,310 --> 01:13:19,890
know, a tread wears out, or
whatever, you know, yeah, you

1339
01:13:19,890 --> 01:13:23,070
need to get to the restaurant to
repair that. Exactly. Yeah. And

1340
01:13:23,070 --> 01:13:27,510
so, so having a service model
made a lot more sense. Yeah. To,

1341
01:13:27,870 --> 01:13:30,240
to us, and to people that have
been talking to so

1342
01:13:30,570 --> 01:13:33,870
Audrow Nash: yeah. Let's see, I
have a few questions that I want

1343
01:13:33,870 --> 01:13:38,070
to ask all at the same time. But
what? So you're doing things

1344
01:13:38,070 --> 01:13:40,800
where it's like, they're in
Atlanta, they're in California,

1345
01:13:40,830 --> 01:13:44,190
they're in Oregon? Does it mean
you're going to have to set up

1346
01:13:44,220 --> 01:13:47,550
technicians at all these
locations so that they can do

1347
01:13:47,550 --> 01:13:51,570
these repairs? Or how the kind
of the logistics of that work?

1348
01:13:52,230 --> 01:13:54,900
Jason Richards: Yeah, so it's,
um, we actually what we do is we

1349
01:13:54,900 --> 01:13:58,890
actually find partners in those
areas, that we can then train on

1350
01:13:58,890 --> 01:14:01,800
some minor stuff, you know,
usually it's, you know, computer

1351
01:14:01,800 --> 01:14:04,770
repair shops, or people that are
in, you know, manufacturing,

1352
01:14:04,800 --> 01:14:07,410
robotics type hardware, maybe
not robotics itself, but you

1353
01:14:07,410 --> 01:14:09,480
know, something that they see a
circuit board and they don't

1354
01:14:09,480 --> 01:14:13,770
freak out, you know, they it's
not like taking into a bicycle

1355
01:14:13,770 --> 01:14:16,740
repair shop. Right? Yeah. So
somebody that has some

1356
01:14:16,740 --> 01:14:19,890
experience with technology, so
we partnered with them, and then

1357
01:14:19,890 --> 01:14:22,320
then become essentially our
point of contact for shipping,

1358
01:14:22,350 --> 01:14:26,160
you know, robots in and out of a
city. And then we also we keep a

1359
01:14:26,160 --> 01:14:30,120
hot spare that's readily
available inside swab. I see.

1360
01:14:30,210 --> 01:14:33,870
Correct. So if the robot has a
problem, something happens. That

1361
01:14:33,870 --> 01:14:36,780
yeah, we can we can within the
day, swap the robot out with a

1362
01:14:36,780 --> 01:14:40,020
new robot ready to go. If it's
simple enough that we can repair

1363
01:14:40,020 --> 01:14:43,920
with remote hands. We'll have
them do that on a surface

1364
01:14:43,920 --> 01:14:46,980
contact. If it's more complex
than that, you know, the one

1365
01:14:46,980 --> 01:14:50,610
that was there's a hot spare in
the crate. We saved throw this

1366
01:14:50,610 --> 01:14:52,860
one in the crate we got another
one coming your way we ship a

1367
01:14:52,860 --> 01:14:56,280
robot and they robots basically
meet, you know, somewhere in the

1368
01:14:56,280 --> 01:14:57,450
middle of America. So

1369
01:14:57,510 --> 01:15:02,400
Audrow Nash: somebody then The
next thing I don't know if I'm

1370
01:15:02,400 --> 01:15:07,050
correct on this, but I imagine
that food delivery is a fairly

1371
01:15:07,050 --> 01:15:10,800
low margin industry, which means
you need a lot of volume.

1372
01:15:12,000 --> 01:15:15,600
Because and I think this and I'm
not I don't have data and I'm

1373
01:15:15,600 --> 01:15:19,020
not really aware. But I don't
think that the like Uber Eats

1374
01:15:19,020 --> 01:15:23,370
drivers are paid that well, or
any of the any of the delivery

1375
01:15:23,370 --> 01:15:29,310
services. And so I think it's
more about like, How many can

1376
01:15:29,310 --> 01:15:38,280
you get? Does this and I'm
wondering basically, for how

1377
01:15:38,280 --> 01:15:42,420
long before the robot would like
pay off its existence, and start

1378
01:15:42,420 --> 01:15:45,840
being profitable to the
companies in this kind of thing?

1379
01:15:45,840 --> 01:15:48,090
If it is a low margin thing?
Because they have to make a lot

1380
01:15:48,090 --> 01:15:51,480
of deliveries? Probably, yeah.
This kind of thing? Yeah,

1381
01:15:51,480 --> 01:15:54,510
Jason Richards: it really
depends. So a good example of

1382
01:15:54,510 --> 01:15:58,110
this would be one of the people
that is getting the service down

1383
01:15:58,110 --> 01:16:00,840
in the Atlanta area in
Fayetteville, a company called

1384
01:16:00,840 --> 01:16:05,700
nourish and bloom, and they're a
kind of a cool new concept of a

1385
01:16:05,700 --> 01:16:08,220
grocery store that is
essentially fully autonomous

1386
01:16:08,460 --> 01:16:11,910
with people that can help
conserve eat it. Yeah, so Amazon

1387
01:16:11,910 --> 01:16:14,940
had one that was similar in
Seattle, where you walk in

1388
01:16:15,630 --> 01:16:17,760
facial recognition, here's why.
So it's

1389
01:16:17,760 --> 01:16:18,570
Audrow Nash: that kind of thing.

1390
01:16:18,720 --> 01:16:23,940
Jason Richards: No, it's not,
not quite that that degree, but

1391
01:16:23,940 --> 01:16:27,030
similar. And so people are
coming in, they can kind of self

1392
01:16:27,480 --> 01:16:29,640
self serve, they don't need to
get checked out. Right. It's,

1393
01:16:29,640 --> 01:16:33,120
it's, it's automated, you know,
okay, so they, they wanted

1394
01:16:33,120 --> 01:16:36,690
robots as well. But if, for
example, you know, the, the

1395
01:16:36,690 --> 01:16:42,420
payload is huge in a robot. So,
how do you how do you make that

1396
01:16:42,450 --> 01:16:46,260
valuable, and it'd be similar
to, you know, if you're going to

1397
01:16:46,260 --> 01:16:49,470
shop at Walmart, for example,
versus Whole Foods, you know,

1398
01:16:49,470 --> 01:16:52,470
the amount of money that you're
going to make per bag at Whole

1399
01:16:52,470 --> 01:16:55,890
Foods is, is a much higher
margin. Walmart, for example.

1400
01:16:56,310 --> 01:16:59,100
It's a totally different market.
So we're not It's not like a

1401
01:16:59,100 --> 01:17:01,980
luxury market that we're going
after. But it is more of a high

1402
01:17:01,980 --> 01:17:03,360
end market that we're that we're

1403
01:17:03,360 --> 01:17:07,080
Audrow Nash: targeting, ah, so
these things, evaluate your ad,

1404
01:17:07,080 --> 01:17:10,800
like heating and cooling, and
this kind of thing on your robot

1405
01:17:10,800 --> 01:17:14,670
so that the the food arrives and
is still tasty, not like, with

1406
01:17:14,670 --> 01:17:18,390
DoorDash, if it's slightly cold,
or something like this. So it's

1407
01:17:18,450 --> 01:17:22,890
the luxury market with
subsidized a bit too,

1408
01:17:23,159 --> 01:17:24,689
Jason Richards: you know, if
that makes sense, they're making

1409
01:17:24,689 --> 01:17:27,299
higher margins on products that
they're selling anyway, I mean,

1410
01:17:27,629 --> 01:17:30,329
you know, Whole Foods and
Walmart, they don't sell exactly

1411
01:17:30,329 --> 01:17:33,209
the same thing. But the margins
are definitely different. Right.

1412
01:17:33,329 --> 01:17:37,199
So, so that's part of it. And
then the other part is, is also

1413
01:17:37,199 --> 01:17:40,349
the, that becomes a unique, you
know, unique service that they

1414
01:17:40,349 --> 01:17:43,199
can provide that nobody else
does. So then drive more traffic

1415
01:17:43,199 --> 01:17:46,829
from a marketing perspective.
Yeah, oh, I can have a robot

1416
01:17:46,829 --> 01:17:49,229
deliver it to me, these people
can do that. And these people

1417
01:17:49,229 --> 01:17:51,149
can't. So I'm going to shop
here, even if it cost me a

1418
01:17:51,149 --> 01:17:51,599
little bit more.

1419
01:17:51,960 --> 01:17:53,340
Audrow Nash: I know that it's
cool for you guys. But I'm

1420
01:17:53,340 --> 01:17:56,250
looking forward to the day that
it's not uncommon for robots to

1421
01:17:56,250 --> 01:17:58,470
deliver things. I think that
would be sweet.

1422
01:17:59,640 --> 01:18:01,020
Jason Richards: Yeah, me too.
Actually.

1423
01:18:02,100 --> 01:18:06,600
Audrow Nash: That'd be nice.
Okay, now, complete non

1424
01:18:06,600 --> 01:18:09,540
sequitur. How does your robot
deal with stairs?

1425
01:18:10,710 --> 01:18:11,400
Jason Richards: It doesn't.

1426
01:18:11,940 --> 01:18:17,970
Audrow Nash: Yes, so what? So
I'm imagining, like in San

1427
01:18:17,970 --> 01:18:23,460
Francisco, it's not always the
case that there are nice curbs

1428
01:18:23,610 --> 01:18:28,620
to get around. So it may just be
like a sheer drop onto the

1429
01:18:28,620 --> 01:18:35,250
street kind of thing. Does this?
I don't know. i How do you guys

1430
01:18:35,250 --> 01:18:41,940
decide to not do stairs? With
the tracks? And it? Does that

1431
01:18:41,970 --> 01:18:45,090
affect? Where the rope? I'm sure
it affects where the robot can

1432
01:18:45,090 --> 01:18:49,020
go? And what cities you can
deploy in things like this?

1433
01:18:49,530 --> 01:18:52,680
Jason Richards: Yeah, the the,
because there's a couple things

1434
01:18:52,680 --> 01:18:56,700
as well. I mean, San Francisco,
Seattle as well. The San

1435
01:18:56,700 --> 01:18:59,430
Francisco especially is known
for some very steep streets.

1436
01:18:59,970 --> 01:19:03,360
Right. And so there's some areas
that we were five degrees and

1437
01:19:03,360 --> 01:19:08,610
Audrow Nash: someplace. Yeah,
exactly. Yeah. This kind of, so

1438
01:19:08,610 --> 01:19:10,230
Jason Richards: we wouldn't, we
wouldn't put a robot in that

1439
01:19:10,230 --> 01:19:12,930
area just for that reason. So we
had to make a couple of design

1440
01:19:12,930 --> 01:19:15,510
decisions, right, like all
engineers do. And one of those

1441
01:19:15,510 --> 01:19:19,020
limitations is we said, Okay,
where can we deploy this thing?

1442
01:19:19,080 --> 01:19:22,170
And we said anywhere that is ADA
compliant with current, you

1443
01:19:22,170 --> 01:19:22,680
know, if

1444
01:19:23,250 --> 01:19:27,030
Audrow Nash: visibility American
Disabilities associated to a

1445
01:19:27,030 --> 01:19:28,080
wheelchair should be able to get

1446
01:19:28,080 --> 01:19:31,560
Jason Richards: around there.
Correct. So any ramp, and I

1447
01:19:31,560 --> 01:19:35,820
don't remember what the exact
slope is, for that, but we said

1448
01:19:35,820 --> 01:19:38,670
that that that is going to be
kind of the max grade that we're

1449
01:19:38,670 --> 01:19:41,220
going to we're going to be on
which is still pretty steep. I

1450
01:19:41,220 --> 01:19:43,560
think it's 18%, if I'm not
mistaken. So it's still

1451
01:19:43,560 --> 01:19:48,990
significant. I mean, somebody is
a real metric. Give me

1452
01:19:48,989 --> 01:19:53,069
Audrow Nash: degrees. So yeah,
100% is straight up and so 18%

1453
01:19:53,069 --> 01:19:56,759
is 20% of 90 degrees. So

1454
01:19:57,480 --> 01:20:00,780
Jason Richards: yeah, I think
yeah, exactly like that. So,

1455
01:20:00,960 --> 01:20:04,890
it's not nothing though, right?
It's, it's, it's sticking really

1456
01:20:04,890 --> 01:20:09,660
soon. So, so but you know, same
thing, you know, at some point,

1457
01:20:09,690 --> 01:20:11,700
you know, you don't want this
thing flipping over on its back,

1458
01:20:11,730 --> 01:20:16,920
you know, and, and that kind of
thing. So stairs is something

1459
01:20:16,920 --> 01:20:19,290
that we know that we'll have to
deal with eventually, you know,

1460
01:20:19,290 --> 01:20:20,940
same with elevators, especially
for

1461
01:20:20,940 --> 01:20:24,870
Audrow Nash: delivery to Yeah,
elevators, like I mean, my

1462
01:20:24,900 --> 01:20:29,010
apartment building. There's
stairs on the way in and then

1463
01:20:29,010 --> 01:20:31,770
elevator. So it would have to
meet me outside.

1464
01:20:32,460 --> 01:20:34,890
Jason Richards: Correct? Yeah.
And I know that, you know, a lot

1465
01:20:34,890 --> 01:20:37,980
of UberEATS drivers and things
like that can you know, come to

1466
01:20:37,980 --> 01:20:40,470
the door and true, you know, a
lot of them have to get wrong in

1467
01:20:40,500 --> 01:20:42,570
you know, you hit a code. And
usually people meet you in the

1468
01:20:42,570 --> 01:20:45,510
lobby, something like that. So
there's, it's not completely

1469
01:20:45,510 --> 01:20:48,450
outside that the paradigm. But
when we're looking at places to

1470
01:20:48,450 --> 01:20:50,550
deploy currently, yeah,
obviously, we're looking for

1471
01:20:50,550 --> 01:20:55,440
places that are more flat, that
have less stairs that are

1472
01:20:55,440 --> 01:20:58,830
usually with up kept sidewalks.
I mean, the small town that

1473
01:20:58,830 --> 01:21:02,070
we're in is a nice little small
town to live, nice place to be

1474
01:21:02,070 --> 01:21:05,730
from. But it's a it's a great
place to test because as

1475
01:21:05,730 --> 01:21:09,510
terrible sidewalks. So we're
finding out a lot of things

1476
01:21:09,510 --> 01:21:13,080
about potholes and what we can
and can't do gravel. You know,

1477
01:21:13,080 --> 01:21:15,210
the elements getting through
mud, you know, that kind of

1478
01:21:15,210 --> 01:21:18,630
stuff, which is obviously great
for a testbed, but not an ideal

1479
01:21:18,630 --> 01:21:19,140
deployment.

1480
01:21:19,619 --> 01:21:22,349
Audrow Nash: Yeah, let's see.
And we are starting to run out

1481
01:21:22,349 --> 01:21:27,599
of time. So I would love to talk
about legislation for the robot,

1482
01:21:27,599 --> 01:21:30,449
because this is an interesting
thing that I hadn't considered

1483
01:21:30,449 --> 01:21:34,049
before you brought it up. So
yeah, just tell me a bit about

1484
01:21:34,049 --> 01:21:35,309
it. In January,

1485
01:21:35,340 --> 01:21:38,220
Jason Richards: yep. I didn't
know much about it either. Until

1486
01:21:38,220 --> 01:21:41,190
I until we started looking at
deploying into different places.

1487
01:21:41,190 --> 01:21:45,570
And the first place that had
legislation for delivery robots,

1488
01:21:46,170 --> 01:21:50,520
was in Washington, DC, and that
that came out in 2017. And had

1489
01:21:50,520 --> 01:21:54,270
different different factors
involved weight, maximum speed,

1490
01:21:54,810 --> 01:21:58,140
when it can and can't be on the
roads, you know, safety

1491
01:21:58,140 --> 01:22:01,950
information, etc. And so
different states have adapted

1492
01:22:01,950 --> 01:22:07,590
that to very similar, you know,
everything is fairly similar to

1493
01:22:07,590 --> 01:22:11,370
that. Pennsylvania, for example,
they said that, yeah, we're

1494
01:22:11,370 --> 01:22:14,640
going to give it the same rights
as a pedestrian minus a little

1495
01:22:14,640 --> 01:22:19,710
bit, they do have to yield to
pedestrians. And, and so, in

1496
01:22:19,710 --> 01:22:22,230
California, and Oregon,
interestingly enough, doesn't we

1497
01:22:22,230 --> 01:22:25,590
don't have state laws regarding
robots on sidewalks. So it's up

1498
01:22:25,590 --> 01:22:28,770
to individual cities and
municipalities to do that. San

1499
01:22:28,770 --> 01:22:33,090
Francisco banned them in in
2017. He said, disappointment.

1500
01:22:33,090 --> 01:22:37,380
Yeah, yeah. And, and then they
made it very, they made a very

1501
01:22:37,380 --> 01:22:40,080
small exception and a bunch of
hoops that had to go through, I

1502
01:22:40,080 --> 01:22:43,140
think Postmates is allowed to
have like, two in San Francisco

1503
01:22:43,140 --> 01:22:47,220
now. And that's it. And the
reason for that is, has to do

1504
01:22:47,220 --> 01:22:52,620
with pedestrian space, you know,
people, again, the reason that

1505
01:22:52,620 --> 01:22:56,070
dexpot is, is it's just
different, you know, it's

1506
01:22:56,070 --> 01:22:58,410
different than any other
delivery devices out there. So

1507
01:22:58,410 --> 01:23:01,110
in the legislation, they call,
they call it a PDD, or a

1508
01:23:01,110 --> 01:23:06,120
personal delivery device. So
which essentially can be

1509
01:23:06,120 --> 01:23:09,210
whatever you want it to be to
put a skateboard and put a motor

1510
01:23:09,210 --> 01:23:12,120
on it, and then say, you know,
drive it remotely with a camera.

1511
01:23:12,270 --> 01:23:14,430
That's a personal delivery
device, and it can drive

1512
01:23:14,430 --> 01:23:17,820
wherever it wants on sidewalks.
And that's how this legislation

1513
01:23:17,820 --> 01:23:21,750
refers to correct what we're
referring to as a delivery

1514
01:23:21,750 --> 01:23:27,990
robot. Okay, correct. Yep. And
so the PD ds, and so a PD DS, we

1515
01:23:27,990 --> 01:23:31,170
want it we're working very hard
with we, you know, worked with

1516
01:23:31,170 --> 01:23:34,110
the state of Maryland on their
legislation working with the

1517
01:23:34,110 --> 01:23:38,280
seat at Coronado on on rules
around it. And the thing is, is

1518
01:23:38,280 --> 01:23:42,270
that people don't have a concept
of what it could be like to have

1519
01:23:42,270 --> 01:23:45,990
DAX, if their only experience is
a remote control box that is

1520
01:23:45,990 --> 01:23:49,110
getting in their way at their
feet. They don't want it there,

1521
01:23:49,140 --> 01:23:51,960
versus DAX, which is a robot
like you would see in a movie

1522
01:23:52,140 --> 01:23:57,210
that is welcome in human space,
like a dog is. And so helping,

1523
01:23:57,360 --> 01:24:01,080
you know, change the legislation
is a big deal, because there are

1524
01:24:01,080 --> 01:24:04,590
a lot of places that it does not
make sense. And it's because

1525
01:24:04,590 --> 01:24:08,340
people don't want delivery
robots on their sidewalks, you

1526
01:24:08,340 --> 01:24:11,340
know, but when they if you just
look at it from that box

1527
01:24:11,340 --> 01:24:13,740
perspective, you know, like an
engineer, like I want this box

1528
01:24:13,740 --> 01:24:17,250
on wheels, it makes sense. It's
an easy design, it's easy to do.

1529
01:24:17,370 --> 01:24:19,680
But that is not what is being
accepted. So the human

1530
01:24:19,680 --> 01:24:23,580
acceptance piece and pedestrian
space is a much bigger deal than

1531
01:24:23,580 --> 01:24:26,070
most people realize. And that's
that's the thing, the DAX is

1532
01:24:26,070 --> 01:24:30,390
essentially solved for that, you
know, being able to be welcomed

1533
01:24:30,420 --> 01:24:31,590
on pedestrian spaces.

1534
01:24:33,869 --> 01:24:37,649
Audrow Nash: That's interesting.
So it's, it's as if part of your

1535
01:24:37,649 --> 01:24:43,199
role in being involved with that
spot, is that you have to work

1536
01:24:43,199 --> 01:24:47,519
with lawmakers and things to try
to show them that this robot is

1537
01:24:47,519 --> 01:24:52,439
okay in human spaces, and should
be allowed and this kind

1538
01:24:52,440 --> 01:24:58,230
Jason Richards: of 100% Yeah,
because, because if this is the

1539
01:24:58,230 --> 01:25:01,470
future that we want, namely want
robots to be able to do more

1540
01:25:01,470 --> 01:25:06,270
things in pedestrian spaces, we
have to make sure that we're not

1541
01:25:06,300 --> 01:25:09,570
legislating our way out of
existence. And not just us. I'm

1542
01:25:09,570 --> 01:25:12,390
talking about, you know,
robotics engineers in general,

1543
01:25:12,840 --> 01:25:16,320
if you're doing something that
is in is public facing, most

1544
01:25:16,320 --> 01:25:18,570
people think it's all magic,
right? People that are engineers

1545
01:25:18,570 --> 01:25:21,330
understand there's a lot of, you
know, mechanics and software and

1546
01:25:21,330 --> 01:25:24,960
connectivity. There's a lot
behind these machines. So I

1547
01:25:24,960 --> 01:25:28,470
guess my hope is that, you know,
other people that are out there

1548
01:25:28,470 --> 01:25:33,480
are not doing things that would
then jeopardize, you know, urban

1549
01:25:33,480 --> 01:25:36,840
robots for everybody else,
similar to how drones were, you

1550
01:25:36,840 --> 01:25:39,870
know, when drones were
introduced. A lot of people did

1551
01:25:39,870 --> 01:25:41,670
a lot of really silly things
with them, because it was cool

1552
01:25:41,670 --> 01:25:45,300
technology. And now there's so
many regulations around drones.

1553
01:25:45,480 --> 01:25:47,670
And they can be very, very
useful, but they're not as

1554
01:25:47,670 --> 01:25:51,270
useful as they could be, usually
because of legislation because

1555
01:25:51,780 --> 01:25:55,890
people did some silly things,
especially in the beginning. So

1556
01:25:57,810 --> 01:26:02,670
Audrow Nash: okay, so how do you
avoid doing silly things? For

1557
01:26:02,670 --> 01:26:05,850
this? If you're trying to like,
do something and you don't know,

1558
01:26:05,850 --> 01:26:09,390
it'll be silly. I assume people
don't go into an endeavor going,

1559
01:26:09,600 --> 01:26:13,140
this is going to go badly. And
it's going to hurt everyone,

1560
01:26:13,140 --> 01:26:18,180
because crazy regulations will
be passed for this, like, way

1561
01:26:18,180 --> 01:26:21,720
over correct. And make sure this
doesn't happen again. Oh,

1562
01:26:21,720 --> 01:26:23,610
absolutely. So how do we avoid
things?

1563
01:26:23,880 --> 01:26:26,340
Jason Richards: You have to
question? So like the San

1564
01:26:26,340 --> 01:26:29,700
Francisco problem, for example,
I'm sure that the engineers were

1565
01:26:29,700 --> 01:26:32,100
like, Hey, we're gonna take this
thing on the road, and

1566
01:26:32,100 --> 01:26:34,200
everybody's going to be in love
with it.

1567
01:26:34,320 --> 01:26:38,010
Audrow Nash: What's the problem?
What do you mean, the oh, that

1568
01:26:38,040 --> 01:26:38,550
they're,

1569
01:26:38,579 --> 01:26:40,409
Jason Richards: they're illegal
in San Francisco? Oh, yeah. We

1570
01:26:40,409 --> 01:26:43,949
can't make a delivery robot to
San Francisco. And whoever, you

1571
01:26:43,949 --> 01:26:46,949
know, was a part of doing that
test. The engineers, whomever,

1572
01:26:47,039 --> 01:26:49,079
I'm sure they were thinking this
is going to be amazing. And

1573
01:26:49,079 --> 01:26:53,819
everybody's going to love it.
And then people did. And so when

1574
01:26:53,819 --> 01:26:56,489
we go to new places, for
example, usually we're finding

1575
01:26:56,489 --> 01:26:58,529
out you know, what are the
rules, you have rules around

1576
01:26:58,529 --> 01:27:01,199
this? And then giving usually
the police department a heads

1577
01:27:01,199 --> 01:27:03,449
up, you know, hey, we're going
to be doing this, we're going to

1578
01:27:03,449 --> 01:27:07,199
be here. So when they, yeah,
when they get phone calls from

1579
01:27:07,199 --> 01:27:10,229
people, hey, this robot is or
there's this thing in my way, or

1580
01:27:10,229 --> 01:27:12,749
this thing? hit my car,
whatever. We haven't, hopefully

1581
01:27:12,749 --> 01:27:16,199
not. But yeah, yeah, there are
others that have. So when they

1582
01:27:16,199 --> 01:27:18,539
get calls, they're not
completely blindsided, and then

1583
01:27:18,539 --> 01:27:22,439
they get an angry phone call
from the mayor. Right? And then

1584
01:27:22,439 --> 01:27:24,149
that's how those things happen.
You know, it's a couple

1585
01:27:24,149 --> 01:27:27,599
decisions makers, they hear from
some angry people in their city

1586
01:27:28,139 --> 01:27:31,439
that can then essentially, like,
make it up make little buddy

1587
01:27:31,439 --> 01:27:35,279
eyes. Yeah, yeah. So just more
being aware and conscious of,

1588
01:27:35,309 --> 01:27:38,399
you know, where, where are you
at, you know, who's around, and

1589
01:27:38,429 --> 01:27:41,699
making sure that that, and not
necessarily calling City Hall

1590
01:27:41,699 --> 01:27:44,819
because they won't know. But if
you call usually the local

1591
01:27:44,819 --> 01:27:47,819
police department, and just ask,
Hey, you know, we're gonna be in

1592
01:27:47,819 --> 01:27:50,759
this area with this robot. Is
there any rules around that?

1593
01:27:50,789 --> 01:27:53,129
They'll usually be able to tell
you yes or no, you say, great.

1594
01:27:53,129 --> 01:27:55,499
We just wanted to let you know
we're in the area, give you the

1595
01:27:55,499 --> 01:27:59,909
heads up. And so that, you know,
if it's some robot rolling along

1596
01:27:59,909 --> 01:28:02,489
the street, and people think
Armageddon just happened, that

1597
01:28:02,489 --> 01:28:05,489
they can realize no, no, no,
this is this company, and I've

1598
01:28:05,489 --> 01:28:07,769
got their contact phone number,
you know, so.

1599
01:28:08,279 --> 01:28:11,969
Audrow Nash: So this is like
going to, I don't know small

1600
01:28:11,969 --> 01:28:15,629
towns and things like this and
or at least individual places

1601
01:28:15,629 --> 01:28:19,349
and talking to them and telling
them. This is what we're doing

1602
01:28:19,379 --> 01:28:23,159
talking to the local police
department. How How will it be

1603
01:28:23,159 --> 01:28:27,269
to go bigger than there? So if
it's like, statewide, or federal

1604
01:28:27,269 --> 01:28:27,569
or?

1605
01:28:27,719 --> 01:28:29,549
Jason Richards: Sure, yeah, when
I talk to local police

1606
01:28:29,549 --> 01:28:33,389
department like Portland, Oregon
isn't isn't a small town, right.

1607
01:28:33,509 --> 01:28:36,839
But I'm calling you know, the
Portland Police Department and

1608
01:28:36,869 --> 01:28:39,719
talking to them, whoever you
have to talk to, you know, at

1609
01:28:39,719 --> 01:28:42,899
that, at that level, let them
know what's going on. Going from

1610
01:28:42,899 --> 01:28:46,439
there, though, most of the
regulations are actually going

1611
01:28:46,439 --> 01:28:48,869
through the Department of
Transportation. So it's finding

1612
01:28:48,869 --> 01:28:51,989
who at the department of
transportation needs to be aware

1613
01:28:52,019 --> 01:28:56,009
of this, if you're going to make
a law. Again, the legislative

1614
01:28:56,009 --> 01:28:59,609
process takes a long time, you
know, for laws. So I would not

1615
01:28:59,609 --> 01:29:02,789
recommend usually people go that
route first. Especially if

1616
01:29:02,789 --> 01:29:04,019
they're a smaller VLANs

1617
01:29:04,020 --> 01:29:06,690
Audrow Nash: are going to keep
growing kind of unfettered, you

1618
01:29:06,690 --> 01:29:09,480
would need these laws in place
at a state and then maybe

1619
01:29:09,480 --> 01:29:11,760
federal level, correct for this
kind of thing.

1620
01:29:12,149 --> 01:29:13,829
Jason Richards: Yeah. And
federally, I don't know if that

1621
01:29:13,829 --> 01:29:18,539
will ever, it might, you know,
come come about some regulatory

1622
01:29:18,539 --> 01:29:22,019
agency like the FAA, the Federal
Aviation Administration for

1623
01:29:22,049 --> 01:29:25,769
drones, but probably not, it'll
probably stay at a state level

1624
01:29:25,799 --> 01:29:29,249
and a local level. And so the
more that you can build rapport

1625
01:29:29,249 --> 01:29:29,729
there

1626
01:29:30,419 --> 01:29:34,139
Audrow Nash: by completing all
of the local police departments,

1627
01:29:34,139 --> 01:29:34,349
yep,

1628
01:29:34,890 --> 01:29:37,470
Jason Richards: that you're safe
on there, and you're going to be

1629
01:29:37,470 --> 01:29:39,990
responsible. That's the thing
that people care about more than

1630
01:29:39,990 --> 01:29:42,420
anything, it's this thing is
safe, and we're being

1631
01:29:42,420 --> 01:29:46,530
responsible. And even the bigger
piece of that, you know,

1632
01:29:46,980 --> 01:29:49,590
obviously we don't have time
talking about privacy and you

1633
01:29:49,590 --> 01:29:52,260
know, things like that data
security, this isn't big brother

1634
01:29:52,260 --> 01:29:55,350
spying on you, you know, that
kind of thing. But, but as long

1635
01:29:55,350 --> 01:29:57,060
as people know, they know that
they

1636
01:29:58,529 --> 01:30:05,489
Audrow Nash: may get it written
in Yeah, yeah. Okay. Yeah,

1637
01:30:05,519 --> 01:30:12,299
that's cool. Let's see. So what
is the timeline for you guys

1638
01:30:12,329 --> 01:30:16,679
like beginning to wrap up? What
do you expect the next few years

1639
01:30:16,679 --> 01:30:19,559
for you guys? Or where are you
going? Yeah, yeah.

1640
01:30:19,589 --> 01:30:22,619
Jason Richards: So I expect the
next couple, a couple of years,

1641
01:30:22,619 --> 01:30:26,279
you know, 2022, the goal is
really to have about 100 robots

1642
01:30:26,309 --> 01:30:29,789
out in service in different
places, we want to keep it to

1643
01:30:29,789 --> 01:30:34,619
about, you know, 10, maybe 12
geographic locations, you know,

1644
01:30:34,619 --> 01:30:37,199
so by that, you know, Los
Angeles, well, there's Long

1645
01:30:37,199 --> 01:30:39,119
Beach, and there's different
places. So, you know, if we're

1646
01:30:39,119 --> 01:30:42,329
in Greater Los Angeles, how do
we grow from there from that

1647
01:30:42,329 --> 01:30:45,749
hub? Atlanta, Georgia, you know,
how do we grow from that hub, we

1648
01:30:45,749 --> 01:30:48,389
go to Washington, DC, how do we
go from that hub. And so we want

1649
01:30:48,389 --> 01:30:51,719
to have about 10 to 12 hubs or,
you know, bigger deployments in

1650
01:30:51,719 --> 01:30:54,629
a geographic region that we can
then grow and expand from,

1651
01:30:54,839 --> 01:30:57,419
because that will really, you
know, really set the tempo for

1652
01:30:58,469 --> 01:31:01,019
commercializing. So with the
crowdfunding rounds, being able

1653
01:31:01,019 --> 01:31:04,679
to build the robots that we need
to deploy them into these areas.

1654
01:31:04,889 --> 01:31:07,379
And then once we have that,
essentially, the operational

1655
01:31:07,379 --> 01:31:10,469
piece has been figured out,
right, there's revenue, so that

1656
01:31:10,499 --> 01:31:13,379
more it'll attract more
investors, maybe higher level

1657
01:31:13,379 --> 01:31:16,529
investors. So then we can go
from that and then manufacture

1658
01:31:16,529 --> 01:31:20,489
on a greater scale to, you know,
1000 o'clock, this kind of

1659
01:31:20,489 --> 01:31:23,579
thing? Exactly, yeah, bring the
cost down, bring the autonomy

1660
01:31:23,579 --> 01:31:26,159
up. Because both of those things
happen, right? It makes you

1661
01:31:26,159 --> 01:31:30,419
know, better, better margins, so
you can grow faster, and then be

1662
01:31:30,419 --> 01:31:35,039
able to be more the face of
robots in the same way that

1663
01:31:35,039 --> 01:31:39,089
Tesla became kind of the face of
electric cars. We think that DAX

1664
01:31:39,089 --> 01:31:43,199
is going to be more of the face
of, of urban robotics that does

1665
01:31:43,199 --> 01:31:45,689
delivery, because delivery
robots, again, isn't a unique

1666
01:31:45,689 --> 01:31:49,289
example, or isn't a unique idea.
But the way that we're going

1667
01:31:49,289 --> 01:31:52,649
about it is that it's cool.
People want that to be in their

1668
01:31:52,649 --> 01:31:55,469
towns and on their streets. So
it's going to be readily

1669
01:31:55,469 --> 01:31:59,129
accepted in in ways that other
companies probably won't be

1670
01:31:59,129 --> 01:31:59,489
right now.

1671
01:31:59,520 --> 01:32:04,860
Audrow Nash: Or hopefully, so
yeah. Yeah. Awesome. Okay. And

1672
01:32:04,860 --> 01:32:09,990
then, what have been some of
the, like, larger challenges for

1673
01:32:09,990 --> 01:32:12,570
you guys so far? And what would
you do differently? to kind of

1674
01:32:12,570 --> 01:32:14,040
get even where you are still?

1675
01:32:15,059 --> 01:32:21,329
Jason Richards: Yeah, um, larger
challenges. You know, one of the

1676
01:32:21,329 --> 01:32:24,269
things, you know, is, is
building an operating system

1677
01:32:24,269 --> 01:32:26,159
from the ground up because

1678
01:32:26,159 --> 01:32:28,439
Audrow Nash: of an operating
system from the ground. Yeah.

1679
01:32:28,590 --> 01:32:32,040
Jason Richards: Oh, yeah. Called
DAX Oh, s that are DAX, those,

1680
01:32:32,430 --> 01:32:33,000
say, Dax.

1681
01:32:33,210 --> 01:32:35,430
Audrow Nash: It's like, we're at
the end of the interview. And

1682
01:32:35,430 --> 01:32:37,140
that's a very interesting thing.
Okay.

1683
01:32:37,979 --> 01:32:41,099
Jason Richards: It is very
interesting. The founder, great,

1684
01:32:41,129 --> 01:32:44,009
great guy. And so his his thing
is, you know, obviously,

1685
01:32:44,369 --> 01:32:46,649
business savvy enough to know
that we can't just open up

1686
01:32:46,679 --> 01:32:50,639
access to everybody out there in
open source at this point. But

1687
01:32:50,639 --> 01:32:53,189
that is the ultimate, you know,
I think that's his ultimate

1688
01:32:53,189 --> 01:32:55,889
objective is he wants to be able
to open up DAC, so S, which is a

1689
01:32:55,889 --> 01:32:59,609
Ross like, software, you know,
but it's way more than

1690
01:32:59,610 --> 01:33:01,590
Audrow Nash: that it does
message passing and things like

1691
01:33:01,590 --> 01:33:05,340
this store. Yeah. Yeah. And it
helped and visualization

1692
01:33:05,340 --> 01:33:05,970
perhaps.

1693
01:33:06,659 --> 01:33:08,969
Jason Richards: Yeah, it does,
it does a lot of pieces. And one

1694
01:33:08,969 --> 01:33:12,479
of the pieces that we have is,
as far as the firmware piece is

1695
01:33:12,479 --> 01:33:16,799
called a nova node. So it, it
basically is an extensible piece

1696
01:33:16,859 --> 01:33:20,519
of the robot. So you can plug in
another Nova node. So for

1697
01:33:20,519 --> 01:33:23,039
example, we say, Okay, we need
an iteration where DAX has an

1698
01:33:23,039 --> 01:33:27,089
arm so we can push buttons or
whatever, we plug in another

1699
01:33:27,089 --> 01:33:30,449
Nova node. And then DAX knows,
oh, now I have this arm. And now

1700
01:33:30,449 --> 01:33:34,439
I'm able to do this, this thing.
And it's, it's a higher level

1701
01:33:34,439 --> 01:33:37,379
intelligence and has to do with
a lot of the patents in the

1702
01:33:37,379 --> 01:33:40,709
higher level architecture stuff.
But Daksa was that was that's

1703
01:33:40,709 --> 01:33:43,559
something that is absolutely
fascinating to me, my background

1704
01:33:43,559 --> 01:33:47,519
is software. So it's, it's
really fun for me to be able to

1705
01:33:47,549 --> 01:33:51,269
nerd out with the the software
guys a little bit and hear the

1706
01:33:51,269 --> 01:33:55,019
high level architecture around
it. And again, that that was one

1707
01:33:55,019 --> 01:33:57,929
of the things that the founder
wants, I mean, he he, this is

1708
01:33:57,929 --> 01:34:02,159
the vision that he has for this
could be like a future thing. So

1709
01:34:02,159 --> 01:34:05,159
we want to make a an operating
system that, you know, that is

1710
01:34:05,189 --> 01:34:07,889
able to be open source so that
other people can can develop

1711
01:34:07,889 --> 01:34:11,279
against it as well and build
their own stuff similar to like

1712
01:34:11,309 --> 01:34:13,739
how Ross is, if that makes
sense. You know, people are

1713
01:34:13,919 --> 01:34:16,109
familiar with the Ross, you
know, robot operating system.

1714
01:34:17,069 --> 01:34:22,589
But DAX DAX Oh, s would be
similar in that vein.

1715
01:34:22,619 --> 01:34:25,319
Audrow Nash: So like, is it? Is
it a real operating system?

1716
01:34:25,319 --> 01:34:27,959
Because the robot operating
system is not a real operating

1717
01:34:27,959 --> 01:34:31,679
system? It's like a flavor of
Linux or something, or what is

1718
01:34:31,679 --> 01:34:35,099
your deck? So yeah, you'd be
right about that. Yeah,

1719
01:34:35,130 --> 01:34:37,290
Jason Richards: it's it's it's
more of a flavor of another one.

1720
01:34:37,290 --> 01:34:40,350
But it's, yeah. When you talk
about Ross, people think oh,

1721
01:34:40,350 --> 01:34:43,590
operating. I know. Yes. The
communists know, for sure. Yeah.

1722
01:34:43,920 --> 01:34:48,750
But so. So yeah, we'll put it
that way. Yes. So yeah, on

1723
01:34:48,750 --> 01:34:50,970
Linux, a lot of Linux based.

1724
01:34:54,869 --> 01:34:58,079
Audrow Nash: I feel like it's a
very interesting idea. And one

1725
01:34:58,079 --> 01:35:00,899
of the things that I see is kind
of like I mean, one of the

1726
01:35:00,899 --> 01:35:03,989
things that's really nice with
Ross is the whole community and

1727
01:35:03,989 --> 01:35:06,629
all of the things that have kind
of become fairly battle hardened

1728
01:35:07,139 --> 01:35:13,769
there. If it's if it's too big
of a deviation from like Ubuntu

1729
01:35:13,769 --> 01:35:20,099
or some of the other larger
distributions, I feel like it

1730
01:35:20,099 --> 01:35:24,599
would make it so that there is
you can't reuse a lot of what

1731
01:35:24,599 --> 01:35:27,809
exists even like, sensor
packages and stuff. But you guys

1732
01:35:27,809 --> 01:35:31,889
are clearly doing things with
different sensors. So yeah, I

1733
01:35:31,889 --> 01:35:36,479
don't know, right? Yeah. Yeah.
Yeah. Is it similar enough that

1734
01:35:36,479 --> 01:35:41,969
you can use like anything on
Ubuntu? Or? Oh, okay. So it's

1735
01:35:41,969 --> 01:35:45,179
not that large of a deviation
for this kind of thing? Is it

1736
01:35:45,179 --> 01:35:51,149
real time? So is it a real time
operating system? Yes. Cool. So

1737
01:35:51,149 --> 01:35:52,949
it's deterministic? That's
awesome.

1738
01:35:53,819 --> 01:35:58,559
Jason Richards: Yeah, it's one
of the one of the patents that

1739
01:35:59,159 --> 01:36:01,139
Audrow Nash: could be my talking
a little bit longer. So we could

1740
01:36:01,139 --> 01:36:03,569
talk a bit about this shirt.
Yeah. I talked about that

1741
01:36:03,569 --> 01:36:07,949
something at the hour to run a
no, no, i Yeah. i

1742
01:36:07,979 --> 01:36:10,289
Jason Richards: I'll take off a
little bit after that. But yes,

1743
01:36:10,289 --> 01:36:13,529
yeah. So the the high level, one
of the patents that was out

1744
01:36:13,529 --> 01:36:17,339
there is being able to
essentially have a voting

1745
01:36:17,339 --> 01:36:20,189
system. How did the I don't
remember how was put it

1746
01:36:20,189 --> 01:36:25,019
basically democratizing decision
making. So with the, the Nova

1747
01:36:25,019 --> 01:36:27,989
nodes, for example, you know,
that the robot has several

1748
01:36:27,989 --> 01:36:30,929
different ways that it can
communicate, but then how does

1749
01:36:30,929 --> 01:36:34,679
it know which is is the most
important thing to do? So for

1750
01:36:34,679 --> 01:36:38,009
example, you say, hey, robot,
you need to take this package

1751
01:36:38,009 --> 01:36:40,679
and deliver it to this, this
destination robot thing goes

1752
01:36:40,679 --> 01:36:43,919
down that that road and says,
oh, there's a tree in the way.

1753
01:36:45,569 --> 01:36:48,329
This boat is telling me that I
need to go that way. You know,

1754
01:36:48,359 --> 01:36:51,629
what did the other boats say?
And being able to then analyze,

1755
01:36:51,629 --> 01:36:54,149
what are all the other factors
going on? What is the next thing

1756
01:36:54,149 --> 01:36:59,429
that we do? And so that was that
is? I'm like, What's your

1757
01:36:59,789 --> 01:37:03,149
Audrow Nash: tree? But like, it
sounds like, I mean, so you

1758
01:37:03,149 --> 01:37:05,369
could have a state machine or
something. But it sounds I mean,

1759
01:37:05,369 --> 01:37:09,809
then that could be represented
as behavior a lot more. You have

1760
01:37:09,809 --> 01:37:12,479
more logic kind of built in,
baked into it. Right?

1761
01:37:12,840 --> 01:37:14,640
Jason Richards: Okay, a lot
more. Exactly. And a lot more

1762
01:37:14,640 --> 01:37:18,960
real time type logic, like, what
is going on, right now, as the

1763
01:37:18,960 --> 01:37:20,850
environment is changing? Because
the environments changing all

1764
01:37:20,850 --> 01:37:23,790
the time, like, do I stop? And
so a better example would be

1765
01:37:23,790 --> 01:37:26,220
like, Okay, I'm, I'm supposed to
deliver this package, but I see

1766
01:37:26,220 --> 01:37:29,640
this, this six year old girl
coming towards me, do I stop and

1767
01:37:29,640 --> 01:37:33,360
talk to the girl? You know,
based on what based on? Oh, I've

1768
01:37:33,360 --> 01:37:36,600
got time? You know, okay, yeah,
that would vote. Okay, we're

1769
01:37:36,600 --> 01:37:39,270
gonna vote, we're gonna stop for
30 seconds and say hi to this

1770
01:37:39,270 --> 01:37:42,030
girl, versus just blowing past
because I've got something to

1771
01:37:42,030 --> 01:37:44,490
do. But sometimes it can be
known, this is a higher vote,

1772
01:37:44,490 --> 01:37:48,330
because you need to get to this
destination by this period. And

1773
01:37:48,330 --> 01:37:53,280
you know, so. So that that is a
big piece of kind of the patents

1774
01:37:53,280 --> 01:37:57,030
behind it is being able to have
a voting system. Yeah. I mean,

1775
01:37:57,120 --> 01:37:58,410
to know what to do next.

1776
01:37:58,680 --> 01:38:00,750
Audrow Nash: Interesting. Yeah.
I mean, the voting system, like

1777
01:38:00,750 --> 01:38:03,450
you could you could figure out
an, like, several ways of

1778
01:38:03,450 --> 01:38:05,640
scheduling, what would be
important. It could be

1779
01:38:05,640 --> 01:38:10,260
probabilistic. It could be like
a integer voting system count.

1780
01:38:10,260 --> 01:38:15,180
It could be real numbers, or
whatever it might be. Yeah. And

1781
01:38:15,180 --> 01:38:17,910
then it sounds like it's a
movie. So it sounds like you

1782
01:38:17,910 --> 01:38:22,740
have more of a behavior tree
setup, where it's like, I'll

1783
01:38:22,740 --> 01:38:26,190
choose this or this or this
based on different options that

1784
01:38:26,190 --> 01:38:30,270
are available. And that's very
baked into this, this kind of

1785
01:38:30,270 --> 01:38:34,800
conditional logic of what to do.
So maybe it has a nice

1786
01:38:34,800 --> 01:38:40,770
representation of how to encode
these kinds of behaviors, or

1787
01:38:40,770 --> 01:38:44,430
something is that so it has a
nice way for programmers to

1788
01:38:44,430 --> 01:38:50,040
define what it is to that should
be considered in making

1789
01:38:50,040 --> 01:38:53,520
decisions or how to prioritize?
Yes, yeah.

1790
01:38:55,199 --> 01:38:59,579
Jason Richards: Yes. I'm sure
that Wilkinson is our he was a,

1791
01:38:59,789 --> 01:39:03,629
he worked at Tesla for a while,
and I'm sure he would be shaking

1792
01:39:03,629 --> 01:39:07,649
sensing. Jason, you're totally
butchering it. He was one of the

1793
01:39:07,649 --> 01:39:09,269
main architects of it. But
anyway,

1794
01:39:09,600 --> 01:39:13,590
Audrow Nash: so with that, I
noticed right now, you don't

1795
01:39:13,590 --> 01:39:19,710
have very much on your on the
company GitHub website. Yeah. It

1796
01:39:19,710 --> 01:39:22,890
would be very nice if this that
you're talking about would be

1797
01:39:22,890 --> 01:39:26,490
open source. So that can be
poked around. And we can look at

1798
01:39:26,490 --> 01:39:28,530
it in this kind of thing,
because that would be very cool.

1799
01:39:28,890 --> 01:39:33,150
What Yeah, what is on there is
mostly JavaScript, which is

1800
01:39:33,150 --> 01:39:38,970
interesting to me. Could you
tell me a little bit about the

1801
01:39:38,970 --> 01:39:43,290
decision? I don't know. It's
like your operating system. A

1802
01:39:43,290 --> 01:39:45,540
lot of the code that you're
running is running in like a

1803
01:39:45,540 --> 01:39:49,530
Node js framework, which is like
a, but is it server side

1804
01:39:49,530 --> 01:39:50,940
JavaScript framework?

1805
01:39:53,130 --> 01:39:55,560
Jason Richards: Yeah, we get a
lot. We do do a lot in Node js.

1806
01:39:55,620 --> 01:40:01,380
Audrow Nash: So Gotcha. Yeah,
you Interesting. So it's gonna

1807
01:40:01,380 --> 01:40:06,420
be like a no JS Ross like thing
that has behavior trees. Would

1808
01:40:06,420 --> 01:40:10,470
that be a way to think of? What
do you call it again, called

1809
01:40:10,470 --> 01:40:11,010
DAX.

1810
01:40:11,070 --> 01:40:12,990
Jason Richards: So DAX is just
like the robot XO

1811
01:40:12,990 --> 01:40:17,790
Audrow Nash: s. Okay. Yeah.
Yeah. Gotcha. Cool. Yeah. Any

1812
01:40:17,790 --> 01:40:21,870
timeline on open sourcing that?
Because around No,

1813
01:40:22,560 --> 01:40:25,260
Jason Richards: I don't have a
timeline. And, unfortunately,

1814
01:40:25,260 --> 01:40:27,600
right, you have to wait, you
know, the business, the business

1815
01:40:27,600 --> 01:40:31,620
decisions versus, you know, it's
somewhat similar, similar with

1816
01:40:31,620 --> 01:40:34,800
patents, you know, anybody that
that has dealt with patents or,

1817
01:40:34,830 --> 01:40:38,610
you know, litigation around
that, patents aren't necessarily

1818
01:40:39,720 --> 01:40:41,910
the end all, it's not like, hey,
we invented this thing. And

1819
01:40:41,910 --> 01:40:44,430
nobody, it's actually more of a
defensive mechanism. So if a

1820
01:40:44,430 --> 01:40:47,490
company decided that they wanted
to file a lawsuit with us and

1821
01:40:47,490 --> 01:40:50,130
say, Hey, we have this patent on
this thing, and it's really a,

1822
01:40:50,340 --> 01:40:53,010
well, we have a patent to, you
know, so this is why we're

1823
01:40:53,010 --> 01:40:57,600
allowed to operate in this
space. So, so that that is, from

1824
01:40:57,600 --> 01:40:59,940
a business standpoint, that's
the same reason that patents

1825
01:40:59,940 --> 01:41:03,660
exist. But the other piece of it
too, obviously, is protecting

1826
01:41:03,660 --> 01:41:07,290
intellectual property, I get
that. But in the same way, like,

1827
01:41:07,320 --> 01:41:10,320
you know, writing, you know,
source code and things like

1828
01:41:10,320 --> 01:41:13,500
that, if you let out too much of
the secret sauce, when you're in

1829
01:41:13,530 --> 01:41:19,410
like infancy type stages, you
know, like this, it could, could

1830
01:41:19,410 --> 01:41:22,260
put you out of business, not
super likely know, that somebody

1831
01:41:22,260 --> 01:41:24,510
would take and be like, Oh,
we're gonna do exactly what DAX

1832
01:41:24,510 --> 01:41:28,290
did. But the but the founder,
that's definitely one of the

1833
01:41:28,290 --> 01:41:31,800
things that he absolutely loves,
is, you know, wanting to make

1834
01:41:31,800 --> 01:41:34,470
this open source. So I don't
know what the dollar amount is,

1835
01:41:34,470 --> 01:41:37,140
or the the comfort level is to
the point where he's gonna want

1836
01:41:37,140 --> 01:41:39,360
to open this up to the world.
But that's something that he

1837
01:41:39,360 --> 01:41:42,000
talks about pretty regularly,
you know, wanting Okay, well,

1838
01:41:42,000 --> 01:41:42,810
that'd be really cool. It

1839
01:41:42,810 --> 01:41:45,000
Audrow Nash: would probably be a
good way to battle hardened it

1840
01:41:45,060 --> 01:41:49,350
to absolute like you guys are
running it. And what I mean, so

1841
01:41:49,350 --> 01:41:52,650
you said you currently have
about 10 robots. By the time you

1842
01:41:52,650 --> 01:41:58,500
have 1000 robots or something
like this, it'll probably like,

1843
01:41:58,620 --> 01:42:01,500
the software is going to get a
lot more battle hardened,

1844
01:42:01,530 --> 01:42:03,810
because you'll just have run it
through so many things, but the

1845
01:42:03,810 --> 01:42:07,200
community probably could help in
that process. Oh, yeah.

1846
01:42:07,440 --> 01:42:11,310
Absolutely. Like, it's amazing.
To me some of the things like I

1847
01:42:11,310 --> 01:42:15,690
mean, there's so I guess, I
don't know what's public that I

1848
01:42:15,690 --> 01:42:20,010
can say. But there's a lot of
very, I would not expect open

1849
01:42:20,010 --> 01:42:24,690
source to be used in a lot of
really critical projects that

1850
01:42:24,690 --> 01:42:28,050
are done by companies that have
a lot of stake in reliability.

1851
01:42:28,050 --> 01:42:30,240
And I'm very impressed that a
lot of open source software is

1852
01:42:30,240 --> 01:42:37,170
being used there. So it does
battle hardened. And so it might

1853
01:42:37,170 --> 01:42:42,480
be a benefit for that reason to
and then also people would have

1854
01:42:42,480 --> 01:42:46,140
a better point of comparison to,
to look at it and see if it's

1855
01:42:46,140 --> 01:42:50,430
useful and this kind of thing.
And that would be awesome. Yeah,

1856
01:42:50,460 --> 01:42:57,150
absolutely. Okay, so now we are
going a little over time. What

1857
01:42:57,570 --> 01:43:04,680
do you I guess, wrapping up any
links or contact info or

1858
01:43:04,680 --> 01:43:06,810
anything you'd like to share
with our listeners?

1859
01:43:07,439 --> 01:43:10,859
Jason Richards: Sure. Yeah, I
think that, again, you know,

1860
01:43:10,859 --> 01:43:13,919
those that are listening just
via audio, you know, can't see

1861
01:43:14,009 --> 01:43:19,319
the robot behind me. So if you
go to DAX bot.com, that's D AX

1862
01:43:19,499 --> 01:43:23,579
vo t.com. That's, you know, you
can see kind will

1863
01:43:23,999 --> 01:43:26,759
Audrow Nash: links with the
episode so that they can just

1864
01:43:26,759 --> 01:43:28,199
click if they look in the
description.

1865
01:43:28,739 --> 01:43:31,769
Jason Richards: Yeah. And then
those of you that that are so

1866
01:43:31,769 --> 01:43:35,909
inclined, start engine comm
slash dexpot is where we're

1867
01:43:35,909 --> 01:43:39,059
doing our crowdfunding raise.
And you can take a look at kind

1868
01:43:39,059 --> 01:43:42,509
of the kind of the more the
business plan around it, you

1869
01:43:42,509 --> 01:43:45,029
know, kind of the the need and
the focus that we're, we're

1870
01:43:45,029 --> 01:43:48,449
filling and all the legal stuff
we filed with the Securities

1871
01:43:48,449 --> 01:43:51,389
Exchange Commission. So anyway,
those would be the two things

1872
01:43:51,389 --> 01:43:55,829
tax bot.com and and start engine
slash tax bot. And yeah,

1873
01:43:55,859 --> 01:43:58,499
Audrow Nash: appreciate that.
All right. Awesome. Been great

1874
01:43:58,499 --> 01:44:03,569
interviewing you. Yeah, you too.
Thanks so much. Audrow. Thanks

1875
01:44:03,569 --> 01:44:05,459
for listening to this
conversation with Jason

1876
01:44:05,459 --> 01:44:08,819
Richards. Thank you again to our
founding sponsor, open robotics

1877
01:44:08,849 --> 01:44:10,019
and I hope to see you next time


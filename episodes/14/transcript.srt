1
00:00:02,790 --> 00:00:05,070
Audrow Nash: This is a
conversation with Tim Chung, who

2
00:00:05,070 --> 00:00:08,280
is a program manager in the
Tactical Defense Office at the

3
00:00:08,280 --> 00:00:12,060
Defense Advanced Research
Projects Agency, also known as

4
00:00:12,060 --> 00:00:16,530
DARPA. We speak about the DARPA
subterranean challenge or SubT,

5
00:00:16,920 --> 00:00:20,490
which is a robotics challenge
that aims to develop innovative

6
00:00:20,490 --> 00:00:24,660
technologies to augment
operations underground. I was

7
00:00:24,660 --> 00:00:28,020
excited about this interview, as
I have heard a lot about the sub

8
00:00:28,020 --> 00:00:31,710
D challenge around the edges
here at open robotics. But it

9
00:00:31,710 --> 00:00:34,590
wasn't one of the projects that
I'm working on. So I was happy

10
00:00:34,590 --> 00:00:38,310
for this opportunity to learn
more. From the interviewer, it

11
00:00:38,310 --> 00:00:41,430
seems like the SubT challenge
was a great success, and that

12
00:00:41,430 --> 00:00:44,730
the challenge stimulated a lot
of great work. You'll hear more

13
00:00:44,730 --> 00:00:48,270
from Tim during this interview.
I also found it fascinating to

14
00:00:48,270 --> 00:00:52,230
talk about how DARPA picks their
robotics challenges, and very

15
00:00:52,230 --> 00:00:55,440
importantly, how they scope the
difficulty of these challenges.

16
00:00:56,040 --> 00:00:59,730
This is the sense think act
Podcast. I'm Audrow. Nash. Thank

17
00:00:59,730 --> 00:01:03,390
you to our founding sponsor,
open robotics. And now, here is

18
00:01:03,390 --> 00:01:08,190
my conversation with Tim Chung.
I Tim, would you introduce

19
00:01:08,190 --> 00:01:08,730
yourself?

20
00:01:09,660 --> 00:01:13,320
Tim Chung: Sure. Hi. My name is
Tim Chung. I'm a program manager

21
00:01:13,320 --> 00:01:17,640
at the Defense Advanced Research
Projects Agency, or DARPA. And

22
00:01:17,670 --> 00:01:20,340
I'm the program manager for a
couple of programs that involve

23
00:01:20,340 --> 00:01:21,510
robotics and autonomy.

24
00:01:22,890 --> 00:01:25,170
Audrow Nash: And so this
interview is mostly going to be

25
00:01:25,170 --> 00:01:27,510
about the subterranean
challenge. Would you tell me a

26
00:01:27,510 --> 00:01:28,230
bit about that?

27
00:01:29,159 --> 00:01:32,699
Tim Chung: Sure. DARPA
subterranean challenge is all

28
00:01:32,699 --> 00:01:37,079
about inspiring folks from all
around the world to discover

29
00:01:37,289 --> 00:01:41,369
revolutionary technologies for
robotics, so that they can help

30
00:01:41,399 --> 00:01:44,639
in diverse underground settings
for time critical missions.

31
00:01:45,959 --> 00:01:48,959
Audrow Nash: And so this
competition has already

32
00:01:48,959 --> 00:01:53,039
occurred, correct? Yeah, what is
right, what did it look like?

33
00:01:53,249 --> 00:01:56,699
Like, what kind of challenges
were involved?

34
00:01:57,629 --> 00:02:01,139
Tim Chung: Yeah, so I like to
describe the DARPA SubT

35
00:02:01,139 --> 00:02:04,709
challenge, as we call it, as
kind of like this underground

36
00:02:04,709 --> 00:02:10,199
scavenger hunt slash triathlon
for robots. So the general idea

37
00:02:10,199 --> 00:02:13,529
here that we've heard time and
time again, from many of the

38
00:02:13,529 --> 00:02:17,099
folks like those first
responders that need to go into

39
00:02:17,099 --> 00:02:20,849
really challenging environments
is, you know, how come this one

40
00:02:20,849 --> 00:02:24,989
piece of component technology
works in the, in the one tunnel

41
00:02:24,989 --> 00:02:29,489
or the or the one parking garage
that I tested it in, but not in

42
00:02:29,519 --> 00:02:32,939
all the others that I have to
take this equipment into? And so

43
00:02:32,939 --> 00:02:36,149
the SubT challenge was about
how do we get robotic

44
00:02:36,149 --> 00:02:40,139
technologies to work across many
different types of underground

45
00:02:40,139 --> 00:02:43,379
settings. And that includes
human made tunnels like mines

46
00:02:43,379 --> 00:02:46,889
for mine search and rescue,
urban underground. That's

47
00:02:46,889 --> 00:02:50,819
everything from subway tunnels
to infrastructure, like storm

48
00:02:50,819 --> 00:02:54,869
drains, to parking garages, and
even naturally occurring cave

49
00:02:54,869 --> 00:02:59,729
network. So for the cave rescue
type settings. How would you get

50
00:02:59,729 --> 00:03:03,509
robots to be the best
triathlete? Not just the best

51
00:03:03,569 --> 00:03:07,169
runner cyclist, but you really
want that well rounded, robust,

52
00:03:08,279 --> 00:03:11,729
capable set of teams or robots
that go do this. And the problem

53
00:03:11,729 --> 00:03:14,369
that we posed in the safety
challenges, this underground

54
00:03:14,369 --> 00:03:18,419
scavenger hunt, DARPA goes into
these environments, places,

55
00:03:18,659 --> 00:03:21,659
objects of interest, and it's up
to these teams of robots to go

56
00:03:21,659 --> 00:03:24,209
out and find them and report
them back to their human

57
00:03:24,209 --> 00:03:24,749
teammates.

58
00:03:24,780 --> 00:03:26,940
Audrow Nash: What would be an
object of interest? I could say,

59
00:03:26,970 --> 00:03:27,870
what's an example?

60
00:03:28,860 --> 00:03:32,580
Tim Chung: Yeah, so we really
intentionally set up a whole

61
00:03:32,610 --> 00:03:36,210
array of artifacts, as we like
to call them. So you might

62
00:03:36,210 --> 00:03:39,660
think, for example, we put a
survivor artifact. This is a

63
00:03:39,690 --> 00:03:44,940
mannequin. So it looks like a
survivor. It has that visual

64
00:03:44,940 --> 00:03:48,090
appearance, but it also was a
thermal mannequin. So it has a

65
00:03:48,210 --> 00:03:51,570
thermal signature that's human
like, wow. And it even had a

66
00:03:51,570 --> 00:03:55,110
voice box. So you could have
voice recordings or sound

67
00:03:55,110 --> 00:03:58,110
recordings in there. And so
those robots going in to look

68
00:03:58,110 --> 00:04:01,950
for the survivor artifacts. Of
course, most typically carry

69
00:04:01,950 --> 00:04:06,660
cameras, that kind of makes
sense. But if you wanted to do a

70
00:04:06,660 --> 00:04:10,110
little bit better, or use other
cues, maybe you're going to have

71
00:04:10,110 --> 00:04:13,380
a thermal camera on board, or a
set of microphones. And that's

72
00:04:13,380 --> 00:04:17,250
just one example of the 10 types
of artifacts that we placed in

73
00:04:17,250 --> 00:04:21,120
these courses. We had a cell
phone in there that on the

74
00:04:21,120 --> 00:04:24,900
camera, probably really hard to
find, but with its Wi Fi, in

75
00:04:24,900 --> 00:04:27,990
Hotspot mode on cool and
Bluetooth and discoverable mode.

76
00:04:27,990 --> 00:04:31,350
Now you can start listening for
RF signatures for signs of life.

77
00:04:31,800 --> 00:04:36,150
We even had something we call
the gas artifacts. And this gas

78
00:04:36,150 --> 00:04:41,400
artifact clearly has no visual
signature, but instead, in this

79
00:04:41,400 --> 00:04:45,240
case, we're, of course being
saved. So we use co2 carbon

80
00:04:45,240 --> 00:04:50,100
dioxide. But you can imagine
that, that these robots equipped

81
00:04:50,100 --> 00:04:53,880
with co2 sensors, gas sensors
could be used to figure out

82
00:04:53,880 --> 00:04:57,450
where there's good air versus
bad air or noxious chemicals

83
00:04:57,450 --> 00:05:00,360
that might endanger those
responders going into these And

84
00:05:00,360 --> 00:05:04,500
then environments. And so this
quite wide array of types of

85
00:05:04,500 --> 00:05:08,790
artifacts meant that you can't
just send in one robot with one

86
00:05:08,790 --> 00:05:12,210
type of sensor, but in fact,
likely benefit from teams or

87
00:05:12,210 --> 00:05:16,230
robots carrying an array of
sensors to kind of cross cue and

88
00:05:16,230 --> 00:05:18,570
help each other out much like we
do as humans, or you

89
00:05:18,570 --> 00:05:21,150
Audrow Nash: could I mean, so I
could imagine one robot that has

90
00:05:21,150 --> 00:05:24,300
like every sensor possible, but
maybe this isn't the best

91
00:05:24,300 --> 00:05:28,350
strategy. So then you said,
teams of robots, what does a

92
00:05:28,350 --> 00:05:30,240
team of robots look like?

93
00:05:31,440 --> 00:05:34,530
Tim Chung: Yeah, well, one of
the things that we have

94
00:05:35,220 --> 00:05:38,550
telegraphed early on in this
whole competition, and this is

95
00:05:38,550 --> 00:05:42,000
the combination of multiple
years of teams coming together

96
00:05:42,180 --> 00:05:46,080
to build and ultimately break
some of their robots. But we

97
00:05:46,080 --> 00:05:49,800
said attrition is not only
possible, but likely in these

98
00:05:49,800 --> 00:05:52,770
types of really challenging
environment, terrain and other

99
00:05:52,770 --> 00:05:56,010
things. So for any team that
came with that one glorious

100
00:05:56,010 --> 00:06:00,120
robot, loaded out with all of
its sensors, and that's really

101
00:06:00,120 --> 00:06:03,690
putting all your eggs in one
basket. And so, you know,

102
00:06:03,720 --> 00:06:09,150
thinking through what that means
for your strategy for your, your

103
00:06:09,150 --> 00:06:13,230
competition, gameplay, if you
will, really led many of our

104
00:06:13,230 --> 00:06:16,260
teams to think about
diversifying their robot

105
00:06:16,260 --> 00:06:19,890
portfolio. And that not only
means two types of sensors that

106
00:06:19,890 --> 00:06:24,270
go on different types of robots,
but how they even use them in

107
00:06:24,270 --> 00:06:26,970
terms of like the positions on
the team or the roles on the

108
00:06:26,970 --> 00:06:30,930
team. So all of that coming
together, you had all sorts of

109
00:06:30,930 --> 00:06:34,560
robot types that can get into
crawl into different spaces,

110
00:06:34,560 --> 00:06:39,840
maybe fly into high verticals or
other open areas, some had legs

111
00:06:39,840 --> 00:06:44,070
that could walk upstairs or
downstairs or traverse. Adding

112
00:06:44,070 --> 00:06:47,790
more rugged terrain, tracks,
wheels, you name it. That's what

113
00:06:47,820 --> 00:06:51,600
the challenge is all about is to
go and highlight that the

114
00:06:51,600 --> 00:06:54,690
problem is this, bring your best
and brightest solutions to

115
00:06:54,720 --> 00:06:56,190
figure out what solves these
problems.

116
00:06:56,430 --> 00:06:59,490
Audrow Nash: And I'd like to
just think about the kind of

117
00:06:59,490 --> 00:07:04,170
make explicit to like, difficult
robotics challenges that are

118
00:07:04,170 --> 00:07:08,580
involved in this competition. So
like, whether it was like, Can

119
00:07:08,580 --> 00:07:11,040
you speak a bit about it, like I
mentioned, like mud and

120
00:07:11,310 --> 00:07:14,130
reflective surfaces, and like
all sorts of other things. But

121
00:07:14,130 --> 00:07:17,700
what were some of the challenges
that were especially difficult

122
00:07:17,700 --> 00:07:19,110
for robots in this situation?

123
00:07:20,070 --> 00:07:23,670
Tim Chung: Yeah, you name some
of them, for sure. I think

124
00:07:23,730 --> 00:07:28,110
having been crawled through a
number of tunnels and urban

125
00:07:28,110 --> 00:07:31,920
settings and caves myself, you
know, I can testify, I can

126
00:07:31,920 --> 00:07:35,490
testify that these are the types
of challenge elements that made

127
00:07:35,490 --> 00:07:39,630
it hard for humans, let alone
robots to navigate. So some of

128
00:07:39,630 --> 00:07:42,540
those you already mentioned, the
types of surfaces, mud,

129
00:07:42,660 --> 00:07:47,040
moisture, puddles, water, you
never know, we might be able to

130
00:07:47,040 --> 00:07:51,360
see your tell if the puddle
depth is just an inch, and I'm

131
00:07:51,390 --> 00:07:53,640
going to be able to step on
right through. But the

132
00:07:53,640 --> 00:07:56,370
reflectivity of that and the
difficulty that robots might

133
00:07:56,370 --> 00:08:00,960
have, that makes it a really
daunting kind of, you know,

134
00:08:01,020 --> 00:08:04,770
survival question of whether or
not to step foot or roll into a

135
00:08:04,770 --> 00:08:09,180
potential puddle, or, you know,
potential hole. Yeah, lighting,

136
00:08:09,420 --> 00:08:14,220
lighting is a major issue. So
oftentimes he or only relying on

137
00:08:14,220 --> 00:08:19,380
the lighting you bring yourself.
A lot of times there's dust or

138
00:08:19,380 --> 00:08:22,380
fog, or certainly in emergency
settings, you can imagine things

139
00:08:22,380 --> 00:08:26,130
like smoke, including many of
your sensors. So thinking

140
00:08:26,130 --> 00:08:29,970
through what kind of paradigms
you would use to be able to

141
00:08:29,970 --> 00:08:34,050
navigate when you really can't
see, again, in the context of

142
00:08:34,050 --> 00:08:41,430
cameras, so DARPA used smoke
machines to go and learn from

143
00:08:41,430 --> 00:08:45,750
sections of these environments,
so that we would intentionally

144
00:08:45,750 --> 00:08:48,840
stress test some of that. And
then you start to learn, you

145
00:08:48,840 --> 00:08:53,670
know, things like what types of
sensors are transparent to

146
00:08:53,670 --> 00:08:57,690
certain types of dust or a if I
fly my multi rotors, my flying

147
00:08:57,690 --> 00:09:00,300
vehicles, they're going to kick
up dust as they're flying

148
00:09:00,450 --> 00:09:03,720
through. Yeah, as was the case
in these minds. And that can

149
00:09:03,750 --> 00:09:07,950
hose your senses as well. So
self inflicted something

150
00:09:08,100 --> 00:09:12,570
problems as well. So
verticality, I'll throw that one

151
00:09:12,570 --> 00:09:17,100
out. So the three dimensional
nature's a really big player in

152
00:09:17,100 --> 00:09:19,560
these underground settings,
because you're not just going

153
00:09:19,560 --> 00:09:24,840
out, but you're also going down
or up. And so elevator shafts to

154
00:09:24,900 --> 00:09:30,210
stairwells of varying types to
scrambling up older piles to

155
00:09:30,240 --> 00:09:34,140
wow, other opportunities to look
in these wide open caverns that

156
00:09:34,140 --> 00:09:39,930
you might find a real real, you
know, like a real diverse set of

157
00:09:39,930 --> 00:09:44,340
those kinds of challenge
elements. And never knowing

158
00:09:44,460 --> 00:09:47,700
what's around that next corner
is, is perhaps the most daunting

159
00:09:47,730 --> 00:09:49,530
challenge for most of these
robots.

160
00:09:49,680 --> 00:09:54,390
Audrow Nash: Yeah, sure. How did
you how were teams structured so

161
00:09:54,390 --> 00:09:57,480
I know there was the hardware
and the software come or the

162
00:09:58,110 --> 00:10:03,180
systems and simulation
components of this. But for the

163
00:10:03,210 --> 00:10:09,180
hardware one, how did you say a
team can have some certain

164
00:10:09,180 --> 00:10:12,360
number? How do you organize what
robots a team could have? And

165
00:10:12,360 --> 00:10:13,770
was there standardization?

166
00:10:15,719 --> 00:10:18,359
Tim Chung: Sure, well, as you
mentioned, there's the systems

167
00:10:18,359 --> 00:10:21,629
competition and the virtual
competition. And there were our

168
00:10:21,719 --> 00:10:27,929
our sister competitions, where
one could build and design and

169
00:10:27,929 --> 00:10:30,779
break robots. And the other, you
would do all of that in the

170
00:10:30,779 --> 00:10:35,189
cloud in a simulator. And so in
the context of the parameters,

171
00:10:35,189 --> 00:10:38,909
which we defined the challenge,
it was, quite frankly, quite

172
00:10:38,939 --> 00:10:44,789
loosely defined intentionally.
In some cases. It's the you

173
00:10:44,789 --> 00:10:47,519
know, without constraining the
design or solution space,

174
00:10:47,519 --> 00:10:50,459
sometimes that's where you find
those really creative or

175
00:10:50,459 --> 00:10:52,949
solutions, innovative solutions,
right. And so this is

176
00:10:52,949 --> 00:10:56,609
specifically for the hardware.
The system, yeah, version of the

177
00:10:56,609 --> 00:11:01,019
competition now. Yeah, that's
right. And so we would specify

178
00:11:01,049 --> 00:11:08,339
that the type of environment is
a coal mine, and highlight that

179
00:11:08,339 --> 00:11:13,319
most robots, if you want to
reach all parts of the course

180
00:11:13,349 --> 00:11:17,609
need to be able to fit through a
one by one meter area. So

181
00:11:17,609 --> 00:11:21,899
something like a manhole cover,
if you elected to bring much

182
00:11:21,899 --> 00:11:24,659
larger robots, then yeah, you
still might be able to access

183
00:11:24,659 --> 00:11:27,059
parts of the course. But you're
probably going to be leaving

184
00:11:27,059 --> 00:11:31,379
some points on the table, if you
can't access some other segments

185
00:11:31,379 --> 00:11:36,479
of the course. And then beyond
that, quite frankly, DARPA, you

186
00:11:36,479 --> 00:11:41,039
know, left it to the competitors
imaginations, to identify not

187
00:11:41,039 --> 00:11:45,269
only what they might face, but
what robot types or technologies

188
00:11:45,449 --> 00:11:50,129
would be advantageous for them
to bring two to conquer that

189
00:11:50,129 --> 00:11:53,699
kind of an environment. So there
was no limit on the number of

190
00:11:53,699 --> 00:11:59,849
robots, there was very little
limit on the style of robots.

191
00:12:00,389 --> 00:12:03,509
There were some safety
considerations on maybe exotic

192
00:12:03,509 --> 00:12:07,859
fuel types or what have you. But
other than that, teams were able

193
00:12:07,859 --> 00:12:12,749
to use existing available
platforms, and then really just

194
00:12:12,989 --> 00:12:17,309
keep on sensors or payloads,
radios, or what have you on top.

195
00:12:17,339 --> 00:12:21,449
Or we did have teams that design
robots from scratch, really

196
00:12:21,449 --> 00:12:24,119
thinking through the nuances of
these underground environments

197
00:12:24,119 --> 00:12:28,019
in their build design power. So
yeah, what, you know, clean

198
00:12:28,019 --> 00:12:32,039
sheet approach to trying to
solve a really hard technical

199
00:12:32,039 --> 00:12:38,339
problem as a kind of a mechanism
here for identifying interesting

200
00:12:38,339 --> 00:12:40,799
solutions that, you know,
frankly, we might not have seen

201
00:12:40,829 --> 00:12:44,339
or thought about. And we too
narrowly defined the problem.

202
00:12:44,730 --> 00:12:48,000
Audrow Nash: Gotcha. Was there
like a budget constraint for the

203
00:12:48,000 --> 00:12:50,970
teams? Or was there was it like
open budget?

204
00:12:52,080 --> 00:12:54,780
Tim Chung: You can kind of buy
by and bring whatever you will

205
00:12:54,810 --> 00:13:01,680
for the systems competition?
Yeah, it really was kind of open

206
00:13:01,710 --> 00:13:05,550
teams could and in fact, many
didn't seek out external

207
00:13:05,550 --> 00:13:10,740
sponsorships or donated
hardware. One of our teams found

208
00:13:10,740 --> 00:13:15,120
some repurpose some robots that
he found on Craigslist. Wow,

209
00:13:15,210 --> 00:13:19,590
that's crazy, was able to turn
those into ground robotic suite.

210
00:13:19,650 --> 00:13:24,660
Right. And so being really
creative with both the the the

211
00:13:24,660 --> 00:13:27,960
resources that you have, and the
solutions you want to bring to

212
00:13:27,960 --> 00:13:30,540
bear. Yeah, that's where a lot
of that creativity came.

213
00:13:30,690 --> 00:13:33,780
Audrow Nash: Gotcha. Well, so
what was the what did some of

214
00:13:33,780 --> 00:13:36,360
the teams look like? Like? How
would you describe some of the

215
00:13:36,360 --> 00:13:42,510
robots maybe that show kind of
the diversity of teams of robots

216
00:13:42,510 --> 00:13:43,410
that were entered?

217
00:13:44,460 --> 00:13:47,580
Tim Chung: Yeah. Well, what was
really exciting from my

218
00:13:47,640 --> 00:13:50,880
perspective about this empty
challenge is we had constructed

219
00:13:50,880 --> 00:13:57,240
it in kind of a iterative
development manner. And so we

220
00:13:57,240 --> 00:14:01,590
started off, for example, in a
coal mine, and we call it our

221
00:14:01,650 --> 00:14:05,640
tunnel circuit. So, of course,
this human made tunnels,

222
00:14:05,790 --> 00:14:08,730
thinking about the mine rescue
type scenarios, consulting with

223
00:14:08,730 --> 00:14:13,020
mine, rescue responders, and so
forth. We went to a coal mine.

224
00:14:13,020 --> 00:14:16,530
And then approximately six
months after that, we went to an

225
00:14:16,530 --> 00:14:19,500
unfinished nuclear power plant,
and that was our urban circuit,

226
00:14:20,220 --> 00:14:24,390
and then working and exploring
the possibility of kind of

227
00:14:24,390 --> 00:14:28,110
transforming a cave into a test
course. But of course, we did

228
00:14:28,110 --> 00:14:32,280
have to cancel that due to
COVID. But then, the final

229
00:14:32,280 --> 00:14:36,060
events allowed us to bring
together all three of those

230
00:14:36,060 --> 00:14:40,230
types of environments together.
And well, I'm sure we'll have a

231
00:14:40,230 --> 00:14:43,770
chance to talk about that here
in a bit. But we fabricated this

232
00:14:43,770 --> 00:14:48,300
course that allowed teams to
test their robots against all

233
00:14:48,300 --> 00:14:51,930
three of these environment types
all in this magnificent course

234
00:14:51,930 --> 00:14:57,150
that that DARPA fabricated. And
so, the reason I bring that up

235
00:14:57,150 --> 00:15:01,980
is because you saw there was no
one form factor for a team,

236
00:15:02,250 --> 00:15:06,630
there was this continuous growth
or evolution, if you will, who

237
00:15:06,630 --> 00:15:10,770
will have both the individual
technologies, but also the team

238
00:15:10,770 --> 00:15:15,630
compositions. And so we saw a
lot of robots that were wheeled,

239
00:15:15,900 --> 00:15:18,930
and we tried to see some flying
robots in the tunnels circuit in

240
00:15:18,930 --> 00:15:21,600
the coal mines. And I can say
that the wheeled robots did

241
00:15:21,600 --> 00:15:24,450
well, and the flying robots did
not. And that was a great

242
00:15:24,450 --> 00:15:28,500
learning point. Whereas we pivot
to the urban circuit. Now these

243
00:15:28,500 --> 00:15:34,500
are expansive think everything
from warehouses to elevator

244
00:15:34,500 --> 00:15:38,790
shafts to other things that
these drones really helped allow

245
00:15:38,820 --> 00:15:43,320
robots in Canada teams to go and
explore places that just the

246
00:15:43,320 --> 00:15:46,500
ground robotically would never
have been able to do. And then

247
00:15:46,500 --> 00:15:50,790
also saw the showcasing of
legged robots, these quadruped

248
00:15:50,790 --> 00:15:54,990
Ed robots, so that they could go
up and down stairs with a fair

249
00:15:54,990 --> 00:16:00,600
bit more ease than some of their
wheeled counterparts. And so you

250
00:16:00,600 --> 00:16:05,670
kind of see this progression of
both tailoring the fleets right

251
00:16:05,670 --> 00:16:08,700
who's going to be in your
starting lineup might be a

252
00:16:08,700 --> 00:16:11,970
little bit shaped by the type of
environment you're going to send

253
00:16:11,970 --> 00:16:16,260
these fleets of robots into. But
by and large, that continued

254
00:16:16,410 --> 00:16:20,190
learning process culminated in
the final event, where I'd say

255
00:16:20,190 --> 00:16:24,750
you saw a pretty healthy mix of
air grounds, wheeled and legged.

256
00:16:25,410 --> 00:16:29,670
We even had marsupial robots, so
ground robots carrying aerial

257
00:16:29,670 --> 00:16:32,520
robots on their on their back,
piggybacking them, it's so cool

258
00:16:32,520 --> 00:16:36,960
to other parts of the course.
And really, that, that

259
00:16:36,960 --> 00:16:42,540
diversity, the fact that there's
not one and have common makeup

260
00:16:42,540 --> 00:16:47,670
of a team, I think is a hallmark
of kind of the, you know, the

261
00:16:47,790 --> 00:16:51,660
the what we're going for in a
challenge of being able to tease

262
00:16:51,660 --> 00:16:55,170
out all these various ways to
attack this problem.

263
00:16:56,940 --> 00:17:01,200
Audrow Nash: Now, how do you, I
guess, so you have these teams,

264
00:17:01,200 --> 00:17:05,340
these teams all have the
hardware. They go and the

265
00:17:05,340 --> 00:17:11,250
competition is about finding the
markers, or what we find them

266
00:17:11,280 --> 00:17:15,330
the points that artifact
artifacts, artifacts, and also

267
00:17:15,330 --> 00:17:20,430
it was about what total distance
covered or something like this

268
00:17:20,430 --> 00:17:24,210
with this, these were the two
significant indicators of

269
00:17:24,210 --> 00:17:25,020
performance,

270
00:17:25,530 --> 00:17:29,910
Tim Chung: or Yeah, so work, we
yeah, we, you know, we kept it

271
00:17:29,910 --> 00:17:33,930
as simple as you have a fixed
amount of time, you have an

272
00:17:33,930 --> 00:17:37,290
hour. And you have to go and
find these artifacts. And what

273
00:17:37,290 --> 00:17:41,370
we mean by that is you have to
be able to return the location

274
00:17:41,400 --> 00:17:45,510
and the type of the artifact
that you found, you know, you

275
00:17:45,510 --> 00:17:48,030
have to do that location
positioning to within five

276
00:17:48,030 --> 00:17:52,140
meters. So the idea here is,
yeah, so you go into these

277
00:17:52,140 --> 00:17:55,380
places, you have no GPS, I
should have, you know, maybe

278
00:17:55,380 --> 00:17:59,280
stated that upfront. And so
you're moving through these

279
00:17:59,280 --> 00:18:02,460
environments, and you know, it's
pretty easy to get twisted and

280
00:18:02,460 --> 00:18:07,530
turned around and get lost. And
so being able to report out

281
00:18:07,560 --> 00:18:11,940
where you found that survivor
artifact, for example, to within

282
00:18:11,940 --> 00:18:16,560
five meters of global position.
So that if you had to send in

283
00:18:16,560 --> 00:18:20,310
humans and would know, that
location, it'd be pinpointed

284
00:18:20,310 --> 00:18:23,220
within, nominally within arm's
reach. That's the kind of

285
00:18:23,430 --> 00:18:26,220
notional metrics for
firefighters, you want to get

286
00:18:26,220 --> 00:18:29,190
them within arm's reach, because
it's going to be smoke filled,

287
00:18:29,460 --> 00:18:32,040
and they need to be able to
reach out, touch the thing that

288
00:18:32,040 --> 00:18:36,720
they got sent into to go find.
So within five meters, you have

289
00:18:36,720 --> 00:18:39,960
to go find that out, and it's
not sufficient to have found it.

290
00:18:40,350 --> 00:18:44,280
But you have to get to get that
information back out. Right? You

291
00:18:44,280 --> 00:18:46,230
have to get it correct, of
course, what type it is, but

292
00:18:46,230 --> 00:18:49,260
then you have to report it out.
And if you successfully report

293
00:18:49,260 --> 00:18:53,550
out the physician to within that
five meters and the correct

294
00:18:53,550 --> 00:18:57,120
time, then you get a point. And
so the aims that find the most

295
00:18:57,120 --> 00:19:01,770
artifacts in that hour, are the
ones that take home. Yeah. It to

296
00:19:01,770 --> 00:19:02,610
do the victory lap at

297
00:19:02,610 --> 00:19:06,420
Audrow Nash: the prize. And it
was the prize was for the what

298
00:19:06,420 --> 00:19:08,070
was the prize for the
competition?

299
00:19:08,100 --> 00:19:11,190
Tim Chung: Yeah, so we had cash
prizes. This is one of the

300
00:19:11,220 --> 00:19:14,730
hallmarks of these DARPA Grand
Challenges. And so for that

301
00:19:14,880 --> 00:19:18,750
systems competition, building
the robots in such the top place

302
00:19:18,750 --> 00:19:22,890
to come $2 million. Wow, to
their credit, that's awesome.

303
00:19:23,550 --> 00:19:27,390
And Wow, very good. Like
publicity and everything to

304
00:19:27,390 --> 00:19:31,470
which is wonderful. Yeah. All
right. It's the glory of winning

305
00:19:31,470 --> 00:19:32,730
the challenge for DARPA.

306
00:19:33,270 --> 00:19:36,960
Audrow Nash: So the robots going
back to finding the artifacts.

307
00:19:37,410 --> 00:19:41,790
They would you're not providing
the robots a map of the minds or

308
00:19:41,790 --> 00:19:44,400
of the environment beforehand.
They're building it while

309
00:19:44,400 --> 00:19:45,300
they're going through it. Is

310
00:19:45,300 --> 00:19:47,970
Tim Chung: that correct? Yeah,
that's absolutely correct. In

311
00:19:47,970 --> 00:19:51,390
fact, that's one of the
hallmarks of how we constructed

312
00:19:51,450 --> 00:19:55,860
the SubT challenge. Neither
robot nor human knew the man on

313
00:19:55,860 --> 00:19:59,070
these competitor teams has set
foot inside this course and know

314
00:19:59,070 --> 00:20:03,930
anything really about And so it
really is about exploring the

315
00:20:03,960 --> 00:20:07,020
unknown environment and then
exploiting it to go and find

316
00:20:07,020 --> 00:20:09,090
those artifacts quickly and
returning that

317
00:20:09,090 --> 00:20:11,010
Audrow Nash: information.
Gotcha. So the robots are

318
00:20:11,010 --> 00:20:13,740
building a map while they're
going in it. And this is why you

319
00:20:13,740 --> 00:20:16,410
can get twisted up. So if the
robots building in an accurate

320
00:20:16,410 --> 00:20:19,680
map, say it's like a crew
accumulating error as it's

321
00:20:19,680 --> 00:20:22,320
building the man, yeah,
absolutely, then the map might

322
00:20:22,320 --> 00:20:26,220
be inaccurate, inaccurate. And
then you could, you could say, I

323
00:20:26,220 --> 00:20:30,480
found something, I found it
here. But because it found it,

324
00:20:30,540 --> 00:20:34,980
and the map is distorted, the
location is wrong. And they

325
00:20:34,980 --> 00:20:38,850
would get no points because of a
distorted map. Gotcha. That's

326
00:20:38,850 --> 00:20:42,000
right. Now, one thing that I
imagined because it's or that

327
00:20:42,000 --> 00:20:45,690
I've seen discussed in watching
some of the videos about SubT,

328
00:20:46,380 --> 00:20:50,250
one of the major challenges was
networking. Can you talk a bit

329
00:20:50,250 --> 00:20:50,880
about this?

330
00:20:51,750 --> 00:20:55,230
Tim Chung: Yeah, sure. I think
this is probably one of the

331
00:20:56,670 --> 00:20:59,430
understated types of problems,
when you go into these

332
00:20:59,430 --> 00:21:04,050
underground environments, that
no amount of testing in a lab or

333
00:21:04,080 --> 00:21:08,550
in even the basement of a
university building, will really

334
00:21:08,550 --> 00:21:13,710
get you the feel of RF, the
radio frequency of underground

335
00:21:13,710 --> 00:21:17,820
environments, because it's just
so tightly coupled to everything

336
00:21:17,820 --> 00:21:21,300
from the thickness of the
concrete, whether they're steel

337
00:21:21,300 --> 00:21:25,860
girders, or you know, rebar, if
there's, you know, metallic, you

338
00:21:25,860 --> 00:21:31,020
know, metals in the in the soil,
you know, all of those, if it

339
00:21:31,020 --> 00:21:35,490
rains that morning, it that
plays a role. And so because of

340
00:21:35,490 --> 00:21:39,390
those dependencies Sue so hard
to really think about

341
00:21:40,560 --> 00:21:45,870
communications in a robust way.
Now, that was one of the four

342
00:21:45,870 --> 00:21:48,630
technology areas that the subsea
challenge was deeply interested

343
00:21:48,630 --> 00:21:54,570
in. And so we did see teams
really get in some cases, both

344
00:21:55,170 --> 00:22:00,840
surprised and confounded by how
they're calm solutions were

345
00:22:00,840 --> 00:22:04,080
working under ground. So we saw
a lot of different approaches,

346
00:22:04,110 --> 00:22:08,370
we had some teams come up with
tethered robots that they would

347
00:22:08,370 --> 00:22:14,790
deploy from the outside, and
tethers long as they could to

348
00:22:14,790 --> 00:22:19,560
maintain connectivity. And that
robot, imagine, yeah, it's a

349
00:22:19,560 --> 00:22:24,030
long cable. I certainly twisted
it will get snagged it would be

350
00:22:24,480 --> 00:22:28,890
a tangle hazard. And so there
are many, many good reasons why

351
00:22:28,920 --> 00:22:32,940
not to use a tether, but some
were able to make good use of

352
00:22:32,940 --> 00:22:35,880
that. There were others that
used breadcrumbs. So now I'm

353
00:22:35,880 --> 00:22:41,940
carrying breadcrumb relay nodes
on on the robot. And as they

354
00:22:42,090 --> 00:22:46,110
traverse either fixed amount of
distance, and to like console

355
00:22:46,110 --> 00:22:49,620
and Gretel, they were at corner.
Drop. Yeah, that's right. So

356
00:22:49,620 --> 00:22:50,400
they would drop.

357
00:22:51,480 --> 00:22:52,740
Audrow Nash: Hansel and Gretel.
You're right.

358
00:22:52,920 --> 00:22:57,360
Tim Chung: Yeah. breadcrumb
comms nodes. And that would

359
00:22:57,390 --> 00:23:01,320
allow them to relay their
communications from robots robot

360
00:23:01,320 --> 00:23:05,730
or robots a base station. And
then you had other models of

361
00:23:05,730 --> 00:23:08,820
comms, which was, you know, I'm
gonna live without

362
00:23:08,820 --> 00:23:12,720
communications, I'm gonna go,
I'm gonna go off the grid for a

363
00:23:12,720 --> 00:23:17,190
little while do my searching,
scouting, mapping, finding. And

364
00:23:17,190 --> 00:23:21,450
then after either a set amount
of time, or maybe I have really

365
00:23:21,450 --> 00:23:24,180
good data that I don't want to
lose, I'm going to come back

366
00:23:24,180 --> 00:23:27,840
into columns, and then try to
dump it, you know, to my

367
00:23:27,840 --> 00:23:32,610
teammate, or, or to the base
station. So there are all sorts

368
00:23:32,610 --> 00:23:36,360
of columns technologies, but
also come strategies that

369
00:23:36,360 --> 00:23:39,960
emerged as a way to deal with
the difficulties of RF of

370
00:23:39,960 --> 00:23:42,810
communicating underground, is
there.

371
00:23:44,520 --> 00:23:46,710
Audrow Nash: So I guess there's
a lot of structural things that

372
00:23:46,710 --> 00:23:50,940
make it really hard to do radio
frequencies in these like

373
00:23:50,940 --> 00:23:54,270
tunnels and things like this,
like you turn a corner, and then

374
00:23:54,270 --> 00:23:59,130
the signal kind of just doesn't
bounce down the hall back down

375
00:23:59,160 --> 00:24:02,580
back through the corner, and so
you can't get it out. And so

376
00:24:02,580 --> 00:24:05,070
that's why there were all these
difficult. This was this was

377
00:24:05,070 --> 00:24:07,050
such a difficulty. Right?

378
00:24:07,080 --> 00:24:10,590
Tim Chung: Yeah, yeah, exactly.
And that's a good rule of thumb.

379
00:24:10,620 --> 00:24:13,800
You know, if you turn the
corner, your signal goes down

380
00:24:13,800 --> 00:24:18,960
dramatically. Yep. And, you
know, if you've ever had those

381
00:24:18,960 --> 00:24:22,080
kinds of problems, getting your
cell service, working in a

382
00:24:22,080 --> 00:24:26,430
subway station, or, you know,
something like that, you know,

383
00:24:26,430 --> 00:24:32,340
that RF just doesn't propagate
as well underground and through

384
00:24:32,340 --> 00:24:35,880
the earth. And so that's
something that the teams really

385
00:24:35,880 --> 00:24:36,570
had to wrestle with.

386
00:24:37,230 --> 00:24:43,050
Audrow Nash: Yeah, I'm curious
about. So the motivation for

387
00:24:43,050 --> 00:24:47,190
this competition. Part of it,
it's first responders and things

388
00:24:47,190 --> 00:24:53,490
like this. Can you talk a bit
more about this? Sure.

389
00:24:53,970 --> 00:24:58,620
Tim Chung: Well, I'd say it was
highly motivated at many of the

390
00:24:58,620 --> 00:25:02,340
robotics technologies We have as
well as some of the component

391
00:25:02,340 --> 00:25:05,280
technologies, whether it's
sensors or radios.

392
00:25:05,520 --> 00:25:09,240
Audrow Nash: You still there?
Yep. Still, yeah, you're saying

393
00:25:09,240 --> 00:25:11,550
so many of the robotics
technology seems to freeze.

394
00:25:12,450 --> 00:25:15,060
Tim Chung: Yeah, there's so many
other robotics technologies that

395
00:25:15,060 --> 00:25:19,590
we had, you know, four years
ago. You know, even the

396
00:25:19,590 --> 00:25:24,270
component technologies at radios
or component sensors, they were,

397
00:25:24,300 --> 00:25:27,690
you know, frankly, limited in
how robust they were how

398
00:25:27,690 --> 00:25:32,040
resilient, they were to that
vast, very diverse set of

399
00:25:32,040 --> 00:25:35,520
underground settings. And so
that's kind of know, from a

400
00:25:35,520 --> 00:25:39,750
technology point of view, we
viewed this DARPA subterranean

401
00:25:39,750 --> 00:25:45,240
challenge as a way to spotlight
and accelerate technology

402
00:25:45,240 --> 00:25:48,870
development, not just of these
individual small component

403
00:25:48,870 --> 00:25:52,500
technologies, but in fact, how
we think about integrating all

404
00:25:52,500 --> 00:25:56,310
of them into resilient systems
of technologies. And I think

405
00:25:56,310 --> 00:25:59,610
that's kind of the technology
motivation. But when we come

406
00:25:59,610 --> 00:26:04,620
back to it operationally, you
know, if there is this period of

407
00:26:04,620 --> 00:26:09,840
time where an emergency response
scenario is occurring, and the

408
00:26:09,840 --> 00:26:12,600
incident commanders marshaling
our forces and trying to figure

409
00:26:12,600 --> 00:26:17,010
out what resources he has
available, wouldn't it be great

410
00:26:17,010 --> 00:26:21,480
in that kind of roughly one to,
say, four hour time period, you

411
00:26:21,480 --> 00:26:26,790
know, that, that golden hour, if
you will, or she knows enough to

412
00:26:26,790 --> 00:26:30,900
send in humans, you know, at the
at great risk, wouldn't it be

413
00:26:30,900 --> 00:26:34,620
great to send it robots, if we
could do that, and pinpoint

414
00:26:34,620 --> 00:26:39,570
where the survivors are located,
or trapped or otherwise, or even

415
00:26:39,570 --> 00:26:42,600
if it's what kind of equipment
these first responders should be

416
00:26:42,780 --> 00:26:46,800
carrying in with them. That was
kind of the key takeaway, if you

417
00:26:46,800 --> 00:26:49,830
can tell me that I should be
bringing in shoring equipment,

418
00:26:49,830 --> 00:26:54,390
because the ceiling might
collapse. And that will prolong

419
00:26:54,720 --> 00:26:57,900
and reduce risk to the
responders that have to operate

420
00:26:57,900 --> 00:27:00,480
in that environment. It'd be
great that the firefighter

421
00:27:00,480 --> 00:27:03,120
didn't have to go in there
first, figure that out, come all

422
00:27:03,120 --> 00:27:06,750
the way back out, and then carry
in all of that equipment. Same

423
00:27:06,750 --> 00:27:10,800
with re breathing equipments,
and things of that nature. So

424
00:27:11,040 --> 00:27:14,100
just being able to we like to
call it actionable situational

425
00:27:14,100 --> 00:27:18,090
awareness. Yeah, it's it's not
just not just the map, which is

426
00:27:18,090 --> 00:27:22,590
really helpful. And in fact, you
know, a lot of folks just kind

427
00:27:22,590 --> 00:27:26,310
of would be grateful just to
even have a map. Yeah, but DARPA

428
00:27:26,310 --> 00:27:31,920
likes to go well beyond what is
needed today, and anticipate

429
00:27:31,920 --> 00:27:35,550
what would be really beneficial
in the future. And this notion

430
00:27:35,550 --> 00:27:38,640
of actionable situational
awareness, is this idea of being

431
00:27:38,640 --> 00:27:42,000
able to provide the incident
commander, who's about to send

432
00:27:42,000 --> 00:27:47,430
in her teams of humans, the best
possible set of information like

433
00:27:47,490 --> 00:27:51,540
hazards, where key objects of
interest are like those

434
00:27:51,540 --> 00:27:56,160
survivors, where you know, where
blind corners might be, where

435
00:27:56,160 --> 00:28:00,450
she might lose comms, all of
those points are very relevant.

436
00:28:00,450 --> 00:28:03,030
And that's the kind of
information we really want it to

437
00:28:03,030 --> 00:28:05,520
be able to develop the
technologies to be able to

438
00:28:05,520 --> 00:28:08,820
provide or in the long run, so
that really served as the

439
00:28:08,880 --> 00:28:14,670
motivating operational setting.
Yeah. And I think it's been

440
00:28:14,670 --> 00:28:17,400
exciting to see how this
technology is mature to try to

441
00:28:17,400 --> 00:28:18,060
meet that Mark,

442
00:28:18,090 --> 00:28:22,380
Audrow Nash: for sure. To me, it
seems like the two hardest parts

443
00:28:22,380 --> 00:28:26,550
of this competition for robotics
teams would be first, the

444
00:28:26,550 --> 00:28:32,400
networking, and then also just
the mobility, getting around in

445
00:28:32,400 --> 00:28:36,360
these environments, because I
think, like, you can find

446
00:28:36,360 --> 00:28:39,690
different sensor pack, and maybe
I'm mistaken in this. But I

447
00:28:39,690 --> 00:28:42,330
imagine you can find different
sensors that are good in

448
00:28:42,330 --> 00:28:45,900
different environments. But they
may have big, they may be bulky,

449
00:28:45,900 --> 00:28:48,120
they may have three power
requirements, whatever it might

450
00:28:48,120 --> 00:28:54,720
be. But it seems like given the
constraint of these tunnels, or

451
00:28:54,720 --> 00:28:57,570
these environments, where cons
aren't really good, it's like,

452
00:28:58,170 --> 00:29:00,780
the big benefits of this
challenge would be testing

453
00:29:00,780 --> 00:29:04,050
actual robot hardware in these
environments to see how well it

454
00:29:04,050 --> 00:29:07,800
can do. And maybe like, what
approaches might be a bit more

455
00:29:07,800 --> 00:29:12,060
robust. And then the second part
would be okay, now that we have

456
00:29:12,060 --> 00:29:15,300
these robots moving around, how
do we get them communicating

457
00:29:15,300 --> 00:29:19,590
their information out? Or
connecting? So I mean, like, you

458
00:29:19,590 --> 00:29:22,350
could have kind of done robots
that aren't doing much of the

459
00:29:22,410 --> 00:29:25,170
compute other than like, their
localization. And whatever

460
00:29:25,290 --> 00:29:27,390
you're doing, they're sending
information all the way back

461
00:29:27,420 --> 00:29:32,550
outside, for processing and kind
of keeping a global state of the

462
00:29:32,550 --> 00:29:36,720
situation or whatever it would
be. It seems like kind of the

463
00:29:36,720 --> 00:29:40,080
two areas that are pushed most
by this competition.

464
00:29:40,770 --> 00:29:44,100
Tim Chung: So So you nailed
them. And so in fact, we had

465
00:29:44,100 --> 00:29:48,930
four tech areas that we
predominantly focused on the

466
00:29:48,930 --> 00:29:51,660
first. The first ones you've
already mentioned, it's the

467
00:29:51,660 --> 00:29:56,010
networking. Mobility is the
second one. You got to be able

468
00:29:56,010 --> 00:29:59,490
to get to these places, of
course, but the other two areas

469
00:29:59,490 --> 00:30:03,780
that I'd also highlight are the
autonomy the decision making of

470
00:30:03,780 --> 00:30:08,100
these robots in the face of
significant uncertainty. And

471
00:30:08,370 --> 00:30:11,070
fourth and final was the
perception piece. So how do you

472
00:30:11,070 --> 00:30:15,000
tie together? Not just like you
said, the sensors, but what you

473
00:30:15,000 --> 00:30:19,260
do with that information? How do
you fuse different sensing

474
00:30:19,260 --> 00:30:21,990
views, right? Those myriad
sensors that you might be

475
00:30:21,990 --> 00:30:25,650
carrying? How do they provide
you with a common picture of

476
00:30:25,650 --> 00:30:29,250
what is actually happening in
this environment? And so all for

477
00:30:29,280 --> 00:30:33,060
autonomy, mobility, networking,
and perception, we kind of

478
00:30:33,060 --> 00:30:35,790
really need to all come
together, for sure. And we found

479
00:30:36,090 --> 00:30:39,450
the most successful teams were
the ones that really thought

480
00:30:39,450 --> 00:30:43,410
about it holistically, and they
didn't, again, rely solely on an

481
00:30:43,410 --> 00:30:47,220
autonomy solution to solve all
of their networking problems, or

482
00:30:47,430 --> 00:30:52,980
didn't, you know, invest all of
their dollars in the best radios

483
00:30:53,010 --> 00:30:58,050
and forego thinking about
sensors in perception? So?

484
00:30:59,130 --> 00:31:02,190
Audrow Nash: Yeah, it's like
four dimensions, really? And

485
00:31:02,190 --> 00:31:05,070
yeah, to be successful in this
company?

486
00:31:05,370 --> 00:31:07,200
Tim Chung: I think that's a
twist. Yeah, that's a true

487
00:31:07,200 --> 00:31:07,620
statement.

488
00:31:09,090 --> 00:31:11,190
Audrow Nash: I think I need to
change the name of this podcast

489
00:31:11,190 --> 00:31:13,980
to have something about
networking. Because something

490
00:31:13,980 --> 00:31:16,740
thinking acting are kind of
They're analogous to three of

491
00:31:16,740 --> 00:31:20,970
those. But then, yeah, working
systems, robots, and have fun.

492
00:31:21,900 --> 00:31:25,740
Tim Chung: And, and that's kind
of another insight coming from

493
00:31:26,100 --> 00:31:32,850
this competition has been the
role of teams, not just, you

494
00:31:32,850 --> 00:31:37,140
know, we might think of robots
and humans as teammates, and you

495
00:31:37,140 --> 00:31:40,620
have robots and robots as
teammates. And so just like you

496
00:31:40,620 --> 00:31:44,520
said, the name of this podcast,
is this additional element

497
00:31:44,550 --> 00:31:48,990
where, in addition to what
autonomy, perception and

498
00:31:48,990 --> 00:31:53,370
mobility I might have, as an
individual player, how do I

499
00:31:53,400 --> 00:31:57,780
leverage the team so that I can
cover all the bases

500
00:31:57,780 --> 00:32:01,080
appropriately, so that maybe I
don't need, you know, it's like

501
00:32:01,080 --> 00:32:04,320
the relay races, I don't need to
be the best cyclist. If I know,

502
00:32:04,860 --> 00:32:09,090
I'm a good runner, and my
teammates are a good cyclist,

503
00:32:09,120 --> 00:32:14,910
oh, they can help augment what I
might be deficient in. And so

504
00:32:14,910 --> 00:32:20,880
that, that kind of level of
flexibility, kind of the, the

505
00:32:20,910 --> 00:32:25,560
design choices that that just
explodes. The design space,

506
00:32:25,590 --> 00:32:30,900
really. And, you know, I think
that's really one of the

507
00:32:30,900 --> 00:32:34,050
fascinating pieces. And so
communication, communicating,

508
00:32:34,050 --> 00:32:42,390
teaming is that additional
angle, additional perspective

509
00:32:42,390 --> 00:32:45,300
that the teams really needed to
think about and not again, not

510
00:32:45,300 --> 00:32:49,680
just on transmitting bits, over
wireless signals, but in fact,

511
00:32:49,710 --> 00:32:53,130
what information to which
teammate when to send this

512
00:32:53,130 --> 00:32:57,390
information, stuff like that.
And it turned out to be a really

513
00:32:57,390 --> 00:33:01,320
interesting dynamic to the way
this research progressed.

514
00:33:01,440 --> 00:33:03,750
Audrow Nash: Yeah. And it's
interesting, it kind of catches

515
00:33:03,750 --> 00:33:07,560
my attention, because you were
saying triathlon earlier. And I

516
00:33:07,560 --> 00:33:10,860
didn't quite understand. But now
it's like, there are mixed

517
00:33:10,860 --> 00:33:16,170
events that each mode of robot
may be best at. And so it's

518
00:33:16,170 --> 00:33:20,340
like, if one robot say a flying
robot is really good at going up

519
00:33:20,340 --> 00:33:23,580
elevator shafts, which might be
a very important thing in this

520
00:33:23,580 --> 00:33:27,450
competition, then that robot is
specialized for that, but it's

521
00:33:27,450 --> 00:33:29,730
not going to be good for
exploring in general, because it

522
00:33:29,730 --> 00:33:32,700
might kick up a bunch of dust.
And that could be prohibitive to

523
00:33:32,700 --> 00:33:36,000
its sensors, so then it can't do
its job. Well, this kind of

524
00:33:36,000 --> 00:33:39,450
thing. It's a triathlon, because
it has many events at which of

525
00:33:39,450 --> 00:33:44,130
which different modes of robot
may do better. Yes, yeah.

526
00:33:44,250 --> 00:33:48,030
Tim Chung: And yeah, exactly
right. And even calling it a

527
00:33:48,030 --> 00:33:53,220
triathlon kind of glosses over
the fact that our urban

528
00:33:53,220 --> 00:33:57,030
environments, when you think
urban, or your listeners are

529
00:33:57,030 --> 00:34:00,840
thinking about urban, they might
have many different mental

530
00:34:00,840 --> 00:34:03,990
pictures of what urban
underground might actually be.

531
00:34:03,990 --> 00:34:07,830
Some might be thinking about the
sewer system, or others might be

532
00:34:07,830 --> 00:34:11,730
thinking about the metro rail or
others are thinking about where

533
00:34:11,730 --> 00:34:15,810
they park their car, in the, you
know, in the basement lot. And

534
00:34:15,810 --> 00:34:20,820
so that, that even within a
single so called event, right,

535
00:34:21,240 --> 00:34:26,310
like, like running, it's like
running in a very different very

536
00:34:26,310 --> 00:34:28,260
courts and trying to figure out
how

537
00:34:28,260 --> 00:34:32,250
Audrow Nash: best suited you're
running. You're running. Yeah, I

538
00:34:32,250 --> 00:34:34,620
don't know, upstairs, you're
running. Like there's a ton of

539
00:34:34,620 --> 00:34:38,580
different environments. Exactly.
It's like a Tough Mudder. In a

540
00:34:38,580 --> 00:34:38,880
sense.

541
00:34:38,910 --> 00:34:41,850
Tim Chung: That's not that
sounds like a future DARPA

542
00:34:41,850 --> 00:34:43,410
program, right? Yeah.

543
00:34:44,370 --> 00:34:48,750
Audrow Nash: Gotcha. How did
you? How do you disseminate the

544
00:34:48,750 --> 00:34:51,690
results of this? Because I
imagine a lot of teams came up

545
00:34:51,690 --> 00:34:56,040
with clever solutions. There was
probably some push in terms of

546
00:34:56,040 --> 00:35:02,100
algorithms or ways of doing
things. How do they results get

547
00:35:02,130 --> 00:35:04,560
pushed and folded back into the
public.

548
00:35:05,190 --> 00:35:08,490
Tim Chung: Yeah. So that's one
of the things I'm probably most

549
00:35:08,490 --> 00:35:12,540
proud of from a kind of a
community sense is that the

550
00:35:12,540 --> 00:35:15,930
subsea challenge competitors,
you know, they are competitors,

551
00:35:15,930 --> 00:35:19,710
they're all vying for that top
prize. But in the end, they were

552
00:35:19,740 --> 00:35:23,340
all wreck, you know, recognizing
that this is a an opportunity as

553
00:35:23,340 --> 00:35:27,660
a community to leap ahead and
change that, you know, potential

554
00:35:27,660 --> 00:35:32,220
trajectory of, of this research
in robotics. And so many of

555
00:35:32,220 --> 00:35:36,360
these teams have gone to great
lengths to open source their

556
00:35:36,360 --> 00:35:43,530
code, to share data extensively.
In fact, one of the limitations

557
00:35:43,560 --> 00:35:47,520
of trying to, you know, with,
with our off the shelf, image

558
00:35:47,520 --> 00:35:50,610
classifier, so object
recognition, you can get off the

559
00:35:50,610 --> 00:35:52,800
shelf classifiers. But what do
you need, you need a lot of

560
00:35:52,800 --> 00:35:56,820
training data, Well, turns out
finding training data in

561
00:35:57,420 --> 00:36:00,750
underground environments and
various lighting out there for

562
00:36:00,750 --> 00:36:05,880
that, that's just like, you
know, I left it Yeah. And so,

563
00:36:06,600 --> 00:36:10,230
you know, finding that and
aggregating that, and even even

564
00:36:10,290 --> 00:36:14,910
collecting such data is also
very intensive, and time and

565
00:36:14,910 --> 00:36:18,750
labor. And so we found teams
actually sharing their datasets

566
00:36:18,780 --> 00:36:22,260
by collecting so they had this
kind of pool of data that they

567
00:36:22,260 --> 00:36:25,050
could train, and improve
collectively. And I think that

568
00:36:25,050 --> 00:36:30,300
was kind of just one small
example of how communal and

569
00:36:30,300 --> 00:36:33,480
collaborative that this
community ended up being. And so

570
00:36:33,480 --> 00:36:37,890
all of that to say, even amidst
helping each other, you know,

571
00:36:37,890 --> 00:36:41,040
whether it's lending, monitors,
and screwdrivers all the way to

572
00:36:41,040 --> 00:36:46,980
sharing open source code and
data, I'd say all of the members

573
00:36:46,980 --> 00:36:51,210
of the subsea community has
really embraced this idea of, of

574
00:36:51,210 --> 00:36:55,500
sharing it, and, and helping
this community grow. So you can

575
00:36:55,500 --> 00:36:59,460
go to their listing of
repositories they've shared, not

576
00:36:59,460 --> 00:37:03,540
only in their publications, but
also in their online videos.

577
00:37:04,260 --> 00:37:07,290
They've shared their research,
they've shared their code share

578
00:37:07,290 --> 00:37:11,580
their data, many of the
researchers have gone on to work

579
00:37:11,580 --> 00:37:15,990
on follow on projects that are
leveraging the technologies that

580
00:37:15,990 --> 00:37:19,350
they developed in sub T's, in
some case, working with former

581
00:37:19,350 --> 00:37:22,560
competitors, now, actual
collaborators on these projects.

582
00:37:22,560 --> 00:37:27,240
And so a lot of that has kind of
kind of made it out to the wild.

583
00:37:27,480 --> 00:37:31,620
And those are just the research
product projects. There's also a

584
00:37:31,620 --> 00:37:36,150
pretty big effort amongst these
teams to kind of take these

585
00:37:36,420 --> 00:37:40,350
pardons technologies, those that
have been tested in the crucible

586
00:37:40,350 --> 00:37:43,560
of the darkness of T challenge,
and turn them into commercial

587
00:37:43,560 --> 00:37:47,880
products. And so we've seen
startup companies spin out from

588
00:37:47,880 --> 00:37:51,480
these competitor teams,
productization, of even some of

589
00:37:51,480 --> 00:37:55,110
these components, technologies,
you can in fact, so go and buy

590
00:37:55,110 --> 00:37:58,770
these sensor integrated sensor
packages, that you can slap on

591
00:37:58,770 --> 00:38:03,330
the hood of your car, or carry
in your backpack, and get a

592
00:38:03,330 --> 00:38:09,060
nominal kind of imaging, mapping
capabilities. That's now as a

593
00:38:09,060 --> 00:38:12,240
product. And a lot of that
really came out from the SubT

594
00:38:12,240 --> 00:38:15,450
challenge. And so again, being
able to make that impact, not

595
00:38:15,450 --> 00:38:19,770
just the research community with
code, but in fact really turning

596
00:38:19,770 --> 00:38:24,000
us around and impacting the, you
know, safety community in the

597
00:38:24,000 --> 00:38:28,740
security community in the mining
industry, in the construction

598
00:38:28,740 --> 00:38:31,500
site industry. And you know, all
those kinds of folks that can

599
00:38:31,500 --> 00:38:35,790
really benefit from these, even
the component technologies have

600
00:38:35,790 --> 00:38:39,240
already started to see that
impact from from getting the

601
00:38:39,330 --> 00:38:42,600
research, but also the
technology out into the wild.

602
00:38:42,690 --> 00:38:45,330
It's been awesome. For sure.
Yeah, that is super cool.

603
00:38:45,990 --> 00:38:48,900
Audrow Nash: How did the so
another part of this that we

604
00:38:48,900 --> 00:38:52,920
haven't really talked about? Is
the simulation or the virtual

605
00:38:52,920 --> 00:38:55,860
competition? Would you tell me a
bit about this and how it

606
00:38:55,860 --> 00:38:56,550
relates?

607
00:38:57,180 --> 00:39:00,870
Tim Chung: Yeah, so we designed
the virtual competition of the

608
00:39:00,870 --> 00:39:05,880
SubT challenge to be an
opportunity for us to kind of

609
00:39:05,880 --> 00:39:10,350
explore some of these What if
cases, and that's one of the

610
00:39:10,380 --> 00:39:14,850
incredible values of using
virtual environments to study

611
00:39:14,940 --> 00:39:18,390
advanced technologies. And so
whereas in the system's

612
00:39:18,390 --> 00:39:23,280
competitions, we might take over
a coal mine or an unfinished

613
00:39:23,280 --> 00:39:28,140
nuclear power plant, you only
going to get one such course you

614
00:39:28,140 --> 00:39:31,590
know, one such environment. And
despite having a lot of

615
00:39:31,770 --> 00:39:34,800
variability within that, one
course, there might be a whole

616
00:39:34,800 --> 00:39:37,110
lot of other things we might
want to study in the virtual

617
00:39:37,110 --> 00:39:40,830
environment gave us those
opportunities to do that. To

618
00:39:40,830 --> 00:39:44,610
give you an example, you know,
if you wanted to, you could use

619
00:39:44,610 --> 00:39:48,720
in the virtual competition, we
enabled this ability to create

620
00:39:48,720 --> 00:39:53,160
your own world, you could
procedurally generate your

621
00:39:53,220 --> 00:39:56,940
tunnel like environments or
procedurally generate your own

622
00:39:56,940 --> 00:40:00,120
cave like environment, specify
some parameters it would be Have

623
00:40:00,120 --> 00:40:03,870
you long and narrow? Or, you
know, highly vertical? Or, you

624
00:40:03,870 --> 00:40:04,950
know, kind of

625
00:40:05,220 --> 00:40:06,480
Audrow Nash: parameterised?
Yeah.

626
00:40:06,990 --> 00:40:10,620
Tim Chung: Yeah. And and that
gives teams on competing in the

627
00:40:10,620 --> 00:40:14,160
virtual competition, this
opportunity to test against a

628
00:40:14,160 --> 00:40:18,690
wide range of worlds far greater
than one would find yet a single

629
00:40:18,690 --> 00:40:23,340
test site to kind of avoid
overfitting to the problem to

630
00:40:23,700 --> 00:40:26,940
avoid studying to that one
specific test. And so this

631
00:40:26,940 --> 00:40:30,990
virtual competition really kind
of embraced that we, instead of

632
00:40:32,430 --> 00:40:35,820
testing against one virtual
world, in many cases, we're

633
00:40:35,820 --> 00:40:39,270
testing against half a dozen
upwards of eight worlds, in some

634
00:40:39,270 --> 00:40:44,730
cases to see how well a given
fixed solution would work

635
00:40:44,940 --> 00:40:49,110
against a varying set of
environments. So the way this

636
00:40:49,110 --> 00:40:55,530
works, we invested heavily in a
next gen simulation capability,

637
00:40:55,530 --> 00:40:59,400
ignition, Zeebo, our SubT
simulator. And that's the

638
00:40:59,400 --> 00:40:59,910
simulation

639
00:40:59,910 --> 00:41:02,610
Audrow Nash: environment made by
open robotics where I'm working.

640
00:41:02,610 --> 00:41:03,030
That's right.

641
00:41:03,480 --> 00:41:06,960
Tim Chung: Yeah, that's right.
So open robotics, provided that

642
00:41:06,960 --> 00:41:11,070
virtual environment capability
and advanced the state of

643
00:41:11,070 --> 00:41:15,360
robotics simulation, I'd say.
And in that simulator, we're

644
00:41:15,390 --> 00:41:18,600
able to run that in the cloud
using our cloud based simulation

645
00:41:18,630 --> 00:41:23,130
infrastructure that we also
developed. And so now teams are

646
00:41:23,160 --> 00:41:28,140
able to develop containerized
solutions. So bundle their

647
00:41:28,140 --> 00:41:31,980
robotic software, if you will,
and upload it to the cloud and

648
00:41:32,010 --> 00:41:35,190
run their software against our
simulation environments, not

649
00:41:35,190 --> 00:41:38,430
knowing what the simulated world
would look like is, of course,

650
00:41:38,640 --> 00:41:41,610
we are managing all that in the
cloud.

651
00:41:41,760 --> 00:41:45,180
Audrow Nash: Yeah. You have your
tests, basically, which this is

652
00:41:45,180 --> 00:41:48,330
how well does it do in the cloud
environment? Okay.

653
00:41:48,930 --> 00:41:51,960
Tim Chung: Yeah. And so the
other fun part of this is we

654
00:41:51,960 --> 00:41:55,470
constructed what we call the sub
t tech repo. Think of this as

655
00:41:55,470 --> 00:41:59,550
like your, you know, your
storefront, your web based

656
00:41:59,550 --> 00:42:04,110
storefront, you can go and add
robots to cart. And what I'm

657
00:42:04,110 --> 00:42:08,130
doing here is I'll give each
competitor 1000 SubT credits.

658
00:42:08,310 --> 00:42:11,880
Yep. And you go to this
storefront, this tech repo, and

659
00:42:11,880 --> 00:42:15,810
each of these robots that we
have virtual models of cost you

660
00:42:15,810 --> 00:42:19,890
some number of sub D credits,
yeah, up to that kind of salary

661
00:42:19,890 --> 00:42:24,840
cap tees of, you know, NAC
league analogy here, you get to

662
00:42:24,840 --> 00:42:28,740
build, mix and match the types
of robots that you want to add

663
00:42:28,740 --> 00:42:32,490
to your team subject to this
budget of 1000s of t credits.

664
00:42:33,090 --> 00:42:36,600
And these different robots are
all different sorts, they have

665
00:42:36,600 --> 00:42:40,020
different sensor payloads, they
might have, again, wheeled or

666
00:42:40,020 --> 00:42:43,020
flying or kind of all the
different different modalities

667
00:42:43,020 --> 00:42:48,060
you might explore. And so it
boils down down to these teams,

668
00:42:48,300 --> 00:42:51,930
thinking about where they want
to spend their 70 credits, and

669
00:42:51,930 --> 00:42:55,830
the number of robots they want
to add to their team. And then

670
00:42:55,830 --> 00:42:59,730
designing their autonomy and
perception solutions to best

671
00:42:59,730 --> 00:43:03,960
match the virtual robot teams
that they're constructing. And

672
00:43:03,960 --> 00:43:10,410
so in this way, you really get
to kind of explore what are the

673
00:43:10,410 --> 00:43:15,090
best composition of teams? What
if, given this cost constraint?

674
00:43:15,120 --> 00:43:17,910
He asked earlier? If they're,
you know, budget limitations for

675
00:43:17,910 --> 00:43:21,120
Audrow Nash: that? Yeah, cuz I
heard about the virtual, the

676
00:43:21,120 --> 00:43:25,230
budget. Yeah, the virtual one.
Yeah. So I was wondering if

677
00:43:25,230 --> 00:43:27,540
there was something similar in
the hardware and how that would

678
00:43:27,540 --> 00:43:29,370
work, because it's quite
complex, if they're building

679
00:43:29,370 --> 00:43:31,200
hardware was my thing. Right,

680
00:43:31,200 --> 00:43:35,790
Tim Chung: right. And so we
wanted to abstract away some of

681
00:43:35,790 --> 00:43:41,220
the, you know, constraints of
having to embody some of the

682
00:43:41,220 --> 00:43:45,630
software based side of things,
and free up that develop, you

683
00:43:45,630 --> 00:43:49,650
know, the development to explore
some of these other scenarios.

684
00:43:49,890 --> 00:43:52,860
The other really big difference
between the systems and the

685
00:43:52,860 --> 00:43:56,400
virtual competition, was that in
the system's competition, these

686
00:43:56,400 --> 00:44:00,690
robots have to report out where
these artifacts are located. And

687
00:44:00,720 --> 00:44:06,240
they can coordinate with a human
supervisor, a single human, to

688
00:44:06,240 --> 00:44:11,370
act as a teammate amongst a
fleet of robots. And that human

689
00:44:11,370 --> 00:44:15,810
supervisor, is typically the one
that's forwarding the artifact

690
00:44:15,810 --> 00:44:19,890
reports to DARPA to get scored.
And so you now have this

691
00:44:19,890 --> 00:44:23,100
partnership, you can have the
human supervisor, kind of

692
00:44:23,340 --> 00:44:26,250
interact with the teams of
robots and have the robots go

693
00:44:26,250 --> 00:44:30,540
take a closer look at things or
maybe show a an image or review

694
00:44:30,570 --> 00:44:34,620
and high man's like, go over
there probably or whatever.

695
00:44:34,650 --> 00:44:38,400
That's right. Yeah. And they
did. And that was really great

696
00:44:38,400 --> 00:44:42,570
partnership. But in the virtual
competition. These robots are

697
00:44:42,570 --> 00:44:45,840
fully autonomous. You have
uploaded your software to the

698
00:44:45,840 --> 00:44:51,720
cloud. It's one set of software
that has to get run against all

699
00:44:51,720 --> 00:44:54,810
of these different types of
virtual environments. And

700
00:44:54,810 --> 00:44:57,810
there's no human back and forth
and so really being able to

701
00:44:57,810 --> 00:45:02,280
explore complete autonomy of The
solution where you don't get the

702
00:45:02,280 --> 00:45:06,090
chance to, you know, back your
robot out manually, even if you

703
00:45:06,570 --> 00:45:11,520
do it, you know, that gave us
again, studying what this what

704
00:45:11,520 --> 00:45:15,480
if scenario in the future where
you didn't have the luxury of

705
00:45:15,480 --> 00:45:18,990
having a human supervisor, or
the levels of autonomy got to

706
00:45:18,990 --> 00:45:22,530
the point where you could deploy
these robots to good effect in

707
00:45:22,530 --> 00:45:26,280
these really diverse
environments. So now we have,

708
00:45:26,790 --> 00:45:31,050
you know, brackets, you have the
system competition, highlighting

709
00:45:31,050 --> 00:45:35,820
where we are, we push the state
of the art in applied realized

710
00:45:35,880 --> 00:45:40,230
technologies. And we also have
the the solution space of what

711
00:45:40,230 --> 00:45:43,980
can be done in the completely
autonomous regime where it's

712
00:45:44,010 --> 00:45:47,760
software only. And now we can
quantifiably say how far are we

713
00:45:47,940 --> 00:45:53,010
from complete autonomy,
autonomous solutions, or we can

714
00:45:53,010 --> 00:45:57,780
quantify it to some degree, the
value of the human teammate. And

715
00:45:57,780 --> 00:46:02,400
so the virtual consecration is
benchmarking ability. Right now

716
00:46:02,400 --> 00:46:07,200
we have comparative analysis
between systems and virtual. And

717
00:46:07,200 --> 00:46:10,410
we did this all throughout the
whole competition, where we had

718
00:46:10,680 --> 00:46:14,730
tunnel circuit in both systems,
virtual, urban in both systems

719
00:46:14,730 --> 00:46:19,200
and virtual in cave, we couldn't
do the system's competition. But

720
00:46:19,200 --> 00:46:22,860
man did the virtual competition
really shine through in the in

721
00:46:22,860 --> 00:46:27,120
the face of the global lockdown,
where now we had teams flocking

722
00:46:27,120 --> 00:46:30,000
to the virtual competition,
because they still were thirsty

723
00:46:30,000 --> 00:46:33,450
to develop their technologies.
And so we had numerous systems,

724
00:46:33,450 --> 00:46:37,590
competitors, crossover into the
virtual competition, in fact,

725
00:46:37,830 --> 00:46:42,450
and carry that through to really
good effect. So it was a lot of

726
00:46:42,450 --> 00:46:47,520
fun on the virtual side to kind
of help us imagine what future

727
00:46:47,520 --> 00:46:51,480
technologies could be while also
being closely coupled to where

728
00:46:52,020 --> 00:46:54,630
the current state of the art of
the technology was, as well.

729
00:46:54,840 --> 00:46:58,290
Audrow Nash: Yeah, I find it to
me the most exciting part about

730
00:46:58,290 --> 00:47:02,970
that, as you said, with the
benchmarking, it's that so like,

731
00:47:02,970 --> 00:47:05,790
I'm thinking about, like the
advancement of technology and

732
00:47:05,790 --> 00:47:09,330
say, like the computer image or
computer vision space, like we

733
00:47:09,330 --> 00:47:12,780
established image net, and it's
a benchmark in a sense, so that

734
00:47:12,780 --> 00:47:17,070
people can more fairly compare
what they're how their algorithm

735
00:47:17,070 --> 00:47:20,940
performs. And so what you see is
people keep trying it, and then

736
00:47:21,000 --> 00:47:25,770
the performance keeps going up.
And robotics, especially on like

737
00:47:25,770 --> 00:47:31,050
a behavioral level, has not
really anything similar to my

738
00:47:31,050 --> 00:47:34,350
knowledge, except for what
you're describing here, which is

739
00:47:34,350 --> 00:47:37,410
really cool, where you can have
some sort of, so you have your

740
00:47:37,410 --> 00:47:42,480
challenge. And you have a way
that people can try by basically

741
00:47:42,480 --> 00:47:46,860
upload their algorithm to it.
And it runs it given their

742
00:47:46,860 --> 00:47:50,640
robots and and all of this, to
see the performance, and you can

743
00:47:50,640 --> 00:47:54,060
see the performance over time.
That's really, really

744
00:47:54,060 --> 00:47:56,040
interesting and very exciting to
me.

745
00:47:56,760 --> 00:48:00,330
Tim Chung: Yeah, totally with
you. I think this idea of being

746
00:48:00,330 --> 00:48:05,340
able to set those benchmarks
allows for us as a community to

747
00:48:05,520 --> 00:48:08,730
see how well we're doing right
and see that progress. And it's

748
00:48:08,730 --> 00:48:11,910
exciting to see that progress.
And to be able to do that in

749
00:48:11,910 --> 00:48:15,450
near real time with this side by
side systems and virtual

750
00:48:15,450 --> 00:48:19,890
competition, I think, you know,
was really, really cool. And

751
00:48:19,890 --> 00:48:24,210
I'll highlight two more things.
One is we took systems robots,

752
00:48:24,210 --> 00:48:28,980
so robots from the system's
competition, and went ahead and

753
00:48:29,010 --> 00:48:34,950
scan them, generate 3d models of
them. So controller code that

754
00:48:34,980 --> 00:48:40,170
were contributed by the
competitor teams, and validated

755
00:48:40,170 --> 00:48:43,590
it against their sensor spec
sheets and battery durations.

756
00:48:43,590 --> 00:48:47,520
And you know, quite laboriously.
But now, in that SubT tech

757
00:48:47,520 --> 00:48:53,700
repo, this is these virtual
robot models that are digital

758
00:48:53,700 --> 00:48:58,140
twins of their systems, real
world robots. Now live in the

759
00:48:58,140 --> 00:49:00,960
tech repo. And now virtual
competitors, they might not have

760
00:49:00,960 --> 00:49:05,190
had the resources to build their
own robots, but they can

761
00:49:05,190 --> 00:49:09,600
actually go and use a systems
embedded robot models in their

762
00:49:09,630 --> 00:49:10,470
virtual fleet.

763
00:49:10,680 --> 00:49:12,570
Audrow Nash: Yeah, and that's
very cool. If anyone had an

764
00:49:12,570 --> 00:49:16,950
especially good idea for like,
how to, like a morphologie.

765
00:49:16,950 --> 00:49:20,190
That's really, really good. And
now, I can play with that more

766
00:49:20,190 --> 00:49:22,470
as a community. Right off. Yeah,

767
00:49:22,470 --> 00:49:25,110
Tim Chung: exactly. Right. So
I'm going to pull the fantasy

768
00:49:25,110 --> 00:49:29,700
league analogy. Again, you know,
if it's a virtual competition,

769
00:49:30,000 --> 00:49:33,450
gave you the flexibility to mix
and match across the league.

770
00:49:33,870 --> 00:49:36,990
With a systems competition was
kind of like the, you know, like

771
00:49:37,020 --> 00:49:40,860
the NFL, right? You kind of have
to pick your roster at the

772
00:49:40,860 --> 00:49:43,140
beginning of the season, you
kind of play the players that

773
00:49:43,140 --> 00:49:46,650
are on your roster, because it
really is too difficult to go

774
00:49:46,650 --> 00:49:50,280
back and do clean sheet design
after each one of these

775
00:49:50,550 --> 00:49:55,350
competition events to start from
scratch, so you're kind of

776
00:49:55,710 --> 00:49:59,100
trying to do that systems
thinking for the systems teams

777
00:49:59,190 --> 00:50:04,320
holistically. But man in virtual
you get to mix and match. Play

778
00:50:04,320 --> 00:50:08,880
around, see, if you want to be
an all ground team or an all air

779
00:50:08,880 --> 00:50:12,690
team or, you know, beg and
borrow from different, try

780
00:50:12,690 --> 00:50:16,110
different styles of robots, you
can do that. And many of the

781
00:50:16,140 --> 00:50:19,650
competitors were able to do that
exploration. And there's still,

782
00:50:19,650 --> 00:50:22,860
of course, significant
exploration left to do. And then

783
00:50:22,920 --> 00:50:25,770
also highlight that there's a
virtual testbed, the SubT

784
00:50:25,770 --> 00:50:29,730
virtual testbed, which
encompasses the simulator, and

785
00:50:29,730 --> 00:50:33,180
the tech repo, all of that is
publicly available and open

786
00:50:33,180 --> 00:50:37,710
source. And so, okay, yeah, you
can still go, despite the

787
00:50:37,710 --> 00:50:42,840
competition being over, we know
that the value to the community

788
00:50:43,560 --> 00:50:50,460
is so important that DARPA is
keeping up the testbed open for

789
00:50:50,460 --> 00:50:54,300
a period to come, you can still
go to SubT challenge that world

790
00:50:54,540 --> 00:51:00,000
and learn how to how to develop
a SubT solution and upload it

791
00:51:00,000 --> 00:51:06,300
to the cloud and run it, you
know, and get free free reps in

792
00:51:06,300 --> 00:51:09,690
for Yeah. For your simulate, you
know, your simulation solutions.

793
00:51:09,690 --> 00:51:13,740
And so I think, yeah, the
availability is another

794
00:51:13,740 --> 00:51:18,540
testament to, you know, how we
really cherish the the community

795
00:51:18,540 --> 00:51:21,600
really are interested in
fostering the growth of the

796
00:51:21,600 --> 00:51:23,820
community, because that's at the
end of the day, what's going to

797
00:51:23,820 --> 00:51:27,720
turn around and develop those
technologies that are, you know,

798
00:51:28,050 --> 00:51:30,990
first responders and warfighters
and others are going to be able

799
00:51:30,990 --> 00:51:34,350
to make use of or precisely the
technologies that these people

800
00:51:34,470 --> 00:51:36,810
that the community develops and
matures?

801
00:51:37,470 --> 00:51:40,770
Audrow Nash: Yes, for sure. Do
you imagine that the the

802
00:51:40,770 --> 00:51:45,180
framework that you guys have
developed which hosts this the

803
00:51:45,180 --> 00:51:49,080
high level testing and
benchmarking of this challenge

804
00:51:49,290 --> 00:51:54,150
to imagine that this can be
applied to other different like,

805
00:51:54,150 --> 00:51:56,070
basically, the whole
infrastructure that you've

806
00:51:56,070 --> 00:51:59,430
built? Do you think it can be
applied to other robotics

807
00:51:59,430 --> 00:52:02,430
challenges, say robots like
moving around in hospital and be

808
00:52:02,430 --> 00:52:06,240
inefficient or any, any rope,
any tasks that robots might be

809
00:52:06,240 --> 00:52:11,190
very good for? To have this
provide like a standard

810
00:52:11,190 --> 00:52:15,180
benchmark, so you can look at it
over time, and compare more

811
00:52:15,180 --> 00:52:17,730
fairly, the algorithms and
approaches?

812
00:52:18,240 --> 00:52:23,190
Tim Chung: Yeah, hands down. I'm
a, I'm a believer in this type

813
00:52:23,190 --> 00:52:28,170
of a model of being able to not
only test at scale, but also

814
00:52:28,680 --> 00:52:33,450
open up the innovation space, by
lowering the barriers to entry.

815
00:52:33,450 --> 00:52:37,260
And simulation is a great way,
you know, to avoid having to lay

816
00:52:37,260 --> 00:52:41,010
out capital expenses to build a
fleet of robot and oh, by the

817
00:52:41,010 --> 00:52:45,330
way, have access to a hospital
or the underground mines or the

818
00:52:45,330 --> 00:52:50,700
moon or wherever your setting
might be. So absolutely think

819
00:52:50,700 --> 00:52:55,620
this back end infrastructure
absolutely critical, as a, as a

820
00:52:55,650 --> 00:53:01,200
method as a set of tools to, to
help out. And in fact, I'm

821
00:53:01,530 --> 00:53:05,880
pleased to say that, in fact, we
are seeing this precise

822
00:53:05,880 --> 00:53:09,180
infrastructure being used in
other robotics development

823
00:53:09,810 --> 00:53:15,660
settings already. So the impact
is already near and dear. And

824
00:53:16,680 --> 00:53:19,980
stay tuned to see some of those
competition events making use of

825
00:53:21,090 --> 00:53:22,290
this kind of infrastructure.

826
00:53:22,650 --> 00:53:24,180
Audrow Nash: And is that
infrastructure? Is that in the

827
00:53:24,180 --> 00:53:27,480
SubT tech repo that you've been
mentioning? Or is it Yeah,

828
00:53:27,480 --> 00:53:27,810
somewhere

829
00:53:27,810 --> 00:53:32,730
Tim Chung: else? Yeah. Yeah. So
the tech repo is the the source

830
00:53:32,730 --> 00:53:36,240
code, if you will, of all, the
majority of everything we talked

831
00:53:36,240 --> 00:53:41,970
about is already available, you
can go to get a, I believe,

832
00:53:42,030 --> 00:53:47,850
under OSR. F, and see the SubT
project there. You know, nearly

833
00:53:47,850 --> 00:53:50,550
everything that we've talked
about is already located there.

834
00:53:50,640 --> 00:53:55,380
So there are a handful of
resources under github.com/sub T

835
00:53:55,380 --> 00:53:58,140
challenge, and you'll be able to
find a number of resources there

836
00:53:58,140 --> 00:53:58,650
as well.

837
00:53:59,880 --> 00:54:03,180
Audrow Nash: Awesome. I didn't
know because I haven't. The sub

838
00:54:03,180 --> 00:54:06,000
T has never been one of my
projects. So I'm not that aware

839
00:54:06,000 --> 00:54:08,190
of what where everything lives
in this kind of thing. It's

840
00:54:08,190 --> 00:54:11,520
awesome to hear that it already
is all online and people can

841
00:54:11,520 --> 00:54:14,790
access it. And that people are
also using this infrastructure

842
00:54:14,790 --> 00:54:17,610
to benchmark other tests in
simulation. I think that's

843
00:54:17,610 --> 00:54:18,600
really, really exciting.

844
00:54:19,110 --> 00:54:23,550
Tim Chung: Yeah, it's gonna be
great when kind of the impact to

845
00:54:23,610 --> 00:54:26,130
not just the subterranean
robotics community, which

846
00:54:26,130 --> 00:54:31,680
admittedly is just one sliver of
the community. But being able to

847
00:54:31,680 --> 00:54:35,760
open this up, you suggested kind
of hospital environments or

848
00:54:36,060 --> 00:54:40,770
maritime settings or other types
of environments where we really

849
00:54:40,800 --> 00:54:45,990
want to explore more broadly, I
do think that, you know, this

850
00:54:45,990 --> 00:54:49,740
investment will pay significant
dividends for many, many years

851
00:54:49,740 --> 00:54:51,240
to come for the robot's
community.

852
00:54:51,480 --> 00:54:55,920
Audrow Nash: Thank you, right.
Let's see. So we are running out

853
00:54:55,920 --> 00:54:58,320
of time, and by that I mean,
like 20 more minutes or so and I

854
00:54:58,320 --> 00:55:02,010
have a lot of things that I want
to talk About in this, but you

855
00:55:02,040 --> 00:55:06,300
you mentioned the final events
and that we do you'd like to

856
00:55:06,300 --> 00:55:09,090
talk about that and kind of how
you fabricated this big

857
00:55:09,090 --> 00:55:11,250
environment? Would you tell me a
little bit more?

858
00:55:11,820 --> 00:55:16,020
Tim Chung: Yeah. So, you know, I
think one of the visions for the

859
00:55:16,020 --> 00:55:21,480
subsea challenge from the get go
was to have teams of robots,

860
00:55:21,750 --> 00:55:28,710
that and, you know, unhindered
by the fact that you might face

861
00:55:28,710 --> 00:55:31,320
all three of these environments.
Now, the challenge, and I've

862
00:55:32,040 --> 00:55:34,950
crossed crisscross the country
to try to find a place that has

863
00:55:35,220 --> 00:55:39,990
tunnels, urban caves, all next
to one another, all co located

864
00:55:39,990 --> 00:55:43,320
all connected in a meaningful
way, if any of your listeners

865
00:55:43,320 --> 00:55:49,830
have mine, please, please let us
know. So instead, as DARPA

866
00:55:50,220 --> 00:55:54,870
intends to do is, you know,
because we are always seeking

867
00:55:54,870 --> 00:55:59,760
those breakthroughs. We
transformed a limestone cavern

868
00:56:00,030 --> 00:56:04,920
and built a one of a kind
course, kind of think, almost

869
00:56:04,920 --> 00:56:10,110
like Hollywood set design with
the level of realism that, you

870
00:56:10,110 --> 00:56:14,370
know, is, you know, very
compelling for both human and

871
00:56:14,370 --> 00:56:19,980
robot, to the degree where we're
getting inspiration from caves

872
00:56:19,980 --> 00:56:24,240
and mines and sites that we've
seen, we replicated New York

873
00:56:24,240 --> 00:56:32,190
City metro station, all the way
to steam tunnels, and old rustic

874
00:56:32,190 --> 00:56:38,760
abandoned mines to large caverns
that are either show caverns

875
00:56:38,790 --> 00:56:42,030
that you might go to as a
tourist to the, you know, the

876
00:56:42,030 --> 00:56:46,140
raw caverns or the untouched
ones that have a very different

877
00:56:46,140 --> 00:56:50,400
feel as well. And so we were
able to take all of those

878
00:56:50,640 --> 00:56:54,840
different sites that we had been
to or found to be really

879
00:56:54,840 --> 00:56:59,100
compelling, and fabricated, we
built that we had warehouse

880
00:56:59,100 --> 00:57:02,820
structures connected to a metro
station connected to a

881
00:57:03,090 --> 00:57:06,510
featureless hallway. And that's
just a fraction of the urban

882
00:57:06,510 --> 00:57:10,830
setting, we had small places
where you had to crawl on your

883
00:57:10,830 --> 00:57:15,810
hands and knees often over the
different segments and places

884
00:57:15,810 --> 00:57:20,280
where I'm certain many dozens of
times, if I hadn't had a helmet

885
00:57:20,280 --> 00:57:25,380
on, if you walk away with a lot
of bumps and bruises, you know,

886
00:57:25,380 --> 00:57:30,060
just trying to build a place
where we can test and push the

887
00:57:30,060 --> 00:57:33,150
boundaries of autonomy,
mobility, networking, and

888
00:57:33,150 --> 00:57:37,890
perception was a phenomenal
experience to, to do that. And

889
00:57:37,920 --> 00:57:43,710
in having to build that course,
going back to where it all

890
00:57:43,710 --> 00:57:47,820
started with all of our
stakeholders, all the end users

891
00:57:47,820 --> 00:57:52,620
of people who this was going to
who we would have to have them

892
00:57:52,620 --> 00:57:56,970
believe that these environments
were realistic enough, right? So

893
00:57:56,970 --> 00:58:01,020
we did go back and say, Hey,
what are the pieces that you

894
00:58:01,050 --> 00:58:05,700
need to see, to be able to say
that you believe this technology

895
00:58:06,660 --> 00:58:10,830
is relevant to you. And we were
able to do that to good effect.

896
00:58:11,040 --> 00:58:14,970
And whether and since we were
building it ourselves, we in

897
00:58:14,970 --> 00:58:18,510
fact, had different segments of
the course take on different

898
00:58:18,510 --> 00:58:22,530
personalities. And so in total,
even though we had three

899
00:58:22,530 --> 00:58:26,940
subdomains, the tunnel urban
cave, we in fact had, like 60

900
00:58:26,940 --> 00:58:30,450
different segments, kind of like
going on a on almost a Disney

901
00:58:30,450 --> 00:58:36,750
ride through different parts of
the subterranean world, is what

902
00:58:36,750 --> 00:58:41,880
we were able to pose to these
robots now facing all of that

903
00:58:41,880 --> 00:58:47,280
difficulty, really, really,
really allowed for testing the

904
00:58:47,280 --> 00:58:52,710
diversity. And we went to great
lengths, I'd say to hone that

905
00:58:52,710 --> 00:58:56,250
realism. We talked earlier about
communications, and networking.

906
00:58:56,580 --> 00:59:00,450
But I'll tell you, we went
through various types of RF

907
00:59:00,450 --> 00:59:04,980
shielding and pains and other
things baked into the walls of,

908
00:59:05,580 --> 00:59:11,670
of, of this course, to be able
to replicate the kind of the RF

909
00:59:12,030 --> 00:59:16,950
propagation in underground
environments. And so I'm pleased

910
00:59:16,950 --> 00:59:21,420
to say that I think we did a
good job of trying to replicate

911
00:59:21,420 --> 00:59:27,030
not just the look and feel but
the RF, the radio frequency, the

912
00:59:27,030 --> 00:59:30,960
wireless transmission all the
way to you know, the little

913
00:59:30,960 --> 00:59:37,470
things like amount of water
moistures, slick surfaces,

914
00:59:38,220 --> 00:59:41,550
Rubble, gravel, you know, all
that kind of stuff. Really,

915
00:59:41,550 --> 00:59:46,200
really proud of this one of a
kind opportunity to, you know,

916
00:59:46,200 --> 00:59:49,680
raise the bar for the robotics
community writ large, for sure.

917
00:59:49,710 --> 00:59:49,980
Yeah, it

918
00:59:49,980 --> 00:59:52,050
Audrow Nash: sounds like such a
great test environment that's so

919
00:59:52,050 --> 00:59:54,480
funny that you like the
shielding and things so you

920
00:59:54,480 --> 00:59:56,910
could get the RF properties you
wanted in the environment.

921
00:59:57,300 --> 01:00:01,560
Amazing. So I assume that will
be used In future competitions

922
01:00:01,560 --> 01:00:06,480
or how so actually How did it
go? In the finals? You use this

923
01:00:06,480 --> 01:00:09,330
environment, correct? Yeah,
that's right. How did it go? How

924
01:00:09,330 --> 01:00:09,780
was it?

925
01:00:10,290 --> 01:00:13,470
Tim Chung: Yeah, I think I think
it was really great. I think,

926
01:00:14,520 --> 01:00:18,330
you know, one of the benchmarks
for me as the DARPA Program

927
01:00:18,330 --> 01:00:23,760
Manager here is, was it hard
enough, right? Does it push the

928
01:00:23,760 --> 01:00:27,420
boundaries, it needs to be hard,
so that not everyone gets 100.

929
01:00:28,380 --> 01:00:32,010
But we don't want it to be
impossibly hard, because then

930
01:00:32,010 --> 01:00:34,260
we're not going to know where
the boundaries are, where the

931
01:00:34,260 --> 01:00:38,490
envelope is. And so being able
to design this course, in a way

932
01:00:38,790 --> 01:00:43,620
that had the level of difficulty
variants, that allowed some

933
01:00:43,680 --> 01:00:48,270
pieces of technology to really
be showcased, and others, you

934
01:00:48,270 --> 01:00:51,300
know, you found where their
limitations were, you know, that

935
01:00:51,300 --> 01:00:56,370
was really great about the the
environment. I'll say, also,

936
01:00:56,370 --> 01:00:58,410
from the kind of the
government's perspective, the

937
01:00:58,410 --> 01:01:02,610
DARPA team, we are in the
command post, as you know, this

938
01:01:02,610 --> 01:01:06,450
is not something that our
competitors get a really good

939
01:01:06,450 --> 01:01:10,410
glimpse of, because you're all
behind the scenes. But in our

940
01:01:10,410 --> 01:01:14,520
command post, we had
instrumented this course, with

941
01:01:14,520 --> 01:01:18,150
everything from cameras, of
course, so we could see where

942
01:01:18,150 --> 01:01:22,080
robots might be. But we had
things like motion detectors, we

943
01:01:22,080 --> 01:01:26,280
had triggers, all of our
artifacts were instrumented so

944
01:01:26,280 --> 01:01:29,970
we could make sure that they're
consistently Halloween house.

945
01:01:29,970 --> 01:01:36,120
Yeah. Yeah, that's right. Yeah.
And so we could orchestrate kind

946
01:01:36,120 --> 01:01:39,570
of like movie style, movies and
style, like, and so we had

947
01:01:39,570 --> 01:01:44,070
dynamic obstacles. And so these
would be triggered when robots

948
01:01:44,100 --> 01:01:48,840
entered a particular area, and
then section of the course would

949
01:01:48,840 --> 01:01:51,900
collapse behind them now
blocking their way home, they

950
01:01:51,900 --> 01:01:56,670
would need to find a new way
they would not be able to follow

951
01:01:56,670 --> 01:02:00,330
their that's amazing, around the
home. And so that was built into

952
01:02:00,330 --> 01:02:04,830
the course as well. actuated,
from, you know, from these types

953
01:02:04,830 --> 01:02:08,550
of signals that we had
instrumented this course with,

954
01:02:08,760 --> 01:02:14,010
and so does a Hollywood set up.
Yeah, in many ways it was. And

955
01:02:14,010 --> 01:02:17,550
it was always done with a
technology objective in mind.

956
01:02:17,580 --> 01:02:22,290
And that's the cool part about
it is and showing this collapse

957
01:02:22,320 --> 01:02:25,440
was something that was very
relevant to many of our, for

958
01:02:25,440 --> 01:02:28,770
example, mine rescue personnel,
because that's one of the hard

959
01:02:28,770 --> 01:02:34,710
things is debris collapses, or
debris, integrity checks of the

960
01:02:34,710 --> 01:02:39,660
environment. And so collapsing,
part of this was near and dear

961
01:02:39,660 --> 01:02:43,290
to them. But for us, we are
interested in testing the

962
01:02:43,290 --> 01:02:45,600
autonomy, the ability to
recognize that your map has

963
01:02:45,600 --> 01:02:49,830
changed. And then your way home
is no longer viable, you got to

964
01:02:49,830 --> 01:02:54,180
find another way home. And so
there was a really fun way, a

965
01:02:54,180 --> 01:02:58,830
really tight way of coupling,
the operational kind of need and

966
01:02:58,830 --> 01:03:02,280
insight with the technology that
we were trying to push in this

967
01:03:02,280 --> 01:03:07,470
one, of course, was explicitly
designed, you know, from from

968
01:03:07,650 --> 01:03:12,420
CAD drawing all the way to
graphic artists painting inside

969
01:03:12,420 --> 01:03:16,560
the course. So to be able to
deliver that type of tight

970
01:03:16,560 --> 01:03:19,740
coupling between operational and
technical objectives.

971
01:03:20,760 --> 01:03:23,250
Audrow Nash: Gosh, that sounds
amazing. Do you have is there

972
01:03:23,250 --> 01:03:26,730
like a YouTube of like a
walkthrough of this course? I

973
01:03:26,730 --> 01:03:27,420
haven't seen it.

974
01:03:27,689 --> 01:03:30,959
Tim Chung: Yeah, there are. So
you can go to DARPA TV, that's

975
01:03:30,959 --> 01:03:35,819
the YouTube channel. And all of
our videos have been placed

976
01:03:35,819 --> 01:03:41,309
there. There are walkthroughs of
every course that we DARPA has

977
01:03:41,669 --> 01:03:45,209
transformed into a competition
course. So you can go and do a

978
01:03:45,209 --> 01:03:50,309
walkthrough of Hardrock, like a
gold mine, coal mine, that

979
01:03:50,609 --> 01:03:54,479
nuclear power plant, and then of
course, our grand finale, final

980
01:03:54,479 --> 01:03:58,289
event course there multiple
walkthrough videos. And then

981
01:03:58,319 --> 01:04:01,559
this might be a good opportunity
to share that DARPA also

982
01:04:01,619 --> 01:04:07,199
collected very high resolution,
high accuracy datasets of these

983
01:04:07,199 --> 01:04:11,639
environments, to the point where
we could dance, of course,

984
01:04:11,639 --> 01:04:16,169
because we're also scoring the
artifact position reports. So we

985
01:04:16,169 --> 01:04:20,609
were down at the, you know,
millimeter scale of accuracy.

986
01:04:20,729 --> 01:04:23,549
And we've released that as a
public data set as well. So if

987
01:04:23,549 --> 01:04:28,619
you were unable to test in the
final event course, well, you

988
01:04:28,619 --> 01:04:32,249
can actually go to the SubT
tech repo on the virtual side

989
01:04:32,579 --> 01:04:39,149
and get a mesh 3d rendering a 3d
version of the final event world

990
01:04:39,389 --> 01:04:44,819
and, and see what it looks like
for your robots as well. And fun

991
01:04:44,819 --> 01:04:49,259
fact here, one of the final
event, virtual competition

992
01:04:49,259 --> 01:04:54,089
worlds was in fact, a 3d virtual
world of the system's

993
01:04:54,089 --> 01:04:58,619
competition final event world.
So kind of bringing everything

994
01:04:58,739 --> 01:05:02,969
full circle Now you get this
chance to test how the system's

995
01:05:02,969 --> 01:05:06,749
competitors were testing in
this, again, unique one time

996
01:05:06,749 --> 01:05:09,749
world. And at the same time,
we're running virtual

997
01:05:09,749 --> 01:05:14,219
competitors do the same as that
course, having developed this 3d

998
01:05:14,219 --> 01:05:18,689
model of the course, with all
the, all the pain, the same pain

999
01:05:18,689 --> 01:05:22,589
points as the real robots had to
face, but not in a virtual

1000
01:05:22,589 --> 01:05:26,639
domain. And so all of that's
been publicly released, we can

1001
01:05:26,669 --> 01:05:28,919
see the video, but if you want
the point cloud, you can

1002
01:05:28,919 --> 01:05:32,429
download the Point Cloud
yourself and work with it, or

1003
01:05:32,429 --> 01:05:37,589
the mesh, 3d virtual mesh file
is available. So you can, you

1004
01:05:37,589 --> 01:05:40,649
know, do put in your own
simulators, if you wish, or

1005
01:05:40,649 --> 01:05:44,129
loaded up in the SubT simulator
and, and drive some robots

1006
01:05:44,129 --> 01:05:44,429
around.

1007
01:05:44,939 --> 01:05:47,789
Audrow Nash: So cool. What a
great public offering. That's so

1008
01:05:47,819 --> 01:05:51,629
it's so neat, that people have
access to all of this. Let's

1009
01:05:51,629 --> 01:05:54,329
see. So I have a few things that
I want to talk about. And we

1010
01:05:54,329 --> 01:05:58,109
have like, really 10 minutes
left or so would it be all

1011
01:05:58,109 --> 01:06:02,429
right? If we run a little longer
is it? Yeah, absolutely. Okay.

1012
01:06:03,329 --> 01:06:07,469
So one thing that's very
interesting to me, and I really,

1013
01:06:07,469 --> 01:06:10,769
really wanted to talk about
this. So I'm very happy. We can

1014
01:06:10,769 --> 01:06:14,039
run a little long. But can you
tell it's like one of the huge

1015
01:06:14,039 --> 01:06:18,179
challenges, I believe in setting
this whole thing up, which we

1016
01:06:18,179 --> 01:06:21,989
talked about before, is figuring
out how to scope these

1017
01:06:21,989 --> 01:06:24,749
challenges, because you want to
make it so it's not like like,

1018
01:06:24,749 --> 01:06:27,959
as you were just saying, you
don't want all the teams to

1019
01:06:27,959 --> 01:06:31,079
fail, because it's too hard. And
then it drives no innovation,

1020
01:06:31,079 --> 01:06:34,679
because no one can do anything.
But you don't want to make it

1021
01:06:34,679 --> 01:06:39,929
too easy. Also, can you just
talk a bit about scoping the

1022
01:06:39,929 --> 01:06:44,969
entire SubT challenge? Yeah.
Yeah, I think process of that.

1023
01:06:45,270 --> 01:06:49,980
Tim Chung: Yeah, you know, I
think it boils back down to the

1024
01:06:49,980 --> 01:06:53,250
really doing our homework and
kind of getting, I think that's

1025
01:06:53,250 --> 01:06:58,080
what DARPA is all about is not
only gauging where the current

1026
01:06:58,080 --> 01:07:01,770
state of technology is, and not
only kind of trying to

1027
01:07:01,770 --> 01:07:04,290
understand the trend lines of
where technology might be

1028
01:07:04,530 --> 01:07:08,430
gradually going, but
understanding the levers where

1029
01:07:08,520 --> 01:07:12,540
if we were to exercise, some,
you know, nudge here, hope

1030
01:07:12,540 --> 01:07:17,250
they're tested that, that we
would actually bend the curve of

1031
01:07:17,250 --> 01:07:21,030
the trajectory, and accelerate
where technology is gonna go. I

1032
01:07:21,030 --> 01:07:25,290
think that's what DARPA is, a
foundational mission is to try

1033
01:07:25,290 --> 01:07:28,470
to create a kind of
technological surprise. And so

1034
01:07:28,470 --> 01:07:31,350
by doing our homework, both on
terms of where the technology

1035
01:07:31,350 --> 01:07:35,520
is, but also what the end users
really needed, kind of doing

1036
01:07:35,520 --> 01:07:40,050
that, that you know, where we
want to be, right. And we want

1037
01:07:40,050 --> 01:07:43,890
to skate to that, to where the
puck is going to be, combined

1038
01:07:43,890 --> 01:07:51,120
with knowing where we were, you
know, I think it really was a

1039
01:07:51,120 --> 01:07:55,620
laborious process to tune and
tweak the evolution of the

1040
01:07:55,620 --> 01:08:00,240
challenge to be able to arrive,
as well as we have to this end

1041
01:08:00,240 --> 01:08:04,050
result of having advanced as
much as we had. And so I'll tell

1042
01:08:04,290 --> 01:08:10,140
a story here, at one of our very
early events, we transformed the

1043
01:08:10,500 --> 01:08:14,640
a gold mine into a test course,
and this was not for

1044
01:08:14,640 --> 01:08:17,910
competition, it was really just
a test. It was an integration

1045
01:08:17,910 --> 01:08:21,720
test, really. So we invited
competitors if they wanted to,

1046
01:08:21,720 --> 01:08:25,860
to come out and test and we had
done it up much like a

1047
01:08:25,860 --> 01:08:29,160
competition course. And this was
really to give teams a first

1048
01:08:29,160 --> 01:08:33,270
look at what DARPA had in store
for them. And so when we

1049
01:08:33,270 --> 01:08:39,990
conducted this, teams went in,
and you can safely say they got

1050
01:08:39,990 --> 01:08:45,060
their butts kicked, right, I
think they realize that, you

1051
01:08:45,060 --> 01:08:47,730
know, it's a little things that
we'll get to, in addition to the

1052
01:08:47,730 --> 01:08:50,760
big things that they were
worried about. And one of the

1053
01:08:50,760 --> 01:08:53,610
things that it became a
tradition afterwards. But we

1054
01:08:53,610 --> 01:08:57,960
took all of our competitors on a
walking tour of the entirety of

1055
01:08:57,960 --> 01:09:01,620
the course that we had laid out
that DARPA had laid out. And

1056
01:09:01,980 --> 01:09:06,390
whereas most teams maybe made
it, I'll be generous and say,

1057
01:09:06,570 --> 01:09:11,430
the best team made it 10% into
this first test course. Yeah.

1058
01:09:11,460 --> 01:09:15,840
When we went on the walking
tour, and showed them how far

1059
01:09:15,840 --> 01:09:19,500
and how expansive this course
really was. You could very

1060
01:09:19,500 --> 01:09:23,130
easily imagine that there are
those out there who would say,

1061
01:09:23,340 --> 01:09:27,240
Man, this is impossible. What is
DARPA thinking? They'll never,

1062
01:09:27,240 --> 01:09:29,400
you know, no one can ever
accomplish this. And yes, they

1063
01:09:29,400 --> 01:09:32,790
did say that to some degree,
like this is impossible, man.

1064
01:09:35,340 --> 01:09:39,840
But no, what really kind of
stuck with me is that the

1065
01:09:40,140 --> 01:09:44,130
prevailing sentiment after that
walk about two are in that very

1066
01:09:44,130 --> 01:09:49,920
first course, was not that this
can't be done, but that we

1067
01:09:49,920 --> 01:09:54,060
didn't do it. But man, somebody
believes in us that, you know,

1068
01:09:54,060 --> 01:09:58,320
DARPA must believe that this is
possible because otherwise why

1069
01:09:58,320 --> 01:10:02,130
would they have crawled up that
One ladder in the far reaches of

1070
01:10:02,130 --> 01:10:05,520
his mind, why would they have
kind of probably twisted ankles

1071
01:10:05,520 --> 01:10:09,090
to climb up to that location,
replace that cell phone company,

1072
01:10:09,180 --> 01:10:13,200
all the, you know, 80 pound man,
again, three kilometers into

1073
01:10:13,200 --> 01:10:16,740
this course, if they didn't
think that this was possible and

1074
01:10:16,740 --> 01:10:22,470
that spark, I think for them of
setting the bar really high, you

1075
01:10:22,470 --> 01:10:28,950
know, and saying yes, we think
that in due time you accelerate

1076
01:10:28,980 --> 01:10:32,580
the technology to get to this
point, you will be able to

1077
01:10:32,580 --> 01:10:36,450
conquer a good portion of this
course in the future. And that

1078
01:10:36,450 --> 01:10:39,930
trajectory, I think, has
continued to manifest throughout

1079
01:10:39,930 --> 01:10:41,880
the challenge. And so by the
time they got to their final

1080
01:10:41,880 --> 01:10:46,620
event, you have many teams on
record saying that you asked if

1081
01:10:46,620 --> 01:10:50,160
you ask them three years ago, or
four years ago, that they could

1082
01:10:50,160 --> 01:10:53,580
have conquered or handled any
part of this final event course

1083
01:10:53,580 --> 01:10:57,150
they'd say, Absolutely not. For
the fact that we're here, in

1084
01:10:57,180 --> 01:11:00,420
truth be told, by the way, you
know, I don't think we talked

1085
01:11:00,420 --> 01:11:04,440
about it. DARPA placed a fixed
number of artifacts in these

1086
01:11:04,440 --> 01:11:08,310
courses. And so for the final
event, we placed 40 artifacts in

1087
01:11:08,310 --> 01:11:11,070
nearly half a mile of
underground terrain that we had

1088
01:11:11,070 --> 01:11:17,550
built. And the highest scoring
teams scored 23 points, 23

1089
01:11:17,550 --> 01:11:21,000
artifacts. So you might say to
him, hey, that's kind of a

1090
01:11:21,000 --> 01:11:26,010
failing grade, right? That's
kind of like, well, 50% is just

1091
01:11:26,010 --> 01:11:28,830
over 50%. And you're saying you
did a great job? Well, yeah,

1092
01:11:28,860 --> 01:11:33,390
that's a phenomenal job, given
the difficulty level that we

1093
01:11:33,690 --> 01:11:37,890
know is where we want to be
right. And so again, it's really

1094
01:11:37,890 --> 01:11:38,370
been

1095
01:11:40,110 --> 01:11:43,500
showcasing that the technology
still has a lot of headroom to

1096
01:11:43,500 --> 01:11:48,510
grow. But to be able to quantify
the level of impacts, right,

1097
01:11:48,510 --> 01:11:54,000
we're able to cover this massive
environment in about an hour

1098
01:11:54,030 --> 01:11:58,770
with robots that, frankly, took
an order of magnitude, even two

1099
01:11:58,770 --> 01:12:01,920
orders of magnitude longer with
my human team to go gather that

1100
01:12:01,920 --> 01:12:07,590
high precision data, you know,
it took them a fair bit longer

1101
01:12:07,620 --> 01:12:10,590
to be able to go and do what
robots are doing in under an

1102
01:12:10,590 --> 01:12:14,790
hour under duress, never having
seen the courts before. That's a

1103
01:12:14,790 --> 01:12:17,940
market improvement that we can
point to and say, four years

1104
01:12:17,940 --> 01:12:20,790
ago, we might not have been able
to do it, if not, for this high

1105
01:12:20,790 --> 01:12:24,840
bar that DARPA sets. And I think
calibration process is a

1106
01:12:25,110 --> 01:12:29,970
iterative thing that we've fine
tuned, I think, here at DARPA,

1107
01:12:31,050 --> 01:12:33,360
Audrow Nash: I think it would be
more worrying if people got all

1108
01:12:33,360 --> 01:12:36,540
40 already. That would mean the
challenge was not hard enough

1109
01:12:36,660 --> 01:12:39,750
today, because it's a benchmark.
I mean, if you look at, again,

1110
01:12:39,750 --> 01:12:44,460
going back to like image net,
it'd be like 50% level when they

1111
01:12:44,460 --> 01:12:47,730
were starting, and then it's
like a free thing up 70% Now,

1112
01:12:47,730 --> 01:12:51,990
maybe, or 80, or whatever it is,
but it starts pretty low, and

1113
01:12:51,990 --> 01:12:54,600
then you have plenty of room to
improve, because you have a hard

1114
01:12:54,870 --> 01:12:57,780
challenge and getting all of it,
especially in that like limited

1115
01:12:57,780 --> 01:13:02,790
period of time. Right? Like
that's a real challenge. Yeah,

1116
01:13:02,790 --> 01:13:03,660
it's very interesting.

1117
01:13:03,870 --> 01:13:07,200
Tim Chung: Yeah, wholly agree.
And think that there's, you

1118
01:13:07,200 --> 01:13:10,320
know, really promising work to
get to be done and to be a

1119
01:13:10,320 --> 01:13:12,930
Audrow Nash: component, for
sure. And you were mentioning,

1120
01:13:12,960 --> 01:13:18,810
mentioning, DARPA has this very
iterative process for scoping,

1121
01:13:19,560 --> 01:13:22,230
like difficulty of these things.
Can you talk a bit about that,

1122
01:13:22,230 --> 01:13:23,610
because that's really
interesting to me.

1123
01:13:23,669 --> 01:13:26,219
Tim Chung: Sure. And so
specifically, for the sub D

1124
01:13:26,219 --> 01:13:29,819
challenge, we had designed it,
so that we would have those

1125
01:13:29,819 --> 01:13:35,279
opportunities to, to to learn as
we go. So by virtue of how we

1126
01:13:35,279 --> 01:13:40,019
constructed the SubT challenge,
we held that first event where

1127
01:13:40,019 --> 01:13:43,889
teams first saw what DARPA had
in store, but then, about six

1128
01:13:43,889 --> 01:13:46,019
months after there was the
tunnel circuit, where we

1129
01:13:46,019 --> 01:13:49,199
conducted our first event
focusing on tunnel environments,

1130
01:13:49,199 --> 01:13:52,829
in this case, a coal mine. And
then we broke that up into

1131
01:13:52,859 --> 01:13:57,209
again, six months after an urban
setting. And so the teams knew

1132
01:13:57,479 --> 01:14:00,329
that if they were going to be
competitive at the final event,

1133
01:14:00,329 --> 01:14:03,119
where they were anticipating
that all three of these

1134
01:14:03,119 --> 01:14:05,849
environments would all get
mashed together, then they

1135
01:14:05,849 --> 01:14:09,539
should be thinking about their
designs from the get go, that

1136
01:14:09,539 --> 01:14:14,549
can survive, quite frankly,
tunnel urban cave, all the way

1137
01:14:14,549 --> 01:14:19,379
to the very ends. And so we saw
teams to kind of internalize

1138
01:14:19,559 --> 01:14:24,419
this, this need for resilience
in their early kind of design

1139
01:14:24,419 --> 01:14:28,049
scoping. And so they went to the
tunnel and man, Were there a lot

1140
01:14:28,049 --> 01:14:31,559
of really hard lessons learned
at the tunnel a bit, but what

1141
01:14:31,559 --> 01:14:37,169
you saw was a learning not just
by team, but across teams. An

1142
01:14:37,169 --> 01:14:44,039
example is that a using a a
sensor on the outside to be able

1143
01:14:44,039 --> 01:14:47,519
to help correct as robots still
within line of sight of the

1144
01:14:47,579 --> 01:14:52,469
mouth of the tunnel. You know,
we had a team that was using

1145
01:14:52,469 --> 01:14:57,449
that and so they could extend
how far their accuracy was, you

1146
01:14:57,449 --> 01:15:00,989
know, subject to drift they
could they could extend And how

1147
01:15:00,989 --> 01:15:04,139
far they were by using this
total station sensor to, to

1148
01:15:04,139 --> 01:15:08,129
correct for any drift. And yeah,
that's, that's clever and so

1149
01:15:08,729 --> 01:15:12,539
even even at the tunnel circuit
he saw teams going out and

1150
01:15:12,719 --> 01:15:16,439
trying to find and they found a
local university and borrowed

1151
01:15:16,679 --> 01:15:19,679
one of these little stations and
tried to use it and by the Urban

1152
01:15:19,679 --> 01:15:23,099
circuit, you had more teams
using such a technology. Same

1153
01:15:23,099 --> 01:15:26,459
goes for legged robots say,
legged robots didn't do so well

1154
01:15:26,459 --> 01:15:29,219
at the tunnel circuit. But at
the Urban circuit where there

1155
01:15:29,219 --> 01:15:33,059
were stairs and curbs and other
things, man, those legged robots

1156
01:15:33,329 --> 01:15:37,799
really shine. And so at the
final event, you saw a lot of

1157
01:15:37,829 --> 01:15:41,489
legged robots coming to the
fore. And so that's what I mean,

1158
01:15:41,519 --> 01:15:45,119
by the iteration, there was
multiple opportunities for these

1159
01:15:45,119 --> 01:15:54,359
teams to come field go through
the the rigor of a competition

1160
01:15:54,359 --> 01:15:58,259
events, but that wasn't the end
of it, they would have to go

1161
01:15:58,259 --> 01:16:02,519
home, lick their wounds, and
then come back and do it again.

1162
01:16:02,759 --> 01:16:06,599
And then, of course, break some
more robots and learn what it

1163
01:16:06,599 --> 01:16:11,789
means to have to operate in the
cold, called nuclear power plant

1164
01:16:11,789 --> 01:16:15,569
or the human limestone cavern
or, you know, all of those kinds

1165
01:16:15,569 --> 01:16:21,749
of things. We baked in that
iteration and the kinds of

1166
01:16:21,779 --> 01:16:26,339
opportunity to learn quickly and
often into the stuff the

1167
01:16:26,339 --> 01:16:29,399
challenge. And I think, as a
roboticist, myself, I think

1168
01:16:29,579 --> 01:16:33,689
that's where you learn the most
by thinking and then doing right

1169
01:16:33,719 --> 01:16:37,949
thinking of doing breaking,
learning, and then thinking and

1170
01:16:37,949 --> 01:16:42,629
doing it again. And Field
Robotics, I think is, you know,

1171
01:16:42,629 --> 01:16:47,969
needs that and sometimes trying
to do robotics in the field,

1172
01:16:47,969 --> 01:16:50,189
especially at the scales that
the sub the challenge was

1173
01:16:50,189 --> 01:16:53,789
interested in really hard,
logistically, you know, it takes

1174
01:16:53,789 --> 01:16:57,869
a lot of time, a lot of people
power to get to transport robots

1175
01:16:57,869 --> 01:17:01,769
and get things set up. But, you
know, sub The challenge really

1176
01:17:01,769 --> 01:17:07,499
gave all these teams, not only
the excuse, but the incentive to

1177
01:17:07,529 --> 01:17:10,709
have to go and test in the wild.
And many of the teams,

1178
01:17:10,739 --> 01:17:14,189
especially the top performing
teams, really took that to heart

1179
01:17:14,369 --> 01:17:19,469
found, you know, and partner
with local caver clubs, local

1180
01:17:19,469 --> 01:17:23,099
gaming, clubs to go and gain
access to their caves and

1181
01:17:23,129 --> 01:17:27,779
practice, or, you know, all of
those kinds of opportunities, I

1182
01:17:27,779 --> 01:17:32,489
think, came about, with many,
many lessons learned a lot, a

1183
01:17:32,489 --> 01:17:38,819
lot of a lot of broken robots,
but a whole cadre of field

1184
01:17:38,819 --> 01:17:42,089
roboticists and the next
generation, I'd say, Field

1185
01:17:42,089 --> 01:17:47,099
Robotics is coming out, having
been battle tested in the subsea

1186
01:17:47,099 --> 01:17:47,729
challenge.

1187
01:17:49,140 --> 01:17:53,700
Audrow Nash: That's really cool.
So that makes me curious about

1188
01:17:53,700 --> 01:17:59,640
kind of the long term role of
DARPA in all of this. So you're,

1189
01:17:59,640 --> 01:18:02,730
you're mentioning like one
thing. Very interesting, as

1190
01:18:02,730 --> 01:18:05,100
you're mentioning, training the
next generation of Field

1191
01:18:05,100 --> 01:18:09,660
Robotics, roboticists, can you
just talk a bit more about the

1192
01:18:09,660 --> 01:18:13,500
role of DARPA? Yeah, as you see
kind of on a bigger picture?

1193
01:18:13,710 --> 01:18:17,430
Tim Chung: Sure. Well, you know,
at its heart, DARPA is always

1194
01:18:17,430 --> 01:18:20,820
going to be about bringing about
technological surprise, and

1195
01:18:20,970 --> 01:18:24,840
those breakthrough technologies
that will have broad impact

1196
01:18:24,930 --> 01:18:29,790
overall, and whether that's deep
investments that will lead to

1197
01:18:30,600 --> 01:18:33,480
breakthroughs when we need them.
And an example of that is that

1198
01:18:33,480 --> 01:18:39,630
mRNA vaccine approach that in
the time COVID, was investments

1199
01:18:39,630 --> 01:18:43,500
by DARPA early on to identify
that kind of methodology in the

1200
01:18:43,500 --> 01:18:47,580
time of need, where it would
arise. And so identifying, you

1201
01:18:47,580 --> 01:18:52,920
know, those opportunities where,
even if you don't need it today,

1202
01:18:52,920 --> 01:18:55,290
but you might need it in the
future. That's what DARPA is all

1203
01:18:55,290 --> 01:19:00,450
about. And I think, recognizing
that the state of the technology

1204
01:19:00,450 --> 01:19:03,810
for robotics Field Robotics
wasn't where we wanted it to be

1205
01:19:03,810 --> 01:19:10,650
in the future. That's what DARPA
was eager to inspire, and

1206
01:19:10,770 --> 01:19:16,230
incentivize, and then help shape
that future trajectory. As far

1207
01:19:16,230 --> 01:19:19,440
as building the community. I
really love the DARPA Challenge

1208
01:19:19,440 --> 01:19:25,020
model for innovation, where, you
know, we have a couple of

1209
01:19:25,020 --> 01:19:29,430
different ways that we seek out
revolutionary ideas. But the

1210
01:19:29,430 --> 01:19:32,880
challenge model is just one type
of those and, and I really love

1211
01:19:32,880 --> 01:19:36,270
it, because it also gives us an
opportunity to think about

1212
01:19:36,480 --> 01:19:41,370
solving a problem without having
defines the solution. When you

1213
01:19:41,370 --> 01:19:46,560
do that, and the community
hasn't yet informed around that

1214
01:19:46,560 --> 01:19:51,000
problem area, or the solution
space. The DARPA Challenge is

1215
01:19:51,000 --> 01:19:54,300
fantastic for being able to
plant the seed and nurture this

1216
01:19:54,540 --> 01:19:57,750
community that's now going to go
off and do great things right.

1217
01:19:57,750 --> 01:20:03,210
So the DARPA first started Grand
Challenge, self driving cars to

1218
01:20:03,210 --> 01:20:06,060
the desert or in the urban
environments, you know, had

1219
01:20:06,120 --> 01:20:09,630
planted the seed for a lot of
the of the self driving tech

1220
01:20:10,290 --> 01:20:16,470
investments today. And I imagine
that many of the types of

1221
01:20:16,500 --> 01:20:19,980
technologies that we've
discovered and invested in for

1222
01:20:19,980 --> 01:20:24,810
the SubT challenge will have
that long term impact. While

1223
01:20:24,810 --> 01:20:29,280
also, and I'm excited to say,
have a very near term impact for

1224
01:20:29,310 --> 01:20:33,600
many of our end users as well,
given how we structured this

1225
01:20:33,600 --> 01:20:37,980
this challenge to kind of think
about marrying, you know, what

1226
01:20:37,980 --> 01:20:43,200
the what the what the end user
needs tomorrow, not just many

1227
01:20:43,200 --> 01:20:46,380
years from now. So I think being
able to span that has been a

1228
01:20:46,380 --> 01:20:50,040
real hallmark of the DARPA
subterranean challenge of

1229
01:20:50,100 --> 01:20:52,920
understanding what the near
problem is, as well as

1230
01:20:52,950 --> 01:20:56,310
anticipating and planting the
seed for the communities to

1231
01:20:56,310 --> 01:20:57,840
address the fire problems as
well.

1232
01:20:59,340 --> 01:21:05,460
Audrow Nash: Awesome. Let's see.
So I think I don't know how it

1233
01:21:05,460 --> 01:21:11,400
works. But what's next? Yeah,
like, what next with this SubT

1234
01:21:11,400 --> 01:21:15,690
challenge next with maybe
another, I suppose Grand

1235
01:21:15,690 --> 01:21:17,910
Challenges take a long time to
think about and I don't know,

1236
01:21:17,910 --> 01:21:20,400
things can be revealed. But
what's next? Yeah,

1237
01:21:20,879 --> 01:21:24,509
Tim Chung: well, I'll say that
the SubT challenge has

1238
01:21:24,509 --> 01:21:29,009
concluded. And while we have all
of those resources, all of the

1239
01:21:29,309 --> 01:21:32,129
Open Source Repositories, all
the things that we talked about,

1240
01:21:32,339 --> 01:21:36,959
available out there, it's my
wish that folks, you know, take

1241
01:21:36,959 --> 01:21:42,689
it and run with it, they can
reinvent or recreate or invent

1242
01:21:42,689 --> 01:21:46,439
the new, different types of
approaches to solving some of

1243
01:21:46,439 --> 01:21:51,149
these types of problems. And so
I think that what we see already

1244
01:21:51,149 --> 01:21:55,049
in the community is kind of
we've spun up the firewall, and

1245
01:21:55,049 --> 01:21:57,809
now it's operating on its own
momentum. And it's really

1246
01:21:57,809 --> 01:22:01,979
invigorating and inspiring to
see all these folks already out

1247
01:22:01,979 --> 01:22:06,899
there. You know, organizing
amongst themselves the the

1248
01:22:06,899 --> 01:22:11,549
ability to have this kind of an
impact. And being a resource. As

1249
01:22:11,549 --> 01:22:15,689
far as DARPA is concerned, I
think, you know, if any of your

1250
01:22:15,689 --> 01:22:19,439
listeners have ideas for Grand
Challenges, that's an

1251
01:22:19,439 --> 01:22:23,549
opportunity here, I think DARPA
is always on the lookout for

1252
01:22:23,549 --> 01:22:26,759
those types of problems, those
kind of technology questions

1253
01:22:26,759 --> 01:22:30,779
that are out there that don't
have or would benefit from not

1254
01:22:30,779 --> 01:22:35,639
having a pre defined solution,
kind of a direction to go, you

1255
01:22:35,639 --> 01:22:39,839
know that there's a breakthrough
waiting to happen. But you don't

1256
01:22:39,839 --> 01:22:42,149
quite have a finger on where
that breakthrough is going to

1257
01:22:42,149 --> 01:22:48,059
come from. And a challenge model
is great. And so DARPA, I am

1258
01:22:49,829 --> 01:22:54,899
certain that the DARPA Challenge
model is here to stay. It will,

1259
01:22:54,899 --> 01:22:59,549
as it at, like all things at
DARPA, continued to evolve and

1260
01:23:00,719 --> 01:23:05,309
tailor itself to the technology
of interest. But now, I'm

1261
01:23:05,309 --> 01:23:09,419
excited to say that, you know,
which DARPA Challenge model is

1262
01:23:09,659 --> 01:23:14,309
demonstrated, at least with sub
t, that it's a it's an exciting

1263
01:23:14,309 --> 01:23:17,249
way to both drive a community
and drive technology

1264
01:23:17,249 --> 01:23:17,789
development.

1265
01:23:20,130 --> 01:23:25,110
Audrow Nash: So wrapping up, do
you have any links, websites,

1266
01:23:25,110 --> 01:23:28,560
anything to share? I know the
SubT website. So I'll include

1267
01:23:28,560 --> 01:23:31,200
that in the post anything,
anything else to highlight?

1268
01:23:31,950 --> 01:23:35,220
Tim Chung: Yeah, I think, you
know, if you go to one of the

1269
01:23:35,220 --> 01:23:38,490
repositories, we mentioned
earlier that a GitHub repo slash

1270
01:23:38,490 --> 01:23:42,810
SubT challenge, there, you'll
see a project there, just called

1271
01:23:42,840 --> 01:23:47,160
simply SubT resources. But this
is where it's kind of the one

1272
01:23:47,160 --> 01:23:50,880
stop shop for all of the
references, links to software

1273
01:23:50,880 --> 01:23:54,840
that the competitors themselves
have now released. So you can

1274
01:23:54,840 --> 01:23:58,710
see that the datasets that both
DARPA have provided, as well as

1275
01:23:58,710 --> 01:24:02,910
many that SubT community has
developed and curated, are

1276
01:24:02,910 --> 01:24:08,460
there, links to papers and other
materials are there. So you

1277
01:24:08,460 --> 01:24:11,910
know, I encourage folks to go
check that out. And then of

1278
01:24:11,910 --> 01:24:17,130
course, the DARPA TV YouTube
channel has many, many videos

1279
01:24:17,130 --> 01:24:20,100
that you can go learn about the
sub D challenge, its impact and

1280
01:24:20,100 --> 01:24:23,490
why it matters, as well as learn
about all the teams as well. So

1281
01:24:23,670 --> 01:24:27,420
I think those are a great place
to start. And if you're

1282
01:24:27,420 --> 01:24:30,150
interested in learning more
about DARPA, I think darpa.mil

1283
01:24:30,180 --> 01:24:33,420
is a is a fantastic landing
point for any of your listeners.

1284
01:24:34,350 --> 01:24:36,810
Audrow Nash: Oh, yeah. Okay,
thank you very much.

1285
01:24:37,530 --> 01:24:39,150
Tim Chung: Cool. It's been a
pleasure. Thank you so much.

1286
01:24:41,250 --> 01:24:42,960
Audrow Nash: Thanks for
listening to my conversation

1287
01:24:42,960 --> 01:24:46,500
with Tim Chung. Thank you again
to our founding sponsor, open

1288
01:24:46,500 --> 01:24:48,210
robotics. See you next time.

